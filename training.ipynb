{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TerminateOnNaN\n",
    "import cv2\n",
    "import itertools\n",
    "import gc\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.schedules import LearningRateSchedule\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "import segmentation_models as sm\n",
    "from collections import defaultdict\n",
    "from tensorflow.keras.metrics import MeanIoU\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras import backend as K\n",
    "K.clear_session()\n",
    "\n",
    "# Set segmentation models to use tf.keras backend\n",
    "os.environ[\"SM_FRAMEWORK\"] = \"tf.keras\"\n",
    "\n",
    "\n",
    "\n",
    "# --- Model Building --- \n",
    "miou_metric = MeanIoUMetric(num_classes=6)\n",
    "\n",
    "# --- U-Net Model ---\n",
    "\n",
    "def train_unet(\n",
    "    base_dir: str = \"/content/chipped_data/content/chipped_data\",\n",
    "    out_dir: str = \"/content/figs\",\n",
    "    input_type: str = \"rgb_elev\",\n",
    "    model_type: str = \"enhanced_unet\",\n",
    "    tile_size: int = 256,\n",
    "    batch_size: int = 8,\n",
    "    epochs: int = 50,\n",
    "    train_time: int = 20,\n",
    "    verbose: int = 1,\n",
    "    yummy: bool = False,\n",
    "    model_path: str = None,\n",
    "):\n",
    "    \"\"\"Trains a semantic segmentation model using a specified configuration.\n",
    "\n",
    "    Args:\n",
    "        base_dir (str): Path to the base data directory.\n",
    "        out_dir (str): Output directory to save plots and the model.\n",
    "        input_type (str): Input configuration, e.g., 'rgb' or 'rgb_elev'.\n",
    "        model_type (str): Type of model to build (e.g., 'unet', 'resnet34').\n",
    "        tile_size (int): Width and height of each input tile in pixels.\n",
    "        batch_size (int): Batch size for training.\n",
    "        epochs (int): Maximum number of training epochs.\n",
    "        train_time (int): Maximum training time in minutes.\n",
    "        verbose (int): Verbosity level for training output.\n",
    "        yummy (bool): Whether to plot full-tile predictions after training.\n",
    "        model_path (str): Optional path to a pretrained model to resume training.\n",
    "    \"\"\"\n",
    "\n",
    "    assert input_type in INPUT_TYPE_CONFIG, f\"Unknown input type: {input_type}\"\n",
    "    num_channels = INPUT_TYPE_CONFIG[input_type][\"channels\"]\n",
    "\n",
    "    img_dir = os.path.join(base_dir, \"train\", \"images\")\n",
    "    elev_dir = os.path.join(base_dir, \"train\", \"elevations\")\n",
    "    slope_dir = os.path.join(base_dir, \"train\", \"slopes\")\n",
    "    label_dir = os.path.join(base_dir, \"train\", \"labels\")\n",
    "\n",
    "    # Load metadata and define input shape\n",
    "    input_shape = (tile_size, tile_size, num_channels)\n",
    "    train_df = csv_to_df('train', subset=1.0)\n",
    "    val_df = csv_to_df('val')\n",
    "    test_df = csv_to_df('test')\n",
    "\n",
    "    # --- Streaming Data Generator ---\n",
    "    train_gen = build_tf_dataset(train_df, img_dir, elev_dir, slope_dir, label_dir,\n",
    "                                 input_type=input_type, split='train',\n",
    "                                 augment=True, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "    val_gen = build_tf_dataset(val_df, img_dir, elev_dir, slope_dir, label_dir,\n",
    "                                input_type=input_type, split='val',\n",
    "                                augment=False, shuffle=False, batch_size=batch_size)\n",
    "    \n",
    "    test_gen = build_tf_dataset(test_df, img_dir, elev_dir, slope_dir, label_dir,\n",
    "                            input_type=input_type, split='test',\n",
    "                            augment=False, shuffle=False, batch_size=batch_size)\n",
    "\n",
    "\n",
    "    # To visualize augmented training examples:\n",
    "    plot_augmented_grid_from_dataset(\n",
    "        tf_dataset=train_gen,\n",
    "        input_type='rgb', \n",
    "    )\n",
    "\n",
    "\n",
    "    # --- Model ---\n",
    "    import segmentation_models as sm\n",
    "    if model_path is None or not os.path.exists(model_path):\n",
    "        if model_type == \"unet\":\n",
    "            model = build_unet(input_shape=input_shape, num_classes=NUM_CLASSES)\n",
    "        \n",
    "        elif model_type == \"new_unet\":\n",
    "            model, base_model = build_flexible_unet(input_shape=input_shape, num_classes=NUM_CLASSES, freeze_rgb_encoder=False)\n",
    "\n",
    "        elif model_type == \"multi_unet\":\n",
    "            model = build_multi_unet(input_shape=input_shape, num_classes=NUM_CLASSES)\n",
    "\n",
    "        elif model_type == \"enhanced_unet\":\n",
    "            model = enhanced_unet(input_shape=input_shape, num_classes=NUM_CLASSES)\n",
    "\n",
    "        elif model_type == \"resnet34\":\n",
    "            model = sm.Unet(\n",
    "                backbone_name=\"resnet34\",               # or 'efficientnetb0', 'mobilenetv2', etc.\n",
    "                input_shape=input_shape,\n",
    "                classes=NUM_CLASSES,                  \n",
    "                activation='softmax', \n",
    "                encoder_weights='imagenet'              # Load ImageNet pre-trained weights\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model_type: {model_type}\")\n",
    "    else:\n",
    "        custom_objects={\n",
    "            'DiceLoss': sm.losses.DiceLoss,\n",
    "            'CategoricalFocalLoss': sm.losses.CategoricalFocalLoss,\n",
    "            'MeanIoU': MeanIoUMetric\n",
    "        }  \n",
    "\n",
    "        model = tf.keras.models.load_model(\n",
    "            model_path,\n",
    "            custom_objects=custom_objects,\n",
    "            compile=True\n",
    "        )\n",
    "\n",
    "        for layer in model.layers:\n",
    "            layer.trainable = True\n",
    "\n",
    "    model.summary()\n",
    "    print(f\"Number of Parameters: {model.count_params()}\\n\"\n",
    "          f\"Number of Layers: {len(model.layers)}\\n\")\n",
    "\n",
    "\n",
    "    # --- Callbacks --- \n",
    "    monitor = \"val_iou_score\"       # \"val_loss\"\n",
    "    nan_terminate = TerminateOnNaN()\n",
    "    time_limit = TimeLimitCallback(max_minutes=train_time)\n",
    "    early_stop = EarlyStopping(monitor=monitor, mode=\"max\", patience=10, restore_best_weights=True, verbose=1)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor=monitor, mode=\"max\", patience=5, min_lr=5e-7, factor=0.5, verbose=1, min_delta=1e-4)\n",
    "    #weight_callback = DynamicClassWeightUpdater(val_data=val_gen, update_every=5, target='iou', ignore_class=4)\n",
    "    #LearningRateLogger()\n",
    "\n",
    "    callbacks = [\n",
    "        reduce_lr,\n",
    "        time_limit,\n",
    "        early_stop,\n",
    "        nan_terminate, \n",
    "        StepTimer(),\n",
    "    ]\n",
    "\n",
    "    metrics = [\n",
    "        sm.metrics.IOUScore(threshold=None, name=\"iou_score\"),   # fast, approximated mIoU per batch\n",
    "        sm.metrics.FScore(threshold=None, name=\"f1-score\"),\n",
    "        tf.keras.metrics.CategoricalAccuracy(name=\"categorical_accuracy\"),\n",
    "    ]\n",
    "\n",
    "    learning_rate = 5.2e-4\n",
    "    label_smoothing = 0.075\n",
    "    loss_weights = [0.25, 1.5, 2.25]                    # [cce, dice, focal]\n",
    "    class_weights = [6.95, 3.3, 0.3, 12.5, 4.0, 2.6]    # [building, clutter, vegetation, water, background, car]\n",
    "\n",
    "    # Normalize class weights\n",
    "    total = sum(class_weights)\n",
    "    norm_class_weights = [w / total for w in class_weights]\n",
    "\n",
    "    focal_gamma = 4.0\n",
    "    raw_dice = sm.losses.DiceLoss(class_weights=norm_class_weights)\n",
    "    raw_focal = sm.losses.CategoricalFocalLoss(\n",
    "        alpha=norm_class_weights,\n",
    "        gamma=focal_gamma,\n",
    "    )\n",
    "    raw_cce = CategoricalCrossentropy()\n",
    "\n",
    "    optimizer = mixed_precision.LossScaleOptimizer(\n",
    "        Adam(learning_rate=learning_rate), dynamic=True\n",
    "    )\n",
    "\n",
    "\n",
    "    def total_loss(y_true: tf.Tensor, y_pred: tf.Tensor) -> tf.Tensor:\n",
    "        \"\"\"Computes the total weighted loss from CCE, Dice, and Focal losses.\"\"\"\n",
    "        y_true_smoothed = apply_label_smoothing(y_true, smoothing=label_smoothing)\n",
    "\n",
    "        dice = raw_dice(y_true_smoothed, y_pred)\n",
    "        focal = raw_focal(y_true_smoothed, y_pred)\n",
    "        cce = raw_cce(y_true_smoothed, y_pred)\n",
    "\n",
    "        base_loss = (\n",
    "            loss_weights[0] * cce +\n",
    "            loss_weights[1] * dice +\n",
    "            loss_weights[2] * focal\n",
    "        )\n",
    "\n",
    "        #return apply_ignore_class_mask(y_true_smoothed, y_pred, ignore_class=4, loss_fn=lambda yt, yp: base_loss)\n",
    "        return base_loss\n",
    "\n",
    "\n",
    "    # --- Train Model ---\n",
    "    # ‚è±Ô∏è Start training timer\n",
    "    import time\n",
    "    from datetime import datetime\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    print(\"üïí Start Time:\", timestamp)\n",
    "    start_time = time.time()\n",
    "\n",
    "\n",
    "    # Single Stage Training\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=total_loss,\n",
    "        metrics=metrics\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        train_gen, validation_data=val_gen,\n",
    "        epochs=epochs,\n",
    "        callbacks=callbacks,\n",
    "        verbose=verbose\n",
    "    )\n",
    "\n",
    "    # --- Save Model ---\n",
    "    os.makedirs(\"/content/figs\", exist_ok=True)\n",
    "    model.save(\"/content/figs/segmentation_model.keras\")\n",
    "\n",
    "    # Save to OneDrive (Google Drive in Colab)\n",
    "    os.makedirs(\"/content/drive/MyDrive\", exist_ok=True)\n",
    "    model.save(\"/content/drive/MyDrive/segmentation_model.keras\")\n",
    "\n",
    "    # ‚è±Ô∏è End training timer\n",
    "    end_time = time.time()\n",
    "    duration_sec = end_time - start_time\n",
    "    duration_str = time.strftime('%H:%M:%S', time.gmtime(duration_sec))\n",
    "    print(f\"\\n‚úÖ Training complete in {duration_str} ({duration_sec:.2f} seconds)\")\n",
    "\n",
    "\n",
    "    '''\n",
    "    # Two Stage Training\n",
    "    if fine_tune:\n",
    "        hard_df = csv_to_hard_df()\n",
    "        train_hard = build_tf_dataset(hard_df, img_dir, elev_dir, slope_dir, label_dir,\n",
    "                                    input_type=input_type, split='train',\n",
    "                                    augment=True, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "\n",
    "        print(\"Training on hard examples...\")\n",
    "        optimizer = mixed_precision.LossScaleOptimizer(\n",
    "            Adam(learning_rate=1e-5), dynamic=True\n",
    "        )\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss=total_loss,\n",
    "            metrics=metrics\n",
    "        )\n",
    "\n",
    "        history = model.fit(\n",
    "            train_hard, validation_data=val_gen,\n",
    "            epochs=history.epoch[-1] + 16,\n",
    "            initial_epoch=history.epoch[-1] + 1,\n",
    "            callbacks=callbacks,\n",
    "            verbose=verbose\n",
    "        )\n",
    "\n",
    "    '''\n",
    "\n",
    "\n",
    "    def safe_get(var_name, context, default=\"‚ùå Not Defined\"):\n",
    "        \"\"\"Safely retrieve a variable from the local context.\"\"\"\n",
    "        return context.get(var_name, default)\n",
    "\n",
    "    def safe_get_history(history_dict, key):\n",
    "        \"\"\"Safely retrieve the last value of a metric from the history dict.\"\"\"\n",
    "        return history_dict.get(key, [\"N/A\"])[-1] if key in history_dict else \"N/A\"\n",
    "    local_vars = locals()\n",
    "\n",
    "\n",
    "    print(f\"Initial Learning Rate: {safe_get('learning_rate', local_vars)}\\n\"\n",
    "        f\"Loss Weights: {safe_get('loss_weights', local_vars)}, \"\n",
    "        f\"Class Weights: {safe_get('class_weights', local_vars)}\\n\"\n",
    "        f\"Focal Loss Gamma: {safe_get('focal_gamma', local_vars)}\\n\"\n",
    "        f\"Label Smoothing: {safe_get('label_smoothing', local_vars)}\\n\"\n",
    "        f\"Input Type: {safe_get('input_type', local_vars)}, \"\n",
    "        f\"Model Type: {safe_get('model_type', local_vars)}\\n\"\n",
    "        f\"Batch Size: {safe_get('batch_size', local_vars)}, \"\n",
    "        f\"Epochs: {history.epoch[-1] + 1}\\n\"\n",
    "        f\"Number of Parameters: {model.count_params()}, \"\n",
    "        f\"Number of Layers: {len(model.layers)}\\n\"\n",
    "        f\"Final Validation Loss: {safe_get_history(history.history, 'val_loss'):.4f}\\n\"\n",
    "        f\"Final Validation mIoU: {safe_get_history(history.history, 'val_iou_score'):.4f}\\n\"\n",
    "        f\"Final Validation F1 Score: {safe_get_history(history.history, 'val_f1-score'):.4f}\\n\"\n",
    "        f\"Final Validation Categorical Accuracy: {safe_get_history(history.history, 'categorical_accuracy'):.4f}\\n\")\n",
    "\n",
    "\n",
    "    measure_inference_time(model, test_gen, num_batches=5)\n",
    "    plot_training_curves(history, out_dir)\n",
    "    evaluate_on_test(model, test_gen, test_df, \"/content/figs\", img_dir, label_dir, tile_size, n_rows=8, n_cols=3)\n",
    "\n",
    "\n",
    "    # --- Full-Tile Reconstruction (Optional) ---\n",
    "    if yummy:\n",
    "        for tile_prefix in test_files:\n",
    "            img, label, pred = reconstruct_canvas(\n",
    "                model,\n",
    "                test_df,\n",
    "                tile_prefix,\n",
    "                build_tf_dataset,\n",
    "                img_dir,\n",
    "                elev_dir,\n",
    "                slope_dir,\n",
    "                label_dir\n",
    "            )\n",
    "            plot_reconstruction(img, label, pred, tile_prefix)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- SegFormer Training ---\n",
    "\n",
    "def train_segformer(\n",
    "    base_dir: str = \"/content/chipped_data/content/chipped_data\",\n",
    "    out_dir: str = \"/content/figs\",\n",
    "    input_type: str = \"rgb\",\n",
    "    model_type: str = \"B1\",\n",
    "    tile_size: int = 256,\n",
    "    batch_size: int = 8,\n",
    "    epochs: int = 50,\n",
    "    train_time: int = 60,\n",
    "    verbose: int = 1,\n",
    "    yummy: bool = False,\n",
    "    model_path: str = None,\n",
    "):\n",
    "    \"\"\"Trains a semantic segmentation model using a specified configuration.\n",
    "\n",
    "    Args:\n",
    "        base_dir (str): Path to the base data directory.\n",
    "        out_dir (str): Output directory to save plots and the model.\n",
    "        input_type (str): Input configuration, e.g., 'rgb' or 'rgb_elev'.\n",
    "        model_type (str): Type of model to build (e.g., 'B0', 'B1', 'B2' or 'B3').\n",
    "        tile_size (int): Width and height of each input tile in pixels.\n",
    "        batch_size (int): Batch size for training.\n",
    "        epochs (int): Maximum number of training epochs.\n",
    "        train_time (int): Maximum training time in minutes.\n",
    "        verbose (int): Verbosity level for training output.\n",
    "        yummy (bool): Whether to plot full-tile predictions after training.\n",
    "        model_path (str): Optional path to a pretrained model to resume training.\n",
    "    \"\"\"\n",
    "\n",
    "    assert model_type in [\"B0\", \"B1\", \"B2\", \"B3\", \"B4\", \"B5\"], f\"Unknown model type: {model_type}\"\n",
    "    assert input_type in INPUT_TYPE_CONFIG, f\"Unknown input type: {input_type}\"\n",
    "    num_channels = INPUT_TYPE_CONFIG[input_type][\"channels\"]\n",
    "\n",
    "    img_dir = os.path.join(base_dir, \"train\", \"images\")\n",
    "    elev_dir = os.path.join(base_dir, \"train\", \"elevations\")\n",
    "    slope_dir = os.path.join(base_dir, \"train\", \"slopes\")\n",
    "    label_dir = os.path.join(base_dir, \"train\", \"labels\")\n",
    "\n",
    "    # Load metadata and define input shape\n",
    "    input_shape = (tile_size, tile_size, num_channels)\n",
    "    train_df = csv_to_df('train', 0.4)\n",
    "    val_df = csv_to_df('val')\n",
    "    test_df = csv_to_df('test')\n",
    "\n",
    "\n",
    "    # --- Streaming Data Generator ---\n",
    "    train_gen = build_tf_dataset(train_df, img_dir, elev_dir, slope_dir, label_dir,\n",
    "                                 input_type=input_type, split='train',\n",
    "                                 augment=True, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "    val_gen = build_tf_dataset(val_df, img_dir, elev_dir, slope_dir, label_dir,\n",
    "                                input_type=input_type, split='val',\n",
    "                                augment=False, shuffle=False, batch_size=batch_size)\n",
    "    \n",
    "    test_gen = build_tf_dataset(test_df, img_dir, elev_dir, slope_dir, label_dir,\n",
    "                            input_type=input_type, split='test',\n",
    "                            augment=False, shuffle=False, batch_size=batch_size)\n",
    "\n",
    "    for x_batch, y_batch in test_gen.take(1):\n",
    "        y_np = np.argmax(y_batch.numpy(), axis=-1)\n",
    "        print(\"Unique labels in y batch:\", np.unique(y_np))\n",
    "\n",
    "\n",
    "    # --- Model ---\n",
    "    import segmentation_models as sm\n",
    "    if model_path is None or not os.path.exists(model_path):\n",
    "\n",
    "        if model_type == \"B2\":\n",
    "            model = SegFormer_B2(input_shape=input_shape, num_classes=NUM_CLASSES)\n",
    "\n",
    "        elif model_type == \"B0\":\n",
    "            model = SegFormer_B0(input_shape=input_shape, num_classes=NUM_CLASSES)\n",
    "        \n",
    "        elif model_type == \"B5\":\n",
    "            model = SegFormer_B5(input_shape=input_shape, num_classes=NUM_CLASSES)\n",
    "\n",
    "        elif model_type == \"B4\":\n",
    "            model = SegFormer_B4(input_shape=input_shape, num_classes=NUM_CLASSES)\n",
    "\n",
    "        elif model_type == \"B1\":\n",
    "            model = SegFormer_B1(input_shape=input_shape, num_classes=NUM_CLASSES)\n",
    "\n",
    "        elif model_type == \"B3\":\n",
    "            model = SegFormer_B3(input_shape=input_shape, num_classes=NUM_CLASSES)\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model_type: {model_type}\")\n",
    "\n",
    "    else:\n",
    "        custom_objects={\n",
    "            'DiceLoss': sm.losses.DiceLoss,\n",
    "            'CategoricalFocalLoss': sm.losses.CategoricalFocalLoss,\n",
    "            'MeanIoU': MeanIoUMetric\n",
    "        }  \n",
    "        model = tf.keras.models.load_model(\n",
    "            model_path,\n",
    "            custom_objects=custom_objects,\n",
    "            compile=True\n",
    "        )\n",
    "        for layer in model.layers:\n",
    "            layer.trainable = True\n",
    "\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    for i, layer in enumerate(model.layers):\n",
    "        print(i, layer.name)\n",
    "\n",
    "\n",
    "    # --- Callbacks --- \n",
    "    #LearningRateLogger()\n",
    "    #monitor = \"val_iou_score\"\n",
    "    monitor = \"val_loss\"\n",
    "    nan_terminate = TerminateOnNaN()\n",
    "    time_limit = TimeLimitCallback(max_minutes=train_time)\n",
    "    early_stop = EarlyStopping(monitor=monitor, mode=\"max\", patience=20, restore_best_weights=True, verbose=1)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor=monitor, mode=\"max\", patience=6, min_lr=5e-7, factor=0.5, verbose=1, min_delta=1e-4)\n",
    "\n",
    "    lr_schedule = TransformerLRSchedule(d_model=tile_size, warmup_steps=2048)\n",
    "    callbacks = [\n",
    "        time_limit,\n",
    "        early_stop,\n",
    "        nan_terminate, \n",
    "        StepTimer(),\n",
    "    ]\n",
    "\n",
    "    metrics = [\n",
    "        sm.metrics.IOUScore(threshold=None, name=\"iou_score\"),   # fast, approximated mIoU per batch\n",
    "        sm.metrics.FScore(threshold=None, name=\"f1-score\"),\n",
    "        tf.keras.metrics.CategoricalAccuracy(name=\"categorical_accuracy\"),\n",
    "    ]\n",
    "\n",
    "\n",
    "    loss_weights = [0.25, 1.5, 1.0]                 # [cce, dice, focal]\n",
    "    class_weights = [1.1, 1.0, 1.0, 2.0, 0.0, 1.3]  # [building, clutter, vegetation, water, background, car]\n",
    "    label_smoothing = 0.1\n",
    "\n",
    "    raw_dice = sm.losses.DiceLoss(class_weights=class_weights)\n",
    "    raw_focal = sm.losses.CategoricalFocalLoss()\n",
    "    raw_cce = CategoricalCrossentropy()\n",
    "\n",
    "\n",
    "    def total_loss_with_smoothing(y_true: tf.Tensor, y_pred: tf.Tensor) -> tf.Tensor:\n",
    "        \"\"\"Computes the total weighted loss from CCE, Dice, and Focal losses.\"\"\"\n",
    "        y_true_smoothed = apply_label_smoothing(y_true, smoothing=label_smoothing)\n",
    "\n",
    "        dice = raw_dice(y_true_smoothed, y_pred)\n",
    "        focal = raw_focal(y_true_smoothed, y_pred)\n",
    "        cce = raw_cce(y_true_smoothed, y_pred)\n",
    "\n",
    "        return loss_weights[0] * cce + loss_weights[1] * dice + loss_weights[2] * focal\n",
    "\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=lr_schedule, beta_1=0.9, beta_2=0.98, epsilon=1e-9),\n",
    "        loss=total_loss_with_smoothing,\n",
    "        metrics=metrics\n",
    "    )\n",
    "    \n",
    "    history = model.fit(\n",
    "        train_gen, validation_data=val_gen,\n",
    "        epochs=epochs,\n",
    "        callbacks=callbacks,\n",
    "        verbose=verbose\n",
    "    )\n",
    "\n",
    "\n",
    "    # --- Evaluate Model ---\n",
    "    plot_training_curves(history, out_dir)\n",
    "    evaluate_on_test(model, test_gen, test_df, \"/content/figs\", img_dir, label_dir, tile_size, n_rows=4, n_cols=3) \n",
    "    measure_inference_time(model, test_gen, num_batches=5)\n",
    " \n",
    "\n",
    "    # --- Full-Tile Reconstruction (Optional) ---\n",
    "    if yummy:\n",
    "        for tile_prefix in test_files:\n",
    "            img, label, pred = reconstruct_canvas(\n",
    "                model,\n",
    "                test_df,\n",
    "                tile_prefix,\n",
    "                build_tf_dataset,\n",
    "                img_dir,\n",
    "                elev_dir,\n",
    "                slope_dir,\n",
    "                label_dir\n",
    "            )\n",
    "            plot_reconstruction(img, label, pred, tile_prefix)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPw4dXqbrOaTut/+CDo+nZ1",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
