{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TerminateOnNaN\n",
    "import cv2\n",
    "import itertools\n",
    "import gc\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.schedules import LearningRateSchedule\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "import segmentation_models as sm\n",
    "from collections import defaultdict\n",
    "from tensorflow.keras.metrics import MeanIoU\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras import backend as K\n",
    "K.clear_session()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class TransformerLRSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    \"\"\"Custom learning rate schedule based on the Transformer paper.\n",
    "\n",
    "    The schedule increases the learning rate linearly for the first `warmup_steps`,\n",
    "    and then decreases it proportionally to the inverse square root of the step number.\n",
    "\n",
    "    This is commonly used in training Transformer models.\n",
    "\n",
    "    Attributes:\n",
    "        d_model (tf.Tensor): The dimensionality of the model.\n",
    "        warmup_steps (tf.Tensor): Number of steps to linearly increase the learning rate.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int, warmup_steps: int = 4000):\n",
    "        \"\"\"Initialises the TransformerLRSchedule.\n",
    "\n",
    "        Args:\n",
    "            d_model (int): The model dimensionality (e.g., 512).\n",
    "            warmup_steps (int): Number of warm-up steps. Default is 4000.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "        self.warmup_steps = tf.cast(warmup_steps, tf.float32)\n",
    "\n",
    "    def __call__(self, step: tf.Tensor) -> tf.Tensor:\n",
    "        \"\"\"Computes the learning rate at a given training step.\n",
    "\n",
    "        Args:\n",
    "            step (tf.Tensor): The current training step.\n",
    "\n",
    "        Returns:\n",
    "            tf.Tensor: The calculated learning rate for this step.\n",
    "        \"\"\"\n",
    "        step = tf.cast(step, tf.float32)\n",
    "\n",
    "        # Inverse square root decay and warmup scaling\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * tf.pow(self.warmup_steps, -1.5)\n",
    "\n",
    "        # Apply the min schedule\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "    def get_config(self) -> dict:\n",
    "        \"\"\"Returns the config of the learning rate schedule for serialization.\n",
    "\n",
    "        Returns:\n",
    "            dict: Configuration dictionary with d_model and warmup_steps.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"d_model\": self.d_model.numpy(),\n",
    "            \"warmup_steps\": self.warmup_steps.numpy(),\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "INPUT_TYPE_CONFIG = {\n",
    "    \"rgb\": {\"description\": \"RGB only\", \"channels\": 3},\n",
    "    \"rgb_elev\": {\"description\": \"RGB + elevation\", \"channels\": 5}\n",
    "}\n",
    "\n",
    "COLOR_TO_CLASS = {\n",
    "    (230, 25, 75): 0,\n",
    "    (145, 30, 180): 1,\n",
    "    (60, 180, 75): 2,\n",
    "    (245, 130, 48): 3,\n",
    "    (255, 255, 255): 4,\n",
    "    (0, 130, 200): 5,\n",
    "    (255, 0, 255): 6 # Ignore pixel for visualisation\n",
    "}\n",
    "\n",
    "CLASS_TO_COLOR = {v: k for k, v in COLOR_TO_CLASS.items() if v < 6}  # Exclude ignore class\n",
    "NUM_CLASSES = 6\n",
    "COLOR_PALETTE = np.array(list(COLOR_TO_CLASS.keys()), dtype=np.uint8)\n",
    "COLOR_LOOKUP = {tuple(c): i for c, i in COLOR_TO_CLASS.items()}\n",
    "\n",
    "def decode_label_image(label_img: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Converts a colour-coded label image into a class ID map.\n",
    "\n",
    "    Args:\n",
    "        label_img (np.ndarray): A (H, W, 3) RGB label image where each unique colour\n",
    "            represents a class, and colours are defined in COLOR_LOOKUP.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A (H, W) array of class IDs corresponding to each pixel.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If an unknown colour is encountered in the label image.\n",
    "    \"\"\"\n",
    "    h, w, _ = label_img.shape\n",
    "    label_map = np.zeros((h, w), dtype=np.uint8)\n",
    "\n",
    "    for y in range(h):\n",
    "        for x in range(w):\n",
    "            pixel = tuple(label_img[y, x])\n",
    "            if pixel not in COLOR_LOOKUP:\n",
    "                raise ValueError(f\"Unknown label colour {pixel} at ({y}, {x})\")\n",
    "            label_map[y, x] = COLOR_LOOKUP[pixel]\n",
    "\n",
    "    return label_map\n",
    "\n",
    "\n",
    "\n",
    "def plot_augmented_grid_from_dataset(\n",
    "    tf_dataset: tf.data.Dataset,\n",
    "    input_type: str,  # 'rgb' or 'rgb_elev'\n",
    "    n_rows: int = 3,\n",
    "    n_cols: int = 4,\n",
    "    title: str = \"Augmented Training Chips\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Plots a grid of RGB + label masks from a tf.data.Dataset, using CLASS_TO_COLOR for display.\n",
    "    Layout matches the style of visualise_prediction_grid, but without predictions.\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "\n",
    "    print(f\"Fetching one batch for {n_rows * n_cols} RGB + label pairs...\")\n",
    "\n",
    "    try:\n",
    "        batch = next(iter(tf_dataset.take(1)), None)\n",
    "\n",
    "        if batch is None:\n",
    "            print(\"Warning: Dataset is empty. Skipping plot.\")\n",
    "            return\n",
    "\n",
    "        images, labels_one_hot = batch\n",
    "        rgb_images = images[:, :, :, :3].numpy() if input_type == 'rgb_elev' else images.numpy()\n",
    "        rgb_images = np.clip(rgb_images, 0.0, 1.0)\n",
    "\n",
    "        label_ids = tf.argmax(labels_one_hot, axis=-1).numpy()\n",
    "        ignore_mask = tf.reduce_all(labels_one_hot == 0, axis=-1).numpy()\n",
    "\n",
    "        total = n_rows * n_cols\n",
    "        batch_size = rgb_images.shape[0]\n",
    "        total = min(total, batch_size)\n",
    "\n",
    "        fig, axs = plt.subplots(n_rows, n_cols * 2, figsize=(n_cols * 5.6, n_rows * 2.6))\n",
    "\n",
    "        for i in range(total):\n",
    "            rgb = rgb_images[i]\n",
    "            mask = label_ids[i]\n",
    "            ignore = ignore_mask[i]\n",
    "\n",
    "            # Create RGB mask image from class IDs\n",
    "            label_rgb = np.zeros((*mask.shape, 3), dtype=np.uint8)\n",
    "            for class_id, color in CLASS_TO_COLOR.items():\n",
    "                label_rgb[mask == class_id] = color\n",
    "            label_rgb[ignore] = (255, 0, 255)  # Magenta for ignored pixels\n",
    "\n",
    "            row = i // n_cols\n",
    "            col = (i % n_cols) * 2\n",
    "\n",
    "            axs[row, col].imshow(rgb)\n",
    "            axs[row, col].set_title(\"RGB\")\n",
    "            axs[row, col].axis(\"off\")\n",
    "\n",
    "            axs[row, col + 1].imshow(label_rgb)\n",
    "            axs[row, col + 1].set_title(\"Label\")\n",
    "            axs[row, col + 1].axis(\"off\")\n",
    "\n",
    "        # Hide any unused axes\n",
    "        for j in range(total, n_rows * n_cols):\n",
    "            row = j // n_cols\n",
    "            col = (j % n_cols) * 2\n",
    "            axs[row, col].axis(\"off\")\n",
    "            axs[row, col + 1].axis(\"off\")\n",
    "\n",
    "        #plt.suptitle(title, fontsize=16, y=1.02)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        plt.close(fig)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during plotting: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "# --- Metrics ---\n",
    "# Loss function with label smoothing\n",
    "def apply_label_smoothing(y_true, smoothing=0.1):\n",
    "    num_classes = tf.cast(tf.shape(y_true)[-1], tf.float32)\n",
    "    return y_true * (1.0 - smoothing) + (smoothing / num_classes)\n",
    "\n",
    "os.environ[\"SM_FRAMEWORK\"] = \"tf.keras\"\n",
    "\n",
    "# --- Jaccard Index ---\n",
    "class MeanIoUMetric(tf.keras.metrics.MeanIoU):\n",
    "    def __init__(self, num_classes, name=\"mean_iou\", dtype=None):\n",
    "        super().__init__(num_classes=num_classes, name=name, dtype=dtype)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.argmax(y_true, axis=-1)\n",
    "        y_pred = tf.argmax(y_pred, axis=-1)\n",
    "        return super().update_state(y_true, y_pred, sample_weight)\n",
    "\n",
    "miou_metric = MeanIoUMetric(num_classes=6)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- Model Building ---\n",
    "def train_unet(\n",
    "        base_dir=\"/content/chipped_data/content/chipped_data\", out_dir=\"/content/figs\", \n",
    "        input_type=\"rgb_elev\", model_type=\"enhanced_unet\", tile_size=256,\n",
    "        batch_size=8, epochs=50, train_time=20, verbose=1, yummy=False, model_path=None,\n",
    "    ):\n",
    "    \n",
    "\n",
    "    assert input_type in INPUT_TYPE_CONFIG, f\"Unknown input type: {input_type}\"\n",
    "    num_channels = INPUT_TYPE_CONFIG[input_type][\"channels\"]\n",
    "\n",
    "    img_dir = os.path.join(base_dir, \"train\", \"images\")\n",
    "    elev_dir = os.path.join(base_dir, \"train\", \"elevations\")\n",
    "    slope_dir = os.path.join(base_dir, \"train\", \"slopes\")\n",
    "    label_dir = os.path.join(base_dir, \"train\", \"labels\")\n",
    "\n",
    "    # Load metadata and define input shape\n",
    "    input_shape = (tile_size, tile_size, num_channels)\n",
    "    train_df = csv_to_df('train', subset=1.0)\n",
    "    val_df = csv_to_df('val')\n",
    "    test_df = csv_to_df('test')\n",
    "\n",
    "    # --- Streaming Data Generator ---\n",
    "    train_gen = build_tf_dataset(train_df, img_dir, elev_dir, slope_dir, label_dir,\n",
    "                                 input_type=input_type, split='train',\n",
    "                                 augment=True, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "    val_gen = build_tf_dataset(val_df, img_dir, elev_dir, slope_dir, label_dir,\n",
    "                                input_type=input_type, split='val',\n",
    "                                augment=False, shuffle=False, batch_size=batch_size)\n",
    "    \n",
    "    test_gen = build_tf_dataset(test_df, img_dir, elev_dir, slope_dir, label_dir,\n",
    "                            input_type=input_type, split='test',\n",
    "                            augment=False, shuffle=False, batch_size=batch_size)\n",
    "\n",
    "\n",
    "\n",
    "    # To visualize augmented training examples:\n",
    "    plot_augmented_grid_from_dataset(\n",
    "        tf_dataset=train_gen,\n",
    "        input_type='rgb', \n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    # --- Model ---\n",
    "    import segmentation_models as sm\n",
    "    if model_path is None or not os.path.exists(model_path):\n",
    "        if model_type == \"unet\":\n",
    "            model = build_unet(input_shape=input_shape, num_classes=NUM_CLASSES)\n",
    "        \n",
    "        elif model_type == \"new_unet\":\n",
    "            model, base_model = build_flexible_unet(input_shape=input_shape, num_classes=NUM_CLASSES, freeze_rgb_encoder=False)\n",
    "\n",
    "        elif model_type == \"multi_unet\":\n",
    "            model = build_multi_unet(input_shape=input_shape, num_classes=NUM_CLASSES)\n",
    "\n",
    "        elif model_type == \"enhanced_unet\":\n",
    "            model = enhanced_unet(input_shape=input_shape, num_classes=NUM_CLASSES)\n",
    "\n",
    "        elif model_type == \"resnet34\":\n",
    "            model = sm.Unet(\n",
    "                backbone_name=\"resnet34\",               # or 'efficientnetb0', 'mobilenetv2', etc.\n",
    "                input_shape=input_shape,\n",
    "                classes=NUM_CLASSES,                  \n",
    "                activation='softmax', \n",
    "                encoder_weights='imagenet'              # Load ImageNet pre-trained weights\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model_type: {model_type}\")\n",
    "    else:\n",
    "        custom_objects={\n",
    "            'DiceLoss': sm.losses.DiceLoss,\n",
    "            'CategoricalFocalLoss': sm.losses.CategoricalFocalLoss,\n",
    "            'MeanIoU': MeanIoUMetric\n",
    "        }  \n",
    "\n",
    "        model = tf.keras.models.load_model(\n",
    "            model_path,\n",
    "            custom_objects=custom_objects,\n",
    "            compile=True\n",
    "        )\n",
    "\n",
    "        for layer in model.layers:\n",
    "            layer.trainable = True\n",
    "\n",
    "    model.summary()\n",
    "    print(f\"Number of Parameters: {model.count_params()}\\n\"\n",
    "          f\"Number of Layers: {len(model.layers)}\\n\")\n",
    "\n",
    "    '''\n",
    "    for i, layer in enumerate(model.layers):\n",
    "        print(i, layer.name)\n",
    "    '''\n",
    "\n",
    "    # --- Callbacks --- \n",
    "    monitor = \"val_iou_score\"       # \"val_loss\"\n",
    "    nan_terminate = TerminateOnNaN()\n",
    "    time_limit = TimeLimitCallback(max_minutes=train_time)\n",
    "    early_stop = EarlyStopping(monitor=monitor, mode=\"max\", patience=10, restore_best_weights=True, verbose=1)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor=monitor, mode=\"max\", patience=5, min_lr=5e-7, factor=0.5, verbose=1, min_delta=1e-4)\n",
    "    weight_callback = DynamicClassWeightUpdater(val_data=val_gen, update_every=5, target='iou', ignore_class=4)\n",
    "    #LearningRateLogger()\n",
    "\n",
    "    callbacks = [\n",
    "        reduce_lr,\n",
    "        time_limit,\n",
    "        early_stop,\n",
    "        nan_terminate, \n",
    "        StepTimer(),\n",
    "    ]\n",
    "\n",
    "    metrics = [\n",
    "        sm.metrics.IOUScore(threshold=None, name=\"iou_score\"),   # fast, approximated mIoU per batch\n",
    "        sm.metrics.FScore(threshold=None, name=\"f1-score\"),\n",
    "        tf.keras.metrics.CategoricalAccuracy(name=\"categorical_accuracy\"),\n",
    "    ]\n",
    "\n",
    "    learning_rate = 5.2e-4\n",
    "    label_smoothing = 0.01\n",
    "    loss_weights = [0.25, 1.5, 2.2]                    # [cce, dice, focal]\n",
    "    class_weights = [4.95, 3.2, 1.0, 11.6, 3.0, 2.6]              # [building, clutter, vegetation, water, background, car]\n",
    "    \n",
    "    # Normalize class weights\n",
    "    total = sum(class_weights)\n",
    "    norm_class_weights = [w / total for w in class_weights]\n",
    "\n",
    "\n",
    "    #focal_alpha = 0.25\n",
    "    focal_gamma = 3.5\n",
    "    raw_dice = sm.losses.DiceLoss(class_weights=norm_class_weights)\n",
    "    raw_focal = sm.losses.CategoricalFocalLoss(\n",
    "        alpha=norm_class_weights,\n",
    "        gamma=focal_gamma,\n",
    "    )\n",
    "    raw_cce = CategoricalCrossentropy()\n",
    "\n",
    "    optimizer = mixed_precision.LossScaleOptimizer(\n",
    "        Adam(learning_rate=learning_rate), dynamic=True\n",
    "    )\n",
    "\n",
    "\n",
    "    def total_loss(y_true, y_pred):\n",
    "        y_true_smoothed = apply_label_smoothing(y_true, smoothing=label_smoothing)\n",
    "\n",
    "        dice = raw_dice(y_true_smoothed, y_pred)\n",
    "        focal = raw_focal(y_true_smoothed, y_pred)\n",
    "        cce = raw_cce(y_true_smoothed, y_pred)\n",
    "\n",
    "        base_loss = (\n",
    "            loss_weights[0] * cce +\n",
    "            loss_weights[1] * dice +\n",
    "            loss_weights[2] * focal\n",
    "        )\n",
    "\n",
    "        #return apply_ignore_class_mask(y_true_smoothed, y_pred, ignore_class=4, loss_fn=lambda yt, yp: base_loss)\n",
    "        return base_loss\n",
    "\n",
    "\n",
    "\n",
    "    # --- Train Model ---\\\n",
    "    import time\n",
    "    from datetime import datetime\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    print(\"🕒 Start Time:\", timestamp)\n",
    "\n",
    "    # ⏱️ Start training timer\n",
    "    start_time = time.time()\n",
    "\n",
    "\n",
    "\n",
    "    # Single Stage Training\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=total_loss,\n",
    "        metrics=metrics\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        train_gen, validation_data=val_gen,\n",
    "        epochs=epochs,\n",
    "        callbacks=callbacks,\n",
    "        verbose=verbose\n",
    "    )\n",
    "\n",
    "    # --- Save Model ---\n",
    "    os.makedirs(\"/content/figs\", exist_ok=True)\n",
    "    model.save(\"/content/figs/segmentation_model.keras\")\n",
    "\n",
    "\n",
    "    # ⏱️ End training timer\n",
    "    end_time = time.time()\n",
    "    duration_sec = end_time - start_time\n",
    "    duration_str = time.strftime('%H:%M:%S', time.gmtime(duration_sec))\n",
    "    print(f\"\\n✅ Training complete in {duration_str} ({duration_sec:.2f} seconds)\")\n",
    "\n",
    "\n",
    " \n",
    "    '''\n",
    "    # Two Stage Training\n",
    "    if fine_tune:\n",
    "        hard_df = csv_to_hard_df()\n",
    "        train_hard = build_tf_dataset(hard_df, img_dir, elev_dir, slope_dir, label_dir,\n",
    "                                    input_type=input_type, split='train',\n",
    "                                    augment=True, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "\n",
    "        print(\"Training on hard examples...\")\n",
    "        optimizer = mixed_precision.LossScaleOptimizer(\n",
    "            Adam(learning_rate=1e-5), dynamic=True\n",
    "        )\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss=total_loss,\n",
    "            metrics=metrics\n",
    "        )\n",
    "\n",
    "        history = model.fit(\n",
    "            train_hard, validation_data=val_gen,\n",
    "            epochs=history.epoch[-1] + 16,\n",
    "            initial_epoch=history.epoch[-1] + 1,\n",
    "            callbacks=callbacks,\n",
    "            verbose=verbose\n",
    "        )\n",
    "\n",
    "    '''\n",
    "\n",
    "\n",
    "    def safe_get(var_name, default=\"❌ Not Defined\"):\n",
    "        return globals().get(var_name, default)\n",
    "\n",
    "    def safe_get_history(history_dict, key):\n",
    "        return history_dict.get(key, [\"N/A\"])[-1] if key in history_dict else \"N/A\"\n",
    "\n",
    "    # --- Evaluate Model ---\n",
    "    print(f\"Initial Learning Rate: {safe_get('learning_rate')}\\n\"\n",
    "        f\"Loss Weights: {safe_get('loss_weights')}, Class Weights: {safe_get('class_weights')}\\n\"\n",
    "        f\"Focal Loss Gamma: {safe_get('focal_gamma')}\\n\"\n",
    "        f\"Label Smoothing: {safe_get('label_smoothing')}\\n\"\n",
    "        f\"Input Type: {safe_get('input_type')}, Model Type: {safe_get('model_type')}\\n\"\n",
    "        f\"Batch Size: {safe_get('batch_size')}, \"\n",
    "        f\"Epochs: {history.epoch[-1] + 1 if 'history' in globals() else 'N/A'}\\n\"\n",
    "        f\"Number of Parameters: {model.count_params() if 'model' in globals() else 'N/A'}, \"\n",
    "        f\"Number of Layers: {len(model.layers) if 'model' in globals() else 'N/A'}\\n\"\n",
    "        f\"Final Validation Loss: {safe_get_history(history.history, 'val_loss'):.4f}\\n\"\n",
    "        f\"Final Validation mIoU: {safe_get_history(history.history, 'val_iou_score'):.4f}\\n\"\n",
    "        f\"Final Validation F1 Score: {safe_get_history(history.history, 'val_f1-score'):.4f}\\n\"\n",
    "        f\"Final Validation Categorical Accuracy: {safe_get_history(history.history, 'categorical_accuracy'):.4f}\\n\")\n",
    "\n",
    "\n",
    "    measure_inference_time(model, test_gen, num_batches=5)\n",
    "    plot_training_curves(history, out_dir)\n",
    "    evaluate_on_test(model, test_gen, test_df, \"/content/figs\", img_dir, label_dir, tile_size, n_rows=4, n_cols=3)\n",
    "\n",
    "\n",
    "     \n",
    "\n",
    "    test_files = [\n",
    "        \"25f1c24f30_EB81FE6E2BOPENPIPELINE\",\n",
    "        \"1d4fbe33f3_F1BE1D4184INSPIRE\",\n",
    "        \"15efe45820_D95DF0B1F4INSPIRE\",\n",
    "        \"c6d131e346_536DE05ED2OPENPIPELINE\",\n",
    "        \"12fa5e614f_53197F206FOPENPIPELINE\",\n",
    "        \"5fa39d6378_DB9FF730D9OPENPIPELINE\",\n",
    "        \"ebffe540d0_7BA042D858OPENPIPELINE\",\n",
    "        \"8710b98ea0_06E6522D6DINSPIRE\",\n",
    "        \"84410645db_8D20F02042OPENPIPELINE\",\n",
    "        \"a1af86939f_F1BE1D4184OPENPIPELINE\"\n",
    "    ]\n",
    "\n",
    "\n",
    "    # --- Gangster Shit ---\n",
    "    if yummy:\n",
    "        for tile_prefix in test_files:\n",
    "            img, label, pred = reconstruct_canvas(\n",
    "                model,\n",
    "                test_df,\n",
    "                tile_prefix,\n",
    "                build_tf_dataset,\n",
    "                img_dir,\n",
    "                elev_dir,\n",
    "                slope_dir,\n",
    "                label_dir\n",
    "            )\n",
    "            plot_reconstruction(img, label, pred, tile_prefix)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_segformer(\n",
    "        base_dir=\"/content/chipped_data/content/chipped_data\", out_dir=\"/content/figs\", \n",
    "        input_type=\"rgb_elev\", model_type=\"ENHANCED_unet\", tile_size=256,\n",
    "        batch_size=8, epochs=50, train_time=20, verbose=1, yummy=False, model_path=None,\n",
    "    ):\n",
    "    \n",
    "    assert input_type in INPUT_TYPE_CONFIG, f\"Unknown input type: {input_type}\"\n",
    "    num_channels = INPUT_TYPE_CONFIG[input_type][\"channels\"]\n",
    "\n",
    "    img_dir = os.path.join(base_dir, \"train\", \"images\")\n",
    "    elev_dir = os.path.join(base_dir, \"train\", \"elevations\")\n",
    "    slope_dir = os.path.join(base_dir, \"train\", \"slopes\")\n",
    "    label_dir = os.path.join(base_dir, \"train\", \"labels\")\n",
    "\n",
    "    # Load metadata and define input shape\n",
    "    input_shape = (tile_size, tile_size, num_channels)\n",
    "    train_df = csv_to_df('train', 0.4)\n",
    "    val_df = csv_to_df('val')\n",
    "    test_df = csv_to_df('test')\n",
    "\n",
    "\n",
    "    # --- Streaming Data Generator ---\n",
    "    train_gen = build_tf_dataset(train_df, img_dir, elev_dir, slope_dir, label_dir,\n",
    "                                 input_type=input_type, split='train',\n",
    "                                 augment=True, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "    val_gen = build_tf_dataset(val_df, img_dir, elev_dir, slope_dir, label_dir,\n",
    "                                input_type=input_type, split='val',\n",
    "                                augment=False, shuffle=False, batch_size=batch_size)\n",
    "    \n",
    "    test_gen = build_tf_dataset(test_df, img_dir, elev_dir, slope_dir, label_dir,\n",
    "                            input_type=input_type, split='test',\n",
    "                            augment=False, shuffle=False, batch_size=batch_size)\n",
    "\n",
    "    for x_batch, y_batch in test_gen.take(1):\n",
    "        y_np = np.argmax(y_batch.numpy(), axis=-1)\n",
    "        print(\"Unique labels in y batch:\", np.unique(y_np))\n",
    "\n",
    "\n",
    "    # --- Model ---\n",
    "    import segmentation_models as sm\n",
    "    if model_path is None or not os.path.exists(model_path):\n",
    "\n",
    "        if model_type == \"B2\":\n",
    "            model = SegFormer_B2(input_shape=input_shape, num_classes=NUM_CLASSES)\n",
    "\n",
    "        elif model_type == \"B0\":\n",
    "            model = SegFormer_B0(input_shape=input_shape, num_classes=NUM_CLASSES)\n",
    "        \n",
    "        elif model_type == \"B5\":\n",
    "            model = SegFormer_B5(input_shape=input_shape, num_classes=NUM_CLASSES)\n",
    "\n",
    "        elif model_type == \"B4\":\n",
    "            model = SegFormer_B4(input_shape=input_shape, num_classes=NUM_CLASSES)\n",
    "\n",
    "        elif model_type == \"B1\":\n",
    "            model = SegFormer_B1(input_shape=input_shape, num_classes=NUM_CLASSES)\n",
    "\n",
    "        elif model_type == \"B3\":\n",
    "            model = SegFormer_B3(input_shape=input_shape, num_classes=NUM_CLASSES)\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model_type: {model_type}\")\n",
    "\n",
    "    else:\n",
    "        custom_objects={\n",
    "            'DiceLoss': sm.losses.DiceLoss,\n",
    "            'CategoricalFocalLoss': sm.losses.CategoricalFocalLoss,\n",
    "            'MeanIoU': MeanIoUMetric\n",
    "        }  \n",
    "        model = tf.keras.models.load_model(\n",
    "            model_path,\n",
    "            custom_objects=custom_objects,\n",
    "            compile=True\n",
    "        )\n",
    "        for layer in model.layers:\n",
    "            layer.trainable = True\n",
    "\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    for i, layer in enumerate(model.layers):\n",
    "        print(i, layer.name)\n",
    "\n",
    "\n",
    "    # --- Callbacks --- \n",
    "    #LearningRateLogger()\n",
    "    #monitor = \"val_iou_score\"\n",
    "    monitor = \"val_loss\"\n",
    "    nan_terminate = TerminateOnNaN()\n",
    "    time_limit = TimeLimitCallback(max_minutes=train_time)\n",
    "    early_stop = EarlyStopping(monitor=monitor, mode=\"max\", patience=20, restore_best_weights=True, verbose=1)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor=monitor, mode=\"max\", patience=6, min_lr=5e-7, factor=0.5, verbose=1, min_delta=1e-4)\n",
    "\n",
    "    lr_schedule = TransformerLRSchedule(d_model=tile_size, warmup_steps=2048)\n",
    "    callbacks = [\n",
    "        time_limit,\n",
    "        early_stop,\n",
    "        nan_terminate, \n",
    "        StepTimer(),\n",
    "    ]\n",
    "\n",
    "    metrics = [\n",
    "        sm.metrics.IOUScore(threshold=None, name=\"iou_score\"),   # fast, approximated mIoU per batch\n",
    "        sm.metrics.FScore(threshold=None, name=\"f1-score\"),\n",
    "        tf.keras.metrics.CategoricalAccuracy(name=\"categorical_accuracy\"),\n",
    "    ]\n",
    "\n",
    "\n",
    "    loss_weights = [0.25, 1.5, 1.0]                 # [cce, dice, focal]\n",
    "    class_weights = [1.1, 1.0, 1.0, 2.0, 0.0, 1.3]  # [building, clutter, vegetation, water, background, car]\n",
    "    label_smoothing = 0.1\n",
    "\n",
    "    raw_dice = sm.losses.DiceLoss(class_weights=class_weights)\n",
    "    raw_focal = sm.losses.CategoricalFocalLoss()\n",
    "    raw_cce = CategoricalCrossentropy()\n",
    "\n",
    "\n",
    "    def total_loss_with_smoothing(y_true, y_pred):\n",
    "        y_true_smoothed = apply_label_smoothing(y_true, smoothing=label_smoothing)\n",
    "\n",
    "        dice = raw_dice(y_true_smoothed, y_pred)\n",
    "        focal = raw_focal(y_true_smoothed, y_pred)\n",
    "        cce = raw_cce(y_true_smoothed, y_pred)\n",
    "\n",
    "        return loss_weights[0] * cce + loss_weights[1] * dice + loss_weights[2] * focal\n",
    "\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=lr_schedule, beta_1=0.9, beta_2=0.98, epsilon=1e-9),\n",
    "        loss=total_loss_with_smoothing,\n",
    "        metrics=metrics\n",
    "    )\n",
    "    \n",
    "    history = model.fit(\n",
    "        train_gen, validation_data=val_gen,\n",
    "        epochs=epochs,\n",
    "        callbacks=callbacks,\n",
    "        verbose=verbose\n",
    "    )\n",
    "\n",
    "\n",
    "    # --- Evaluate Model ---\n",
    "    plot_training_curves(history, out_dir)\n",
    "    evaluate_on_test(model, test_gen, test_df, \"/content/figs\", img_dir, label_dir, tile_size, n_rows=4, n_cols=3) \n",
    "    measure_inference_time(model, test_gen, num_batches=5)\n",
    " \n",
    "\n",
    "    # --- Gangster Shit ---\n",
    "    if yummy:\n",
    "        for tile_prefix in test_files:\n",
    "            img, label, pred = reconstruct_canvas(\n",
    "                model,\n",
    "                test_df,\n",
    "                tile_prefix,\n",
    "                build_tf_dataset,\n",
    "                img_dir,\n",
    "                elev_dir,\n",
    "                slope_dir,\n",
    "                label_dir\n",
    "            )\n",
    "            plot_reconstruction(img, label, pred, tile_prefix)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPw4dXqbrOaTut/+CDo+nZ1",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
