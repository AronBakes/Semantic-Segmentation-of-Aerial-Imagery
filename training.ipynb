{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TerminateOnNaN\n",
    "import cv2\n",
    "import itertools\n",
    "import gc\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.schedules import LearningRateSchedule\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "import segmentation_models as sm\n",
    "from collections import defaultdict\n",
    "\n",
    "from tensorflow.keras.metrics import MeanIoU\n",
    "\n",
    "\n",
    "INPUT_TYPE_CONFIG = {\n",
    "    \"rgb\": {\"description\": \"RGB only\", \"channels\": 3},\n",
    "    \"rgb_elev\": {\"description\": \"RGB + elevation\", \"channels\": 4}\n",
    "}\n",
    "\n",
    "COLOR_TO_CLASS = {\n",
    "    (230, 25, 75): 0,\n",
    "    (145, 30, 180): 1,\n",
    "    (60, 180, 75): 2,\n",
    "    (245, 130, 48): 3,\n",
    "    (255, 255, 255): 4,\n",
    "    (0, 130, 200): 5,\n",
    "    (255, 0, 255): 6 # Ignore pixel for visualisation\n",
    "}\n",
    "\n",
    "CLASS_TO_COLOR = {v: k for k, v in COLOR_TO_CLASS.items() if v < 6}  # Exclude ignore class\n",
    "NUM_CLASSES = 6\n",
    "COLOR_PALETTE = np.array(list(COLOR_TO_CLASS.keys()), dtype=np.uint8)\n",
    "COLOR_LOOKUP = {tuple(c): i for c, i in COLOR_TO_CLASS.items()}\n",
    "\n",
    "def decode_label_image(label_img):\n",
    "    h, w, _ = label_img.shape\n",
    "    label_map = np.zeros((h, w), dtype=np.uint8)\n",
    "    for y in range(h):\n",
    "        for x in range(w):\n",
    "            pixel = tuple(label_img[y, x])\n",
    "            if pixel not in COLOR_LOOKUP:\n",
    "                raise ValueError(f\"❌ Unknown label colour {pixel} at ({y}, {x})\")\n",
    "            label_map[y, x] = COLOR_LOOKUP[pixel]\n",
    "    return label_map\n",
    "\n",
    "\n",
    "def filter_tile_ids_by_substring(image_dir, base_names):\n",
    "    return [f.replace('-ortho.png', '') for f in os.listdir(image_dir) if any(base in f for base in base_names)]\n",
    "\n",
    "\n",
    "def measure_inference_time(model, generator, num_batches=5):\n",
    "    import time\n",
    "    total_time = 0\n",
    "    total_images = 0\n",
    "\n",
    "    if num_batches is None:\n",
    "        num_batches = tf.data.experimental.cardinality(generator).numpy()\n",
    "\n",
    "    for i, (x_batch, _) in enumerate(generator.take(num_batches)):\n",
    "        start = time.time()\n",
    "        _ = model.predict(x_batch, verbose=0)\n",
    "        end = time.time()\n",
    "        total_time += (end - start)\n",
    "        total_images += x_batch.shape[0]\n",
    "\n",
    "    print(f\"🧠 Inference time: {total_time:.2f} sec for {total_images} images\")\n",
    "    print(f\"⏱️ Avg inference time per image: {total_time / total_images:.4f} sec\")\n",
    "\n",
    "\n",
    "\n",
    "def plot_training_curves(history, out_dir):\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "\n",
    "    history_dict = history.history\n",
    "    required_keys = [\"loss\", \"val_loss\", \"iou_score\", \"val_iou_score\"]\n",
    "\n",
    "    missing_keys = [k for k in required_keys if k not in history_dict]\n",
    "    if missing_keys:\n",
    "        print(f\"⚠️ Missing keys in history: {missing_keys}\")\n",
    "        return\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    # Loss\n",
    "    axs[0].plot(history_dict[\"loss\"], label=\"Train Loss\")\n",
    "    axs[0].plot(history_dict[\"val_loss\"], label=\"Val Loss\")\n",
    "    axs[0].set_title(\"Loss over Epochs\")\n",
    "    axs[0].set_xlabel(\"Epoch\")\n",
    "    axs[0].set_ylabel(\"Loss\")\n",
    "    axs[0].legend()\n",
    "\n",
    "    # IoU\n",
    "    axs[1].plot(history_dict[\"iou_score\"], label=\"Train IoU\")\n",
    "    axs[1].plot(history_dict[\"val_iou_score\"], label=\"Val IoU\")\n",
    "    axs[1].set_title(\"Mean IoU over Epochs\")\n",
    "    axs[1].set_xlabel(\"Epoch\")\n",
    "    axs[1].set_ylabel(\"Mean IoU\")\n",
    "    axs[1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    save_path = os.path.join(out_dir, \"training_curves.png\")\n",
    "    plt.savefig(save_path)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    print(f\"✅ Saved training curves to: {save_path}\")\n",
    "\n",
    "\n",
    "# --- Metrics ---\n",
    "class TransformerLRSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super().__init__()\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "        self.warmup_steps = tf.cast(warmup_steps, tf.float32)\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, tf.float32)\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * tf.pow(self.warmup_steps, -1.5)\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\n",
    "            \"d_model\": self.d_model.numpy(),  # convert back to Python float\n",
    "            \"warmup_steps\": self.warmup_steps.numpy()\n",
    "        }\n",
    "\n",
    "\n",
    "# Loss function with label smoothing\n",
    "def apply_label_smoothing(y_true, smoothing=0.1):\n",
    "    num_classes = tf.cast(tf.shape(y_true)[-1], tf.float32)\n",
    "    return y_true * (1.0 - smoothing) + (smoothing / num_classes)\n",
    "\n",
    "os.environ[\"SM_FRAMEWORK\"] = \"tf.keras\"\n",
    "weights = [0.1666, 0.1666, 0.1666, 0.1666, 0.1666, 0.1666]\n",
    "raw_dice = sm.losses.DiceLoss(class_weights=weights)\n",
    "raw_focal = sm.losses.CategoricalFocalLoss()\n",
    "\n",
    "def total_loss_with_smoothing(y_true, y_pred):\n",
    "    y_true_smoothed = apply_label_smoothing(y_true, smoothing=0.1)\n",
    "    return raw_dice(y_true_smoothed, y_pred) + raw_focal(y_true_smoothed, y_pred)\n",
    "\n",
    "\n",
    "\n",
    "# --- Jaccard Index ---\n",
    "class MeanIoUMetric(tf.keras.metrics.MeanIoU):\n",
    "    def __init__(self, num_classes, name=\"mean_iou\", dtype=None):\n",
    "        super().__init__(num_classes=num_classes, name=name, dtype=dtype)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.argmax(y_true, axis=-1)\n",
    "        y_pred = tf.argmax(y_pred, axis=-1)\n",
    "        return super().update_state(y_true, y_pred, sample_weight)\n",
    "\n",
    "miou_metric = MeanIoUMetric(num_classes=6)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- Model Building ---\n",
    "def train_model(base_dir=\"/content/chipped_data/content/chipped_data\", out_dir=\"/content/figs\", \n",
    "                freeze=False, model_path=None,\n",
    "                input_type=\"rgb_elev\", model_type=\"unet\", tile_size=256,\n",
    "                batch_size=8, epochs=10, train_time=20, verbose=1, google=False, fine_tune=False,\n",
    "                ):\n",
    "    \n",
    "    assert input_type in INPUT_TYPE_CONFIG, f\"Unknown input type: {input_type}\"\n",
    "    num_channels = INPUT_TYPE_CONFIG[input_type][\"channels\"]\n",
    "\n",
    "    img_dir = os.path.join(base_dir, \"train\", \"images\")\n",
    "    elev_dir = os.path.join(base_dir, \"train\", \"elevations\")\n",
    "    label_dir = os.path.join(base_dir, \"train\", \"labels\")\n",
    "\n",
    "    # Load metadata and define input shape\n",
    "    input_shape = (tile_size, tile_size, num_channels)\n",
    "    train_df = csv_to_df('train', 256)\n",
    "    val_df = csv_to_df('val', 256)\n",
    "    test_df = csv_to_df('test', 256)\n",
    "\n",
    "    \n",
    "    # --- Streaming Data Generator ---\n",
    "    train_gen = build_tf_dataset(train_df, img_dir, elev_dir, label_dir,\n",
    "                                 input_type=input_type, split='train',\n",
    "                                 augment=True, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "    val_gen = build_tf_dataset(val_df, img_dir, elev_dir, label_dir,\n",
    "                                input_type=input_type, split='val',\n",
    "                                augment=False, shuffle=False, batch_size=batch_size)\n",
    "    \n",
    "    test_gen = build_tf_dataset(test_df, img_dir, elev_dir, label_dir,\n",
    "                            input_type=input_type, split='test',\n",
    "                            augment=False, shuffle=False, batch_size=batch_size)\n",
    "\n",
    "    \n",
    "    for x_batch, y_batch in test_gen.take(1):\n",
    "        y_np = np.argmax(y_batch.numpy(), axis=-1)\n",
    "        print(\"Unique labels in batch:\", np.unique(y_np))\n",
    "\n",
    "\n",
    "    # --- Model ---\n",
    "    import segmentation_models as sm\n",
    "\n",
    "    if model_path is None or not os.path.exists(model_path):\n",
    "        if model_type == \"unet\":\n",
    "            model = build_unet(input_shape=input_shape, num_classes=NUM_CLASSES)\n",
    "        \n",
    "        elif model_type == \"new_unet\":\n",
    "            model, base_model = build_flexible_unet(input_shape=input_shape, num_classes=NUM_CLASSES, freeze_rgb_encoder=False)\n",
    "\n",
    "        elif model_type == \"multi_unet\":\n",
    "            model = build_multi_unet(input_shape=input_shape, num_classes=NUM_CLASSES)\n",
    "\n",
    "        elif model_type == \"enhanced_unet\":\n",
    "            model = enhanced_unet(input_shape=input_shape, num_classes=NUM_CLASSES)\n",
    "\n",
    "        elif model_type == \"B2\":\n",
    "            model = SegFormer_B2(input_shape=input_shape, num_classes=NUM_CLASSES)\n",
    "\n",
    "        elif model_type == \"B0\":\n",
    "            model = SegFormer_B0(input_shape=input_shape, num_classes=NUM_CLASSES)\n",
    "        \n",
    "        elif model_type == \"B5\":\n",
    "            model = SegFormer_B5(input_shape=input_shape, num_classes=NUM_CLASSES)\n",
    "\n",
    "        elif model_type == \"B4\":\n",
    "            model = SegFormer_B4(input_shape=input_shape, num_classes=NUM_CLASSES)\n",
    "\n",
    "        elif model_type == \"B1\":\n",
    "            model = SegFormer_B1(input_shape=input_shape, num_classes=NUM_CLASSES)\n",
    "\n",
    "        elif model_type == \"B3\":\n",
    "            model = SegFormer_B3(input_shape=input_shape, num_classes=NUM_CLASSES)\n",
    "        \n",
    "        elif model_type == \"resnet34\":\n",
    "            model = sm.Unet(\n",
    "                backbone_name=\"resnet34\",               # or 'efficientnetb0', 'mobilenetv2', etc.\n",
    "                input_shape=input_shape,\n",
    "                classes=NUM_CLASSES,                  \n",
    "                activation='softmax', \n",
    "                encoder_weights='imagenet'              # Load ImageNet pre-trained weights\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model_type: {model_type}\")\n",
    "    \n",
    "    else:\n",
    "        print(f\"Loading Stage 1 model from {model_path} and unfreezing all layers...\")\n",
    "\n",
    "        custom_objects={\n",
    "            'DiceLoss': sm.losses.DiceLoss,\n",
    "            'CategoricalFocalLoss': sm.losses.CategoricalFocalLoss,\n",
    "            'MeanIoU': MeanIoUMetric\n",
    "        }  \n",
    "\n",
    "        model = tf.keras.models.load_model(\n",
    "            model_path,\n",
    "            custom_objects=custom_objects,\n",
    "            compile=True\n",
    "        )\n",
    "\n",
    "        for layer in model.layers:\n",
    "            layer.trainable = True\n",
    "\n",
    "    if freeze:\n",
    "        print(\"Stage 1: Freezing all layers except head...\")\n",
    "        for layer in model.layers:\n",
    "            layer.trainable = False\n",
    "        for layer in model.layers[-10:]:\n",
    "            layer.trainable = True\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    # --- Callbacks --- \n",
    "    nan_terminate = TerminateOnNaN()\n",
    "    #dual_ckpt = DualCheckpointSaver(base_model=model, monitor='val_iou_score', mode='max')\n",
    "    time_limit = TimeLimitCallback(max_minutes=train_time)\n",
    "    early_stop = EarlyStopping(monitor=\"val_iou_score\", mode=\"max\", patience=24, restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor=\"val_iou_score\", mode=\"max\", patience=13, min_lr=5e-7)\n",
    "    #LearningRateLogger(),\n",
    "\n",
    "\n",
    "    if google == True:\n",
    "        lr_schedule = TransformerLRSchedule(d_model=tile_size)\n",
    "        optimizer = Adam(learning_rate=lr_schedule, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "        callbacks = [\n",
    "            time_limit,\n",
    "            early_stop,\n",
    "            nan_terminate, \n",
    "            StepTimer(),\n",
    "        ]\n",
    "\n",
    "    else:\n",
    "        optimizer = Adam(learning_rate=1e-3)\n",
    "        callbacks = [\n",
    "            reduce_lr,\n",
    "            time_limit,\n",
    "            early_stop,\n",
    "            nan_terminate, \n",
    "            StepTimer(),\n",
    "        ]\n",
    "\n",
    "    metrics = [\n",
    "        sm.metrics.IOUScore(threshold=None, name=\"iou_score\"),   # fast, approximated mIoU per batch\n",
    "        sm.metrics.FScore(threshold=None, name=\"f1-score\"),\n",
    "        tf.keras.metrics.CategoricalAccuracy(name=\"categorical_accuracy\"),\n",
    "    ]\n",
    "\n",
    "\n",
    "    # --- Compile Model ---\n",
    "    if fine_tune:\n",
    "\n",
    "        # Force RGB-only data for Stage 1\n",
    "        frozen_train_gen = build_tf_dataset(train_df, img_dir, elev_dir, label_dir,\n",
    "                                input_type=input_type, dummy=True, split='train',\n",
    "                                augment=True, shuffle=True, batch_size=batch_size)\n",
    "        \n",
    "        frozen_val_gen = build_tf_dataset(val_df, img_dir, elev_dir, label_dir,\n",
    "                                    input_type=input_type, dummy=True, split='val',\n",
    "                                    augment=False, shuffle=False, batch_size=batch_size)\n",
    "        \n",
    "        # ✅ Phase 1: Freeze encoder (pretrained backbone)\n",
    "        #encoder = model.get_layer(\"encoder\")  # or change to your encoder name if different\n",
    "        encoder = base_model\n",
    "        for layer in encoder.layers:\n",
    "            layer.trainable = False\n",
    "\n",
    "        # 🔧 Compile model (must compile after changing layer.trainable)\n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "            loss=total_loss_with_smoothing,\n",
    "            metrics=metrics\n",
    "        )\n",
    "\n",
    "        # 🏋️‍♂️ Train for a few epochs (warm-up decoder only)\n",
    "        model.fit(\n",
    "            frozen_train_gen,\n",
    "            validation_data=frozen_val_gen,\n",
    "            epochs=25,\n",
    "            callbacks=callbacks\n",
    "        )\n",
    "\n",
    "        # ✅ Phase 2: Unfreeze encoder\n",
    "        for layer in encoder.layers:\n",
    "            layer.trainable = True\n",
    "\n",
    "        # 🔧 Re-compile again after unfreezing\n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),  # Usually lower LR for fine-tuning\n",
    "            loss=total_loss_with_smoothing,\n",
    "            metrics=metrics\n",
    "        )\n",
    "\n",
    "        # 🏋️‍♂️ Continue training with fine-tuning\n",
    "        history = model.fit(\n",
    "            train_gen,\n",
    "            validation_data=val_gen,\n",
    "            initial_epoch=25,\n",
    "            epochs=epochs,\n",
    "            callbacks=callbacks\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss=total_loss_with_smoothing,\n",
    "            metrics=metrics\n",
    "        )\n",
    "        \n",
    "        # --- Train Model ---\n",
    "        history = model.fit(\n",
    "            train_gen, validation_data=val_gen,\n",
    "            epochs=epochs,\n",
    "            callbacks=callbacks,\n",
    "            verbose=verbose\n",
    "        )\n",
    "\n",
    "\n",
    "    # --- Evaluate Model ---\n",
    "    plot_training_curves(history, out_dir)\n",
    "    evaluate_on_test(model, test_gen, test_df, \"/content/figs\", img_dir, label_dir, tile_size, n_rows=5, n_cols=2) \n",
    "    measure_inference_time(model, test_gen, num_batches=5)\n",
    " \n",
    "    \n",
    "    # --- Gangster Shit ---\n",
    "    img, label, pred = reconstruct_prediction_canvas(model, test_df, \"15efe45820_D95DF0B1F4INSPIRE\", build_tf_dataset, img_dir, elev_dir, label_dir)\n",
    "    show_reconstructed_canvases(img, label, pred, \"15efe45820_D95DF0B1F4INSPIRE\")\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "    def show_reconstructed_canvases(img, label, pred, source_file):\n",
    "        fig, axs = plt.subplots(1, 3, figsize=(30, 15))  # Adjust size as needed\n",
    "\n",
    "        axs[0].imshow(img)\n",
    "        axs[0].set_title(\"RGB Image\")\n",
    "        axs[0].axis('off')\n",
    "\n",
    "        axs[1].imshow(label)\n",
    "        axs[1].set_title(\"Ground Truth\")\n",
    "        axs[1].axis('off')\n",
    "\n",
    "        axs[2].imshow(pred)\n",
    "        axs[2].set_title(\"Model Prediction\")\n",
    "        axs[2].axis('off')\n",
    "\n",
    "        # Big title for the whole figure\n",
    "        plt.suptitle(f\"Reconstruction for: {source_file}\", fontsize=24, y=0.95)\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.93])  # Leave space for the suptitle\n",
    "        plt.show()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPw4dXqbrOaTut/+CDo+nZ1",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
