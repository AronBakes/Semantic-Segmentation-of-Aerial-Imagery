{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TerminateOnNaN\n",
    "import cv2\n",
    "import itertools\n",
    "import gc\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.schedules import LearningRateSchedule\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "import segmentation_models as sm\n",
    "from collections import defaultdict\n",
    "from tensorflow.keras.metrics import MeanIoU\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras import mixed_precision\n",
    "mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "\n",
    "\n",
    "INPUT_TYPE_CONFIG = {\n",
    "    \"rgb\": {\"description\": \"RGB only\", \"channels\": 3},\n",
    "    \"rgb_elev\": {\"description\": \"RGB + elevation\", \"channels\": 5}\n",
    "}\n",
    "\n",
    "COLOR_TO_CLASS = {\n",
    "    (230, 25, 75): 0,\n",
    "    (145, 30, 180): 1,\n",
    "    (60, 180, 75): 2,\n",
    "    (245, 130, 48): 3,\n",
    "    (255, 255, 255): 4,\n",
    "    (0, 130, 200): 5,\n",
    "    (255, 0, 255): 6 # Ignore pixel for visualisation\n",
    "}\n",
    "\n",
    "CLASS_TO_COLOR = {v: k for k, v in COLOR_TO_CLASS.items() if v < 6}  # Exclude ignore class\n",
    "NUM_CLASSES = 6\n",
    "COLOR_PALETTE = np.array(list(COLOR_TO_CLASS.keys()), dtype=np.uint8)\n",
    "COLOR_LOOKUP = {tuple(c): i for c, i in COLOR_TO_CLASS.items()}\n",
    "\n",
    "def decode_label_image(label_img):\n",
    "    h, w, _ = label_img.shape\n",
    "    label_map = np.zeros((h, w), dtype=np.uint8)\n",
    "    for y in range(h):\n",
    "        for x in range(w):\n",
    "            pixel = tuple(label_img[y, x])\n",
    "            if pixel not in COLOR_LOOKUP:\n",
    "                raise ValueError(f\"Unknown label colour {pixel} at ({y}, {x})\")\n",
    "            label_map[y, x] = COLOR_LOOKUP[pixel]\n",
    "    return label_map\n",
    "\n",
    "\n",
    "def filter_tile_ids_by_substring(image_dir, base_names):\n",
    "    return [f.replace('-ortho.png', '') for f in os.listdir(image_dir) if any(base in f for base in base_names)]\n",
    "\n",
    "\n",
    "\n",
    "# --- Metrics ---\n",
    "class TransformerLRSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super().__init__()\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "        self.warmup_steps = tf.cast(warmup_steps, tf.float32)\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, tf.float32)\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * tf.pow(self.warmup_steps, -1.5)\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\n",
    "            \"d_model\": self.d_model.numpy(),  # convert back to Python float\n",
    "            \"warmup_steps\": self.warmup_steps.numpy()\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "# Loss function with label smoothing\n",
    "def apply_label_smoothing(y_true, smoothing=0.1):\n",
    "    num_classes = tf.cast(tf.shape(y_true)[-1], tf.float32)\n",
    "    return y_true * (1.0 - smoothing) + (smoothing / num_classes)\n",
    "\n",
    "os.environ[\"SM_FRAMEWORK\"] = \"tf.keras\"\n",
    "\n",
    "# --- Jaccard Index ---\n",
    "class MeanIoUMetric(tf.keras.metrics.MeanIoU):\n",
    "    def __init__(self, num_classes, name=\"mean_iou\", dtype=None):\n",
    "        super().__init__(num_classes=num_classes, name=name, dtype=dtype)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.argmax(y_true, axis=-1)\n",
    "        y_pred = tf.argmax(y_pred, axis=-1)\n",
    "        return super().update_state(y_true, y_pred, sample_weight)\n",
    "\n",
    "miou_metric = MeanIoUMetric(num_classes=6)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- Model Building ---\n",
    "def train_unet(\n",
    "        base_dir=\"/content/chipped_data/content/chipped_data\", out_dir=\"/content/figs\", \n",
    "        input_type=\"rgb_elev\", model_type=\"ENHANCED_unet\", tile_size=256,\n",
    "        batch_size=8, epochs=50, train_time=20, verbose=1, yummy=False, model_path=None,\n",
    "    ):\n",
    "    \n",
    "\n",
    "    assert input_type in INPUT_TYPE_CONFIG, f\"Unknown input type: {input_type}\"\n",
    "    num_channels = INPUT_TYPE_CONFIG[input_type][\"channels\"]\n",
    "\n",
    "    img_dir = os.path.join(base_dir, \"train\", \"images\")\n",
    "    elev_dir = os.path.join(base_dir, \"train\", \"elevations\")\n",
    "    slope_dir = os.path.join(base_dir, \"train\", \"slopes\")\n",
    "    label_dir = os.path.join(base_dir, \"train\", \"labels\")\n",
    "\n",
    "    # Load metadata and define input shape\n",
    "    input_shape = (tile_size, tile_size, num_channels)\n",
    "    train_df = csv_to_df('train', subset=0.5)\n",
    "    val_df = csv_to_df('val')\n",
    "    test_df = csv_to_df('test')\n",
    "\n",
    "    # --- Streaming Data Generator ---\n",
    "    train_gen = build_tf_dataset(train_df, img_dir, elev_dir, slope_dir, label_dir,\n",
    "                                 input_type=input_type, split='train',\n",
    "                                 augment=True, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "    val_gen = build_tf_dataset(val_df, img_dir, elev_dir, slope_dir, label_dir,\n",
    "                                input_type=input_type, split='val',\n",
    "                                augment=False, shuffle=False, batch_size=batch_size)\n",
    "    \n",
    "    test_gen = build_tf_dataset(test_df, img_dir, elev_dir, slope_dir, label_dir,\n",
    "                            input_type=input_type, split='test',\n",
    "                            augment=False, shuffle=False, batch_size=batch_size)\n",
    "\n",
    "\n",
    "\n",
    "    for x_batch, y_batch in test_gen.take(1):\n",
    "        y_np = np.argmax(y_batch.numpy(), axis=-1)\n",
    "        print(\"Shape of y_batch:\", y_batch.shape)\n",
    "        print(\"Unique labels in y batch:\", np.unique(y_np))\n",
    "\n",
    "\n",
    "    # --- Model ---\n",
    "    import segmentation_models as sm\n",
    "    if model_path is None or not os.path.exists(model_path):\n",
    "        if model_type == \"unet\":\n",
    "            model = build_unet(input_shape=input_shape, num_classes=NUM_CLASSES)\n",
    "        \n",
    "        elif model_type == \"new_unet\":\n",
    "            model, base_model = build_flexible_unet(input_shape=input_shape, num_classes=NUM_CLASSES, freeze_rgb_encoder=False)\n",
    "\n",
    "        elif model_type == \"multi_unet\":\n",
    "            model = build_multi_unet(input_shape=input_shape, num_classes=NUM_CLASSES)\n",
    "\n",
    "        elif model_type == \"enhanced_unet\":\n",
    "            model = enhanced_unet(input_shape=input_shape, num_classes=NUM_CLASSES)\n",
    "\n",
    "        elif model_type == \"resnet34\":\n",
    "            model = sm.Unet(\n",
    "                backbone_name=\"resnet34\",               # or 'efficientnetb0', 'mobilenetv2', etc.\n",
    "                input_shape=input_shape,\n",
    "                classes=NUM_CLASSES,                  \n",
    "                activation='softmax', \n",
    "                encoder_weights='imagenet'              # Load ImageNet pre-trained weights\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model_type: {model_type}\")\n",
    "\n",
    "    else:\n",
    "        custom_objects={\n",
    "            'DiceLoss': sm.losses.DiceLoss,\n",
    "            'CategoricalFocalLoss': sm.losses.CategoricalFocalLoss,\n",
    "            'MeanIoU': MeanIoUMetric\n",
    "        }  \n",
    "\n",
    "        model = tf.keras.models.load_model(\n",
    "            model_path,\n",
    "            custom_objects=custom_objects,\n",
    "            compile=True\n",
    "        )\n",
    "\n",
    "        for layer in model.layers:\n",
    "            layer.trainable = True\n",
    "\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    '''  \n",
    "    for i, layer in enumerate(model.layers):\n",
    "        print(i, layer.name)\n",
    "    '''\n",
    "\n",
    "    # --- Callbacks --- \n",
    "        \n",
    "\n",
    "    class DynamicClassWeightUpdater(tf.keras.callbacks.Callback):\n",
    "        def __init__(self, val_data, update_every=5, target='f1', ignore_class=None):\n",
    "            super().__init__()\n",
    "            self.val_data = val_data\n",
    "            self.update_every = update_every\n",
    "            self.target = target  # 'f1' or 'iou'\n",
    "            self.ignore_class = ignore_class\n",
    "\n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            if (epoch + 1) % self.update_every != 0:\n",
    "                return\n",
    "\n",
    "            y_true_all = []\n",
    "            y_pred_all = []\n",
    "\n",
    "            for x_batch, y_batch in self.val_data:\n",
    "                preds = self.model.predict(x_batch, verbose=0)\n",
    "                y_true = tf.argmax(y_batch, axis=-1).numpy().flatten()\n",
    "                y_pred = tf.argmax(preds, axis=-1).numpy().flatten()\n",
    "\n",
    "                y_true_all.extend(y_true)\n",
    "                y_pred_all.extend(y_pred)\n",
    "\n",
    "            y_true_all = np.array(y_true_all)\n",
    "            y_pred_all = np.array(y_pred_all)\n",
    "\n",
    "            new_weights = []\n",
    "\n",
    "            for i in range(NUM_CLASSES):\n",
    "                if self.ignore_class is not None and i == self.ignore_class:\n",
    "                    new_weights.append(0.0)\n",
    "                    continue\n",
    "\n",
    "                if self.target == 'f1':\n",
    "                    f1 = f1_score(y_true_all == i, y_pred_all == i, zero_division=0)\n",
    "                    weight = 1.0 if f1 == 0 else 1.0 / f1\n",
    "                else:\n",
    "                    # IoU: intersection / union\n",
    "                    intersection = np.logical_and(y_true_all == i, y_pred_all == i).sum()\n",
    "                    union = (y_true_all == i).sum() + (y_pred_all == i).sum() - intersection\n",
    "                    iou = intersection / union if union > 0 else 0.0\n",
    "                    weight = 1.0 if iou == 0 else 1.0 / iou\n",
    "\n",
    "                new_weights.append(weight)\n",
    "\n",
    "            new_weights = np.array(new_weights, dtype=np.float32)\n",
    "            new_weights = new_weights / new_weights.max()  # normalise\n",
    "\n",
    "            print(f\"\\nðŸ“ˆ Updating class weights: {new_weights}\\n\")\n",
    "            class_weights.assign(new_weights)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #LearningRateLogger()\n",
    "    monitor = \"val_iou_score\"\n",
    "    #monitor = \"val_loss\"\n",
    "    nan_terminate = TerminateOnNaN()\n",
    "    time_limit = TimeLimitCallback(max_minutes=train_time)\n",
    "    early_stop = EarlyStopping(monitor=monitor, mode=\"max\", patience=25, restore_best_weights=True, verbose=1)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor=monitor, mode=\"max\", patience=6, min_lr=5e-7, factor=0.5, verbose=1, min_delta=1e-4)\n",
    "\n",
    "    weight_callback = DynamicClassWeightUpdater(val_data=val_gen, update_every=5, target='iou', ignore_class=4)\n",
    "    \n",
    "    callbacks = [\n",
    "        reduce_lr,\n",
    "        time_limit,\n",
    "        early_stop,\n",
    "        nan_terminate, \n",
    "        StepTimer(),\n",
    "    ]\n",
    "\n",
    "    metrics = [\n",
    "        sm.metrics.IOUScore(threshold=None, name=\"iou_score\"),   # fast, approximated mIoU per batch\n",
    "        sm.metrics.FScore(threshold=None, name=\"f1-score\"),\n",
    "        tf.keras.metrics.CategoricalAccuracy(name=\"categorical_accuracy\"),\n",
    "    ]\n",
    "\n",
    "    optimizer = mixed_precision.LossScaleOptimizer(\n",
    "        Adam(learning_rate=1e-4), dynamic=True\n",
    "    )\n",
    "\n",
    "        \n",
    "    #weights = [0.23, 0.01, 0.04, 0.65, 0.0025, 0.36]                  # original\n",
    "    #weights = [0.2, 0.01, 0.04, 0.65, 0.0005, 0.3]\n",
    "    #weights = [0.4, 0.03, 0.015, 1.8, 0.001, 0.3]\n",
    "    #weights = [0.27, 0.01, 0.04, 0.99, 0.0005, 0.2] \n",
    "\n",
    "    label_smoothing = 0.1\n",
    "    loss_weights = [0.05, 1.8, 1.0]\n",
    "\n",
    "    class_weights = [1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
    "    class_weights = tf.Variable([1.0] * NUM_CLASSES, trainable=False, dtype=tf.float32)\n",
    "    class_weights = [1.05, 0.9, 1.0, 1.3, 0.0, 1.0]\n",
    "\n",
    "    raw_dice = sm.losses.DiceLoss(class_weights=class_weights)\n",
    "    raw_focal = sm.losses.CategoricalFocalLoss()\n",
    "    raw_cce = CategoricalCrossentropy()\n",
    "\n",
    "\n",
    "    def apply_ignore_class_mask(y_true, y_pred, ignore_class=4, loss_fn=None):\n",
    "        # y_true, y_pred shape: (batch, h, w, num_classes)\n",
    "        class_ids = tf.argmax(y_true, axis=-1)  # shape: (batch, h, w)\n",
    "        mask = tf.not_equal(class_ids, ignore_class)  # shape: (batch, h, w)\n",
    "\n",
    "        mask = tf.cast(mask, tf.float32)  # same shape as class_ids\n",
    "        mask = tf.expand_dims(mask, axis=-1)  # shape: (batch, h, w, 1)\n",
    "\n",
    "        # Apply mask to loss\n",
    "        loss = loss_fn(y_true, y_pred)  # shape: (batch, h, w, 1) or scalar\n",
    "\n",
    "        # If loss is scalar (e.g. averaged), convert to pixelwise\n",
    "        if len(loss.shape) < 4:\n",
    "            return loss\n",
    "        masked_loss = loss * mask\n",
    "        return tf.reduce_sum(masked_loss) / tf.reduce_sum(mask)\n",
    "\n",
    "\n",
    "    def total_loss(y_true, y_pred):\n",
    "        y_true_smoothed = apply_label_smoothing(y_true, smoothing=label_smoothing)\n",
    "\n",
    "        dice = raw_dice(y_true_smoothed, y_pred)\n",
    "        focal = raw_focal(y_true_smoothed, y_pred)\n",
    "        cce = raw_cce(y_true_smoothed, y_pred)\n",
    "\n",
    "        base_loss = (\n",
    "            loss_weights[0] * cce +\n",
    "            loss_weights[1] * dice +\n",
    "            loss_weights[2] * focal\n",
    "        )\n",
    "\n",
    "        return apply_ignore_class_mask(y_true_smoothed, y_pred, ignore_class=4, loss_fn=lambda yt, yp: base_loss)\n",
    "\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=total_loss,\n",
    "        metrics=metrics\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        train_gen, validation_data=val_gen,\n",
    "        epochs=epochs,\n",
    "        callbacks=callbacks,\n",
    "        verbose=verbose\n",
    "    )\n",
    "\n",
    "\n",
    "    # --- Evaluate Model ---\n",
    "    plot_training_curves(history, out_dir)\n",
    "    evaluate_on_test(model, test_gen, test_df, \"/content/figs\", img_dir, label_dir, tile_size, n_rows=12, n_cols=3) \n",
    "    measure_inference_time(model, test_gen, num_batches=5)\n",
    " \n",
    "    \n",
    "    # --- Gangster Shit ---\n",
    "    if yummy:\n",
    "        img, label, pred = reconstruct_canvas(model, test_df, \"25f1c24f30_EB81FE6E2BOPENPIPELINE\", build_tf_dataset, img_dir, elev_dir, slope_dir, label_dir)\n",
    "        plot_reconstruction(img, label, pred, \"25f1c24f30_EB81FE6E2BOPENPIPELINE\")\n",
    "\n",
    "        img, label, pred = reconstruct_canvas(model, test_df, \"84410645db_8D20F02042OPENPIPELINE\", build_tf_dataset, img_dir, elev_dir, slope_dir, label_dir)\n",
    "        plot_reconstruction(img, label, pred, \"84410645db_8D20F02042OPENPIPELINE\")\n",
    "\n",
    "        img, label, pred = reconstruct_canvas(model, test_df, \"8710b98ea0_06E6522D6DINSPIRE\", build_tf_dataset, img_dir, elev_dir, slope_dir, label_dir)\n",
    "        plot_reconstruction(img, label, pred, \"8710b98ea0_06E6522D6DINSPIRE\")\n",
    "\n",
    "        img, label, pred = reconstruct_canvas(model, test_df, \"a1af86939f_F1BE1D4184OPENPIPELINE\", build_tf_dataset, img_dir, elev_dir, slope_dir, label_dir)\n",
    "        plot_reconstruction(img, label, pred, \"a1af86939f_F1BE1D4184OPENPIPELINE\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_segformer(\n",
    "        base_dir=\"/content/chipped_data/content/chipped_data\", out_dir=\"/content/figs\", \n",
    "        input_type=\"rgb_elev\", model_type=\"ENHANCED_unet\", tile_size=256,\n",
    "        batch_size=8, epochs=50, train_time=20, verbose=1, fine_tune=False, yummy=False, model_path=None,\n",
    "    ):\n",
    "    \n",
    "    assert input_type in INPUT_TYPE_CONFIG, f\"Unknown input type: {input_type}\"\n",
    "    num_channels = INPUT_TYPE_CONFIG[input_type][\"channels\"]\n",
    "\n",
    "    img_dir = os.path.join(base_dir, \"train\", \"images\")\n",
    "    elev_dir = os.path.join(base_dir, \"train\", \"elevations\")\n",
    "    slope_dir = os.path.join(base_dir, \"train\", \"slopes\")\n",
    "    label_dir = os.path.join(base_dir, \"train\", \"labels\")\n",
    "\n",
    "    # Load metadata and define input shape\n",
    "    input_shape = (tile_size, tile_size, num_channels)\n",
    "    train_df = csv_to_df('train')\n",
    "    val_df = csv_to_df('val')\n",
    "    test_df = csv_to_df('test')\n",
    "\n",
    "\n",
    "    # --- Streaming Data Generator ---\n",
    "    train_gen = build_tf_dataset(train_df, img_dir, elev_dir, slope_dir, label_dir,\n",
    "                                 input_type=input_type, split='train',\n",
    "                                 augment=True, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "    val_gen = build_tf_dataset(val_df, img_dir, elev_dir, slope_dir, label_dir,\n",
    "                                input_type=input_type, split='val',\n",
    "                                augment=False, shuffle=False, batch_size=batch_size)\n",
    "    \n",
    "    test_gen = build_tf_dataset(test_df, img_dir, elev_dir, slope_dir, label_dir,\n",
    "                            input_type=input_type, split='test',\n",
    "                            augment=False, shuffle=False, batch_size=batch_size)\n",
    "\n",
    "    for x_batch, y_batch in test_gen.take(1):\n",
    "        y_np = np.argmax(y_batch.numpy(), axis=-1)\n",
    "        print(\"Unique labels in y batch:\", np.unique(y_np))\n",
    "\n",
    "\n",
    "    # --- Model ---\n",
    "    import segmentation_models as sm\n",
    "    if model_path is None or not os.path.exists(model_path):\n",
    "\n",
    "        if model_type == \"B2\":\n",
    "            model = SegFormer_B2(input_shape=input_shape, num_classes=NUM_CLASSES)\n",
    "\n",
    "        elif model_type == \"B0\":\n",
    "            model = SegFormer_B0(input_shape=input_shape, num_classes=NUM_CLASSES)\n",
    "        \n",
    "        elif model_type == \"B5\":\n",
    "            model = SegFormer_B5(input_shape=input_shape, num_classes=NUM_CLASSES)\n",
    "\n",
    "        elif model_type == \"B4\":\n",
    "            model = SegFormer_B4(input_shape=input_shape, num_classes=NUM_CLASSES)\n",
    "\n",
    "        elif model_type == \"B1\":\n",
    "            model = SegFormer_B1(input_shape=input_shape, num_classes=NUM_CLASSES)\n",
    "\n",
    "        elif model_type == \"B3\":\n",
    "            model = SegFormer_B3(input_shape=input_shape, num_classes=NUM_CLASSES)\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model_type: {model_type}\")\n",
    "\n",
    "    else:\n",
    "        custom_objects={\n",
    "            'DiceLoss': sm.losses.DiceLoss,\n",
    "            'CategoricalFocalLoss': sm.losses.CategoricalFocalLoss,\n",
    "            'MeanIoU': MeanIoUMetric\n",
    "        }  \n",
    "        model = tf.keras.models.load_model(\n",
    "            model_path,\n",
    "            custom_objects=custom_objects,\n",
    "            compile=True\n",
    "        )\n",
    "        for layer in model.layers:\n",
    "            layer.trainable = True\n",
    "\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    for i, layer in enumerate(model.layers):\n",
    "        print(i, layer.name)\n",
    "\n",
    "\n",
    "    # --- Callbacks --- \n",
    "    #LearningRateLogger()\n",
    "    #monitor = \"val_iou_score\"\n",
    "    monitor = \"val_loss\"\n",
    "    nan_terminate = TerminateOnNaN()\n",
    "    time_limit = TimeLimitCallback(max_minutes=train_time)\n",
    "    early_stop = EarlyStopping(monitor=monitor, mode=\"max\", patience=20, restore_best_weights=True, verbose=1)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor=monitor, mode=\"max\", patience=6, min_lr=5e-7, factor=0.5, verbose=1, min_delta=1e-4)\n",
    "\n",
    "    lr_schedule = TransformerLRSchedule(d_model=tile_size, warmup_steps=2048)\n",
    "    callbacks = [\n",
    "        time_limit,\n",
    "        early_stop,\n",
    "        nan_terminate, \n",
    "        StepTimer(),\n",
    "    ]\n",
    "\n",
    "    metrics = [\n",
    "        sm.metrics.IOUScore(threshold=None, name=\"iou_score\"),   # fast, approximated mIoU per batch\n",
    "        sm.metrics.FScore(threshold=None, name=\"f1-score\"),\n",
    "        tf.keras.metrics.CategoricalAccuracy(name=\"categorical_accuracy\"),\n",
    "    ]\n",
    "\n",
    "\n",
    "    loss_weights = [0.25, 1.5, 1.0]\n",
    "    class_weights = [1.1, 1.0, 1.0, 2.0, 0.0, 1.3]\n",
    "    label_smoothing = 0.1\n",
    "\n",
    "    raw_dice = sm.losses.DiceLoss(class_weights=class_weights)\n",
    "    raw_focal = sm.losses.CategoricalFocalLoss()\n",
    "    raw_cce = CategoricalCrossentropy()\n",
    "\n",
    "\n",
    "    def total_loss_with_smoothing(y_true, y_pred):\n",
    "        y_true_smoothed = apply_label_smoothing(y_true, smoothing=label_smoothing)\n",
    "\n",
    "        dice = raw_dice(y_true_smoothed, y_pred)\n",
    "        focal = raw_focal(y_true_smoothed, y_pred)\n",
    "        cce = raw_cce(y_true_smoothed, y_pred)\n",
    "\n",
    "        return loss_weights[0] * cce + loss_weights[1] * dice + loss_weights[2] * focal\n",
    "\n",
    "\n",
    "\n",
    "    # --- Compile Model and Train ---\n",
    "    if fine_tune:\n",
    "        \n",
    "        water_df = balanced_stage1_filter(test_df)\n",
    "\n",
    "        frozen_train_gen = build_tf_dataset(water_df, img_dir, elev_dir, slope_dir, label_dir,\n",
    "                                input_type=input_type, split='train',\n",
    "                                augment=True, shuffle=True, batch_size=batch_size)\n",
    "    \n",
    "        # âœ… Phase 1: Freeze encoder (pretrained backbone)\n",
    "        stage1_epochs = 40\n",
    "        N = 40\n",
    "\n",
    "        # Freeze first N layers\n",
    "        for layer in model.layers[:N]:\n",
    "            layer.trainable = False\n",
    "        for layer in model.layers[N:]:\n",
    "            layer.trainable = True\n",
    "\n",
    "        # ðŸ”§ Compile model (must compile after changing layer.trainable)\n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=1e-3, beta_1=0.9, beta_2=0.98, epsilon=1e-7),\n",
    "            loss=total_loss_with_smoothing,\n",
    "            metrics=metrics\n",
    "        )\n",
    "\n",
    "        # ðŸ‹ï¸â€â™‚ï¸ Train for a few epochs (warm-up decoder only)\n",
    "        model.fit(\n",
    "            frozen_train_gen,\n",
    "            validation_data=val_gen,\n",
    "            epochs=stage1_epochs,\n",
    "            callbacks=(reduce_lr, nan_terminate)    \n",
    "        )\n",
    "\n",
    "        # âœ… Phase 2: Unfreeze encoder\n",
    "        for layer in model.layers:\n",
    "            layer.trainable = True\n",
    "\n",
    "        # ðŸ”§ Re-compile again after unfreezing\n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=lr_schedule, beta_1=0.9, beta_2=0.98, epsilon=1e-9), \n",
    "            loss=total_loss_with_smoothing,\n",
    "            metrics=metrics\n",
    "        )\n",
    "\n",
    "        # ðŸ‹ï¸â€â™‚ï¸ Continue training with fine-tuning\n",
    "        history = model.fit(\n",
    "            train_gen,\n",
    "            validation_data=val_gen,\n",
    "            initial_epoch=stage1_epochs,\n",
    "            epochs=epochs,\n",
    "            callbacks=callbacks\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=lr_schedule, beta_1=0.9, beta_2=0.98, epsilon=1e-9),\n",
    "            loss=total_loss_with_smoothing,\n",
    "            metrics=metrics\n",
    "        )\n",
    "        \n",
    "        history = model.fit(\n",
    "            train_gen, validation_data=val_gen,\n",
    "            epochs=epochs,\n",
    "            callbacks=callbacks,\n",
    "            verbose=verbose\n",
    "        )\n",
    "\n",
    "\n",
    "    # --- Evaluate Model ---\n",
    "    plot_training_curves(history, out_dir)\n",
    "    evaluate_on_test(model, test_gen, test_df, \"/content/figs\", img_dir, label_dir, tile_size, n_rows=12, n_cols=3) \n",
    "    measure_inference_time(model, test_gen, num_batches=5)\n",
    " \n",
    "    \n",
    "    # --- Gangster Shit ---\n",
    "    if yummy:\n",
    "        img, label, pred = reconstruct_canvas(model, test_df, \"25f1c24f30_EB81FE6E2BOPENPIPELINE\", build_tf_dataset, img_dir, elev_dir, slope_dir, label_dir)\n",
    "        plot_reconstruction(img, label, pred, \"25f1c24f30_EB81FE6E2BOPENPIPELINE\")\n",
    "\n",
    "        img, label, pred = reconstruct_canvas(model, test_df, \"84410645db_8D20F02042OPENPIPELINE\", build_tf_dataset, img_dir, elev_dir, slope_dir, label_dir)\n",
    "        plot_reconstruction(img, label, pred, \"84410645db_8D20F02042OPENPIPELINE\")\n",
    "\n",
    "        img, label, pred = reconstruct_canvas(model, test_df, \"8710b98ea0_06E6522D6DINSPIRE\", build_tf_dataset, img_dir, elev_dir, slope_dir, label_dir)\n",
    "        plot_reconstruction(img, label, pred, \"8710b98ea0_06E6522D6DINSPIRE\")\n",
    "\n",
    "        img, label, pred = reconstruct_canvas(model, test_df, \"a1af86939f_F1BE1D4184OPENPIPELINE\", build_tf_dataset, img_dir, elev_dir, slope_dir, label_dir)\n",
    "        plot_reconstruction(img, label, pred, \"a1af86939f_F1BE1D4184OPENPIPELINE\")\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPw4dXqbrOaTut/+CDo+nZ1",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
