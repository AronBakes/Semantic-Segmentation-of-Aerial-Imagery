{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TerminateOnNaN\n",
    "import cv2\n",
    "import itertools\n",
    "import gc\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.schedules import LearningRateSchedule\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "import segmentation_models as sm\n",
    "from collections import defaultdict\n",
    "\n",
    "from tensorflow.keras.metrics import MeanIoU\n",
    "\n",
    "\n",
    "INPUT_TYPE_CONFIG = {\n",
    "    \"rgb\": {\"description\": \"RGB only\", \"channels\": 3},\n",
    "    \"rgb_elev\": {\"description\": \"RGB + elevation\", \"channels\": 5}\n",
    "}\n",
    "\n",
    "COLOR_TO_CLASS = {\n",
    "    (230, 25, 75): 0,\n",
    "    (145, 30, 180): 1,\n",
    "    (60, 180, 75): 2,\n",
    "    (245, 130, 48): 3,\n",
    "    (255, 255, 255): 4,\n",
    "    (0, 130, 200): 5,\n",
    "    (255, 0, 255): 6 # Ignore pixel for visualisation\n",
    "}\n",
    "\n",
    "CLASS_TO_COLOR = {v: k for k, v in COLOR_TO_CLASS.items() if v < 6}  # Exclude ignore class\n",
    "NUM_CLASSES = 6\n",
    "COLOR_PALETTE = np.array(list(COLOR_TO_CLASS.keys()), dtype=np.uint8)\n",
    "COLOR_LOOKUP = {tuple(c): i for c, i in COLOR_TO_CLASS.items()}\n",
    "\n",
    "def decode_label_image(label_img):\n",
    "    h, w, _ = label_img.shape\n",
    "    label_map = np.zeros((h, w), dtype=np.uint8)\n",
    "    for y in range(h):\n",
    "        for x in range(w):\n",
    "            pixel = tuple(label_img[y, x])\n",
    "            if pixel not in COLOR_LOOKUP:\n",
    "                raise ValueError(f\"Unknown label colour {pixel} at ({y}, {x})\")\n",
    "            label_map[y, x] = COLOR_LOOKUP[pixel]\n",
    "    return label_map\n",
    "\n",
    "\n",
    "def filter_tile_ids_by_substring(image_dir, base_names):\n",
    "    return [f.replace('-ortho.png', '') for f in os.listdir(image_dir) if any(base in f for base in base_names)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- Metrics ---\n",
    "class TransformerLRSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super().__init__()\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "        self.warmup_steps = tf.cast(warmup_steps, tf.float32)\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, tf.float32)\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * tf.pow(self.warmup_steps, -1.5)\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\n",
    "            \"d_model\": self.d_model.numpy(),  # convert back to Python float\n",
    "            \"warmup_steps\": self.warmup_steps.numpy()\n",
    "        }\n",
    "\n",
    "\n",
    "# Loss function with label smoothing\n",
    "def apply_label_smoothing(y_true, smoothing=0.1):\n",
    "    num_classes = tf.cast(tf.shape(y_true)[-1], tf.float32)\n",
    "    return y_true * (1.0 - smoothing) + (smoothing / num_classes)\n",
    "\n",
    "os.environ[\"SM_FRAMEWORK\"] = \"tf.keras\"\n",
    "\n",
    "#weights = [0.1666, 0.1666, 0.1666, 0.1666, 0.1666, 0.1666]\n",
    "weights = [0.23, 0.01, 0.04, 0.65, 0.0025, 0.36]\n",
    "#weights = [0.0829 0.2623 0.0745 0.1112 0.0185 0.4506]\n",
    "\n",
    "\n",
    "raw_dice = sm.losses.DiceLoss(class_weights=weights)\n",
    "raw_focal = sm.losses.CategoricalFocalLoss()\n",
    "\n",
    "def total_loss_with_smoothing(y_true, y_pred):\n",
    "    y_true_smoothed = apply_label_smoothing(y_true, smoothing=0.1)\n",
    "    dice = raw_dice(y_true_smoothed, y_pred)\n",
    "    focal = raw_focal(y_true_smoothed, y_pred)\n",
    "    return 2 * dice + focal\n",
    "\n",
    "\n",
    "# --- Jaccard Index ---\n",
    "class MeanIoUMetric(tf.keras.metrics.MeanIoU):\n",
    "    def __init__(self, num_classes, name=\"mean_iou\", dtype=None):\n",
    "        super().__init__(num_classes=num_classes, name=name, dtype=dtype)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.argmax(y_true, axis=-1)\n",
    "        y_pred = tf.argmax(y_pred, axis=-1)\n",
    "        return super().update_state(y_true, y_pred, sample_weight)\n",
    "\n",
    "miou_metric = MeanIoUMetric(num_classes=6)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- Model Building ---\n",
    "def train_model(\n",
    "        base_dir=\"/content/chipped_data/content/chipped_data\", out_dir=\"/content/figs\", \n",
    "        input_type=\"rgb_elev\", model_type=\"unet\", tile_size=256,\n",
    "        batch_size=8, epochs=50, train_time=20, verbose=1, google=False, fine_tune=False, yummy=False,\n",
    "        freeze=False, model_path=None,\n",
    "    ):\n",
    "    \n",
    "\n",
    "    assert input_type in INPUT_TYPE_CONFIG, f\"Unknown input type: {input_type}\"\n",
    "    num_channels = INPUT_TYPE_CONFIG[input_type][\"channels\"]\n",
    "\n",
    "    img_dir = os.path.join(base_dir, \"train\", \"images\")\n",
    "    elev_dir = os.path.join(base_dir, \"train\", \"elevations\")\n",
    "    slope_dir = os.path.join(base_dir, \"train\", \"slopes\")\n",
    "    label_dir = os.path.join(base_dir, \"train\", \"labels\")\n",
    "\n",
    "\n",
    "    # Load metadata and define input shape\n",
    "    input_shape = (tile_size, tile_size, num_channels)\n",
    "    train_df = csv_to_df('train', 256)\n",
    "    val_df = csv_to_df('val', 256)\n",
    "    test_df = csv_to_df('test', 256)\n",
    "\n",
    "    \n",
    "    # --- Streaming Data Generator ---\n",
    "    train_gen = build_tf_dataset(train_df, img_dir, elev_dir, label_dir,\n",
    "                                 input_type=input_type, split='train',\n",
    "                                 augment=True, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "    val_gen = build_tf_dataset(val_df, img_dir, elev_dir, label_dir,\n",
    "                                input_type=input_type, split='val',\n",
    "                                augment=False, shuffle=False, batch_size=batch_size)\n",
    "    \n",
    "    test_gen = build_tf_dataset(test_df, img_dir, elev_dir, label_dir,\n",
    "                            input_type=input_type, split='test',\n",
    "                            augment=False, shuffle=False, batch_size=batch_size)\n",
    "\n",
    "    \n",
    "    for x_batch, y_batch in test_gen.take(1):\n",
    "        x_np = np.argmax(x_batch.numpy(), axis=-1)\n",
    "        print(\"Unique labels in x batch:\", np.unique(x_np))\n",
    "        y_np = np.argmax(y_batch.numpy(), axis=-1)\n",
    "        print(\"Unique labels in y batch:\", np.unique(y_np))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # --- Model ---\n",
    "    import segmentation_models as sm\n",
    "\n",
    "    if model_path is None or not os.path.exists(model_path):\n",
    "        if model_type == \"unet\":\n",
    "            model = build_unet(input_shape=input_shape, num_classes=NUM_CLASSES)\n",
    "        \n",
    "        elif model_type == \"new_unet\":\n",
    "            model, base_model = build_flexible_unet(input_shape=input_shape, num_classes=NUM_CLASSES, freeze_rgb_encoder=False)\n",
    "\n",
    "        elif model_type == \"multi_unet\":\n",
    "            model = build_multi_unet(input_shape=input_shape, num_classes=NUM_CLASSES)\n",
    "\n",
    "        elif model_type == \"enhanced_unet\":\n",
    "            model = enhanced_unet(input_shape=input_shape, num_classes=NUM_CLASSES)\n",
    "\n",
    "        elif model_type == \"resnet34\":\n",
    "            model = sm.Unet(\n",
    "                backbone_name=\"resnet34\",               # or 'efficientnetb0', 'mobilenetv2', etc.\n",
    "                input_shape=input_shape,\n",
    "                classes=NUM_CLASSES,                  \n",
    "                activation='softmax', \n",
    "                encoder_weights='imagenet'              # Load ImageNet pre-trained weights\n",
    "            )\n",
    "\n",
    "        \n",
    "\n",
    "        elif model_type == \"B2\":\n",
    "            model = SegFormer_B2(input_shape=input_shape, num_classes=NUM_CLASSES)\n",
    "\n",
    "        elif model_type == \"B0\":\n",
    "            model = SegFormer_B0(input_shape=input_shape, num_classes=NUM_CLASSES)\n",
    "        \n",
    "        elif model_type == \"B5\":\n",
    "            model = SegFormer_B5(input_shape=input_shape, num_classes=NUM_CLASSES)\n",
    "\n",
    "        elif model_type == \"B4\":\n",
    "            model = SegFormer_B4(input_shape=input_shape, num_classes=NUM_CLASSES)\n",
    "\n",
    "        elif model_type == \"B1\":\n",
    "            model = SegFormer_B1(input_shape=input_shape, num_classes=NUM_CLASSES)\n",
    "\n",
    "        elif model_type == \"B3\":\n",
    "            model = SegFormer_B3(input_shape=input_shape, num_classes=NUM_CLASSES)\n",
    "        \n",
    "\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model_type: {model_type}\")\n",
    "\n",
    "\n",
    "\n",
    "    else:\n",
    "        print(f\"Loading Stage 1 model from {model_path} and unfreezing all layers...\")\n",
    "\n",
    "        custom_objects={\n",
    "            'DiceLoss': sm.losses.DiceLoss,\n",
    "            'CategoricalFocalLoss': sm.losses.CategoricalFocalLoss,\n",
    "            'MeanIoU': MeanIoUMetric\n",
    "        }  \n",
    "\n",
    "        model = tf.keras.models.load_model(\n",
    "            model_path,\n",
    "            custom_objects=custom_objects,\n",
    "            compile=True\n",
    "        )\n",
    "\n",
    "        for layer in model.layers:\n",
    "            layer.trainable = True\n",
    "\n",
    "\n",
    "    if freeze:\n",
    "        print(\"Stage 1: Freezing all layers except head...\")\n",
    "        for layer in model.layers:\n",
    "            layer.trainable = False\n",
    "        for layer in model.layers[-10:]:\n",
    "            layer.trainable = True\n",
    "\n",
    "\n",
    "\n",
    "    #model.summary()\n",
    "\n",
    "\n",
    "    # --- Callbacks --- \n",
    "    nan_terminate = TerminateOnNaN()\n",
    "    time_limit = TimeLimitCallback(max_minutes=train_time)\n",
    "    early_stop = EarlyStopping(monitor=\"val_iou_score\", mode=\"max\", patience=25, restore_best_weights=True, verbose=1)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor=\"val_iou_score\", mode=\"max\", patience=12, min_lr=5e-7, factor=0.5, verbose=1, min_delta=1e-4)\n",
    "    #dual_ckpt = DualCheckpointSaver(base_model=model, monitor='val_iou_score', mode='max')\n",
    "    #LearningRateLogger()\n",
    "\n",
    "\n",
    "    if google == True:\n",
    "        lr_schedule = TransformerLRSchedule(d_model=tile_size)\n",
    "        optimizer = Adam(learning_rate=lr_schedule, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "        callbacks = [\n",
    "            time_limit,\n",
    "            early_stop,\n",
    "            nan_terminate, \n",
    "            StepTimer(),\n",
    "        ]\n",
    "\n",
    "    else:\n",
    "        optimizer = Adam(learning_rate=1e-3)\n",
    "        callbacks = [\n",
    "            reduce_lr,\n",
    "            time_limit,\n",
    "            early_stop,\n",
    "            nan_terminate, \n",
    "            StepTimer(),\n",
    "        ]\n",
    "\n",
    "\n",
    "    metrics = [\n",
    "        sm.metrics.IOUScore(threshold=None, name=\"iou_score\"),   # fast, approximated mIoU per batch\n",
    "        sm.metrics.FScore(threshold=None, name=\"f1-score\"),\n",
    "        tf.keras.metrics.CategoricalAccuracy(name=\"categorical_accuracy\"),\n",
    "    ]\n",
    "\n",
    "\n",
    "    # --- Compile Model and Train ---\n",
    "    if fine_tune:\n",
    "        # Force RGB-only data for Stage 1\n",
    "        frozen_train_gen = build_tf_dataset(train_df, img_dir, elev_dir, label_dir,\n",
    "                                input_type=input_type, dummy=True, split='train',\n",
    "                                augment=True, shuffle=True, batch_size=batch_size)\n",
    "        \n",
    "        frozen_val_gen = build_tf_dataset(val_df, img_dir, elev_dir, label_dir,\n",
    "                                    input_type=input_type, dummy=True, split='val',\n",
    "                                    augment=False, shuffle=False, batch_size=batch_size)\n",
    "        \n",
    "        # âœ… Phase 1: Freeze encoder (pretrained backbone)\n",
    "        stage1_epochs = 16\n",
    "\n",
    "        encoder = base_model\n",
    "        for layer in encoder.layers:\n",
    "            layer.trainable = False\n",
    "\n",
    "        # ðŸ”§ Compile model (must compile after changing layer.trainable)\n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=5e-4),\n",
    "            loss=total_loss_with_smoothing,\n",
    "            metrics=metrics\n",
    "        )\n",
    "\n",
    "        # ðŸ‹ï¸â€â™‚ï¸ Train for a few epochs (warm-up decoder only)\n",
    "        model.fit(\n",
    "            frozen_train_gen,\n",
    "            validation_data=frozen_val_gen,\n",
    "            epochs=stage1_epochs,\n",
    "            callbacks=(reduce_lr, nan_terminate)    \n",
    "        )\n",
    "\n",
    "        # âœ… Phase 2: Unfreeze encoder\n",
    "        for layer in encoder.layers:\n",
    "            layer.trainable = True\n",
    "\n",
    "        # ðŸ”§ Re-compile again after unfreezing\n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=2e-4),  # Usually lower LR for fine-tuning\n",
    "            loss=total_loss_with_smoothing,\n",
    "            metrics=metrics\n",
    "        )\n",
    "\n",
    "        # ðŸ‹ï¸â€â™‚ï¸ Continue training with fine-tuning\n",
    "        history = model.fit(\n",
    "            train_gen,\n",
    "            validation_data=val_gen,\n",
    "            initial_epoch=stage1_epochs,\n",
    "            epochs=epochs,\n",
    "            callbacks=callbacks\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss=total_loss_with_smoothing,\n",
    "            metrics=metrics\n",
    "        )\n",
    "        \n",
    "        history = model.fit(\n",
    "            train_gen, validation_data=val_gen,\n",
    "            epochs=epochs,\n",
    "            callbacks=callbacks,\n",
    "            verbose=verbose\n",
    "        )\n",
    "\n",
    "\n",
    "    # --- Evaluate Model ---\n",
    "    plot_training_curves(history, out_dir)\n",
    "    evaluate_on_test(model, test_gen, test_df, \"/content/figs\", img_dir, label_dir, tile_size, n_rows=6, n_cols=3) \n",
    "    measure_inference_time(model, test_gen, num_batches=5)\n",
    " \n",
    "    \n",
    "    # --- Gangster Shit ---\n",
    "    if yummy:\n",
    "        img, label, pred = reconstruct_canvas(model, test_df, \"25f1c24f30_EB81FE6E2BOPENPIPELINE\", build_tf_dataset, img_dir, elev_dir, label_dir)\n",
    "        plot_reconstruction(img, label, pred, \"25f1c24f30_EB81FE6E2BOPENPIPELINE\")\n",
    "\n",
    "        img, label, pred = reconstruct_canvas(model, test_df, \"84410645db_8D20F02042OPENPIPELINE\", build_tf_dataset, img_dir, elev_dir, label_dir)\n",
    "        plot_reconstruction(img, label, pred, \"84410645db_8D20F02042OPENPIPELINE\")\n",
    "\n",
    "        img, label, pred = reconstruct_canvas(model, test_df, \"8710b98ea0_06E6522D6DINSPIRE\", build_tf_dataset, img_dir, elev_dir, label_dir)\n",
    "        plot_reconstruction(img, label, pred, \"8710b98ea0_06E6522D6DINSPIRE\")\n",
    "\n",
    "        img, label, pred = reconstruct_canvas(model, test_df, \"a1af86939f_F1BE1D4184OPENPIPELINE\", build_tf_dataset, img_dir, elev_dir, label_dir)\n",
    "        plot_reconstruction(img, label, pred, \"a1af86939f_F1BE1D4184OPENPIPELINE\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPw4dXqbrOaTut/+CDo+nZ1",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
