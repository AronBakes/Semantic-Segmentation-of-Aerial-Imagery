{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TerminateOnNaN\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import gc\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.schedules import LearningRateSchedule\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "import segmentation_models as sm\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- Sanity ---\n",
    "def test_training_sanity():\n",
    "    print(\"âœ… from training.ipynb\")\n",
    "\n",
    "\n",
    "\n",
    "INPUT_TYPE_CONFIG = {\n",
    "    \"1ch\": {\"description\": \"grayscale only\", \"channels\": 1},\n",
    "    \"2ch\": {\"description\": \"grayscale + elevation\", \"channels\": 2},\n",
    "    \"rgb\": {\"description\": \"RGB only\", \"channels\": 3},\n",
    "    \"rgb_elev\": {\"description\": \"RGB + elevation\", \"channels\": 4}\n",
    "}\n",
    "\n",
    "COLOR_TO_CLASS = {\n",
    "    (230, 25, 75): 0,\n",
    "    (145, 30, 180): 1,\n",
    "    (60, 180, 75): 2,\n",
    "    (245, 130, 48): 3,\n",
    "    (255, 255, 255): 4,\n",
    "    (0, 130, 200): 5,\n",
    "    (255, 0, 255): 6 # Ignore pixel for visualisation\n",
    "}\n",
    "\n",
    "CLASS_TO_COLOR = {v: k for k, v in COLOR_TO_CLASS.items() if v < 6}  # Exclude ignore class\n",
    "NUM_CLASSES = 6\n",
    "COLOR_PALETTE = np.array(list(COLOR_TO_CLASS.keys()), dtype=np.uint8)\n",
    "COLOR_LOOKUP = {tuple(c): i for c, i in COLOR_TO_CLASS.items()}\n",
    "\n",
    "def decode_label_image(label_img):\n",
    "    h, w, _ = label_img.shape\n",
    "    label_map = np.zeros((h, w), dtype=np.uint8)\n",
    "    for y in range(h):\n",
    "        for x in range(w):\n",
    "            pixel = tuple(label_img[y, x])\n",
    "            if pixel not in COLOR_LOOKUP:\n",
    "                raise ValueError(f\"âŒ Unknown label colour {pixel} at ({y}, {x})\")\n",
    "            label_map[y, x] = COLOR_LOOKUP[pixel]\n",
    "    return label_map\n",
    "\n",
    "\n",
    "def filter_tile_ids_by_substring(image_dir, base_names):\n",
    "    return [f.replace('-ortho.png', '') for f in os.listdir(image_dir) if any(base in f for base in base_names)]\n",
    "\n",
    "\n",
    "'''\n",
    "def measure_inference_time(model, generator, num_batches=5):\n",
    "    import time\n",
    "    total_time = 0\n",
    "    total_images = 0\n",
    "\n",
    "    for i, (x_batch, _) in enumerate(generator):\n",
    "        if i >= num_batches:\n",
    "            break\n",
    "        start = time.time()\n",
    "        _ = model.predict(x_batch, verbose=0)\n",
    "        end = time.time()\n",
    "        total_time += (end - start)\n",
    "        total_images += x_batch.shape[0]\n",
    "\n",
    "    print(f\"ðŸ§  Inference time: {total_time:.2f} sec for {total_images} images\")\n",
    "    print(f\"â±ï¸ Avg inference time per image: {total_time / total_images:.4f} sec\")\n",
    "'''\n",
    "\n",
    "\n",
    "def measure_inference_time(model, generator, num_batches=5):\n",
    "    import time\n",
    "    total_time = 0\n",
    "    total_images = 0\n",
    "\n",
    "    for i, (x_batch, _) in enumerate(generator.take(num_batches)):\n",
    "        start = time.time()\n",
    "        _ = model.predict(x_batch, verbose=0)\n",
    "        end = time.time()\n",
    "        total_time += (end - start)\n",
    "        total_images += x_batch.shape[0]\n",
    "\n",
    "    print(f\"ðŸ§  Inference time: {total_time:.2f} sec for {total_images} images\")\n",
    "    print(f\"â±ï¸ Avg inference time per image: {total_time / total_images:.4f} sec\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "def plot_training_curves(history, out_dir):\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "\n",
    "    # Define metric pairs to plot\n",
    "    metric_pairs = [\n",
    "        (\"loss\", \"val_loss\"),\n",
    "        (\"f1-score\", \"val_f1-score\"),\n",
    "        (\"iou_score\", \"val_iou_score\"),\n",
    "        (\"categorical_accuracy\", \"val_categorical_accuracy\")\n",
    "    ]\n",
    "\n",
    "    for train_metric, val_metric in metric_pairs:\n",
    "        if train_metric not in history.history or val_metric not in history.history:\n",
    "            print(f\"âš ï¸ Skipping {train_metric} â€” missing in history.\")\n",
    "            continue\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(history.history[train_metric], label=f\"Train {train_metric}\")\n",
    "        plt.plot(history.history[val_metric], label=f\"Val {val_metric}\")\n",
    "        plt.title(f\"{train_metric} over Epochs\")\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(train_metric)\n",
    "        plt.legend()\n",
    "        filename = os.path.join(out_dir, f\"{train_metric}_plot.png\")\n",
    "        plt.savefig(filename)\n",
    "        plt.close()\n",
    "        print(f\"âœ… Saved {filename}\")\n",
    "\n",
    "\n",
    "\n",
    "# --- Metrics ---\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TerminateOnNaN\n",
    "\n",
    "# Custom learning rate schedule\n",
    "class TransformerLRSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super().__init__()\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "        self.warmup_steps = tf.cast(warmup_steps, tf.float32)\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, tf.float32)\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * tf.pow(self.warmup_steps, -1.5)\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\n",
    "            \"d_model\": self.d_model.numpy(),  # convert back to Python float\n",
    "            \"warmup_steps\": self.warmup_steps.numpy()\n",
    "        }\n",
    "\n",
    "\n",
    "# Instantiate learning rate schedule and optimizer\n",
    "lr_schedule = TransformerLRSchedule(d_model=256)\n",
    "optimizer = Adam(learning_rate=lr_schedule, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "# Loss function with label smoothing\n",
    "def apply_label_smoothing(y_true, smoothing=0.1):\n",
    "    num_classes = tf.cast(tf.shape(y_true)[-1], tf.float32)\n",
    "    return y_true * (1.0 - smoothing) + (smoothing / num_classes)\n",
    "\n",
    "\n",
    "os.environ[\"SM_FRAMEWORK\"] = \"tf.keras\"\n",
    "\n",
    "# Set class weights\n",
    "weights = [0.1666, 0.1666, 0.1666, 0.1666, 0.1666, 0.1666]\n",
    "\n",
    "# Raw losses from segmentation_models\n",
    "raw_dice = sm.losses.DiceLoss(class_weights=weights)\n",
    "raw_focal = sm.losses.CategoricalFocalLoss()\n",
    "\n",
    "# Final loss function with label smoothing applied\n",
    "def total_loss_with_smoothing(y_true, y_pred):\n",
    "    y_true_smoothed = apply_label_smoothing(y_true, smoothing=0.1)\n",
    "    return raw_dice(y_true_smoothed, y_pred) + raw_focal(y_true_smoothed, y_pred)\n",
    "\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.metrics import MeanIoU\n",
    "\n",
    "# --- Jaccard Index ---\n",
    "\n",
    "class MeanIoUMetric(tf.keras.metrics.MeanIoU):\n",
    "    def __init__(self, num_classes, name=\"mean_iou\", dtype=None):\n",
    "        super().__init__(num_classes=num_classes, name=name, dtype=dtype)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.argmax(y_true, axis=-1)\n",
    "        y_pred = tf.argmax(y_pred, axis=-1)\n",
    "        return super().update_state(y_true, y_pred, sample_weight)\n",
    "\n",
    "miou_metric = MeanIoUMetric(num_classes=6)\n",
    "\n",
    "'''\n",
    "class MaskedMeanIoU(tf.keras.metrics.Metric):\n",
    "    def __init__(self, num_classes, name=\"masked_mean_iou\", **kwargs):\n",
    "        super(MaskedMeanIoU, self).__init__(name=name, **kwargs)\n",
    "        self.num_classes = num_classes\n",
    "        self.total_cm = self.add_weight(\n",
    "            name=\"total_confusion_matrix\",\n",
    "            shape=(num_classes, num_classes),\n",
    "            initializer=\"zeros\",\n",
    "            dtype=tf.float32,\n",
    "        )\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.argmax(y_true, axis=-1)\n",
    "        y_pred = tf.argmax(y_pred, axis=-1)\n",
    "\n",
    "        # MASK: Ignore pixels labeled with 6 (your ignore class)\n",
    "        mask = tf.not_equal(y_true, 6)\n",
    "        y_true = tf.boolean_mask(y_true, mask)\n",
    "        y_pred = tf.boolean_mask(y_pred, mask)\n",
    "\n",
    "        current_cm = tf.math.confusion_matrix(y_true, y_pred, num_classes=self.num_classes, dtype=tf.float32)\n",
    "        self.total_cm.assign_add(current_cm)\n",
    "\n",
    "    def result(self):\n",
    "        sum_over_row = tf.reduce_sum(self.total_cm, axis=0)\n",
    "        sum_over_col = tf.reduce_sum(self.total_cm, axis=1)\n",
    "        true_positives = tf.linalg.diag_part(self.total_cm)\n",
    "        denominator = sum_over_row + sum_over_col - true_positives\n",
    "\n",
    "        iou = tf.math.divide_no_nan(true_positives, denominator)\n",
    "        return tf.reduce_mean(iou)\n",
    "\n",
    "    def reset_states(self):\n",
    "        tf.keras.backend.set_value(self.total_cm, tf.zeros((self.num_classes, self.num_classes)))\n",
    "'''\n",
    "\n",
    "\n",
    "# --- Callbacks ---\n",
    "class LearningRateLogger(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        lr = self.model.optimizer.learning_rate.numpy()\n",
    "        print(f\"ðŸ“‰ Learning Rate at epoch {epoch + 1}: {lr:.6f}\")\n",
    "\n",
    "class TimeLimitCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, max_minutes=20):\n",
    "        super().__init__()\n",
    "        self.max_duration = max_minutes * 60\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.start_time = tf.timestamp()\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        elapsed = tf.timestamp() - self.start_time\n",
    "        if elapsed > self.max_duration:\n",
    "            print(f\"â±ï¸ Training time exceeded {self.max_duration // 60} minutes. Stopping early.\")\n",
    "            self.model.stop_training = True\n",
    "\n",
    "class StepTimer(tf.keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.total_time = 0.0\n",
    "        self.total_steps = 0\n",
    "\n",
    "    def on_train_batch_begin(self, batch, logs=None):\n",
    "        self.start_time = tf.timestamp()\n",
    "\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        elapsed = tf.timestamp() - self.start_time\n",
    "        self.total_time += elapsed\n",
    "        self.total_steps += 1\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        avg_step_time = self.total_time / self.total_steps\n",
    "        print(f\"ðŸ•’ Average training step time: {avg_step_time:.4f} sec\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "CLASS_NAMES = ['Building', 'Clutter', 'Vegetation', 'Water', 'Background', 'Car']\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class DistributionLogger(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, generator, name=\"Training\", max_batches=16, visualise_samples=2):\n",
    "        super().__init__()\n",
    "        self.generator = generator\n",
    "        self.name = name\n",
    "        self.max_batches = max_batches\n",
    "        self.visualise_samples = visualise_samples\n",
    "        self.cumulative_class_counts = defaultdict(int)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        batch_class_counts = defaultdict(int)\n",
    "        all_samples = []\n",
    "        batches_seen = 0\n",
    "\n",
    "        for batch_images, batch_labels in self.generator:\n",
    "            if batches_seen >= self.max_batches:\n",
    "                break\n",
    "\n",
    "            batch_preds = np.argmax(batch_labels, axis=-1)\n",
    "            unique, counts = np.unique(batch_preds, return_counts=True)\n",
    "\n",
    "            for u, c in zip(unique, counts):\n",
    "                batch_class_counts[u] += c\n",
    "                self.cumulative_class_counts[u] += c\n",
    "\n",
    "            for img, label in zip(batch_images, batch_preds):\n",
    "                all_samples.append((img, label))\n",
    "\n",
    "            batches_seen += 1\n",
    "\n",
    "        total_pixels = sum(batch_class_counts.values())\n",
    "        total_images = batches_seen * self.generator.batch_size\n",
    "\n",
    "        print(f\"ðŸ“Š {self.name} Distribution After Epoch {epoch + 1} ({total_images:,} images):\")\n",
    "        for cls in sorted(batch_class_counts):\n",
    "            count = batch_class_counts[cls]\n",
    "            percent = 100.0 * count / total_pixels\n",
    "            print(f\"  Class {cls} ({CLASS_NAMES[cls]}): {count:,} px ({percent:.2f}%)\")\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        total_pixels = sum(self.cumulative_class_counts.values())\n",
    "        print(\"ðŸ“Š Final Cumulative Training Class Distribution:\")\n",
    "        print(f\"Total pixels: {total_pixels:,} px\")\n",
    "        for cls in sorted(self.cumulative_class_counts):\n",
    "            count = self.cumulative_class_counts[cls]\n",
    "            percent = 100.0 * count / total_pixels\n",
    "            print(f\"  Class {cls} ({CLASS_NAMES[cls]}): {count:,} px ({percent:.2f}%)\")\n",
    "\n",
    "        plot_class_distribution(self.cumulative_class_counts)\n",
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "class ValidationPredictionLogger(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, val_gen, user_model, out_dir=\"/content/figs\", max_batches=1):\n",
    "        super().__init__()\n",
    "        self.val_gen = val_gen\n",
    "        self.user_model = user_model\n",
    "        self.max_batches = max_batches\n",
    "        self.out_dir = out_dir\n",
    "        self.ignore_color = (255, 0, 255)\n",
    "        self.class_to_color = {\n",
    "            0: (230, 25, 75),    # Building\n",
    "            1: (145, 30, 180),   # Clutter\n",
    "            2: (60, 180, 75),    # Vegetation\n",
    "            3: (245, 130, 48),   # Water\n",
    "            4: (255, 255, 255),  # Background\n",
    "            5: (0, 130, 200),    # Car\n",
    "        }\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if (epoch + 1) % 8 != 0:\n",
    "            return\n",
    "\n",
    "        batches_seen = 0\n",
    "        for batch_images, batch_labels in self.val_gen:\n",
    "            if batches_seen >= self.max_batches:\n",
    "                break\n",
    "\n",
    "            preds = self.user_model.predict(batch_images)\n",
    "            preds_argmax = np.argmax(preds, axis=-1)\n",
    "            true_argmax = np.argmax(batch_labels.numpy(), axis=-1)\n",
    "\n",
    "            num_samples = min(8, len(batch_images))\n",
    "            fig, axs = plt.subplots(4, 6, figsize=(18, 12))\n",
    "\n",
    "            for i in range(num_samples):\n",
    "                row = i // 2\n",
    "                col = (i % 2) * 3\n",
    "\n",
    "                rgb = (batch_images[i].numpy() * 255).astype(np.uint8)\n",
    "                h, w = true_argmax[i].shape\n",
    "\n",
    "                true_rgb = np.zeros((h, w, 3), dtype=np.uint8)\n",
    "                pred_rgb = np.zeros((h, w, 3), dtype=np.uint8)\n",
    "\n",
    "                for cid, col_rgb in self.class_to_color.items():\n",
    "                    true_rgb[true_argmax[i] == cid] = col_rgb\n",
    "                    pred_rgb[preds_argmax[i] == cid] = col_rgb\n",
    "\n",
    "                # Convert batch_labels[i] to numpy if not already\n",
    "                mask = batch_labels[i].numpy()\n",
    "                ignore_mask = np.all(mask == 0, axis=-1)\n",
    "                true_rgb[ignore_mask] = self.ignore_color\n",
    "                pred_rgb[ignore_mask] = self.ignore_color\n",
    "\n",
    "                axs[row, col].imshow(rgb)\n",
    "                axs[row, col].set_title(\"Input\")\n",
    "                axs[row, col].axis(\"off\")\n",
    "\n",
    "                axs[row, col + 1].imshow(true_rgb)\n",
    "                axs[row, col + 1].set_title(\"Ground Truth\")\n",
    "                axs[row, col + 1].axis(\"off\")\n",
    "\n",
    "                axs[row, col + 2].imshow(pred_rgb)\n",
    "                axs[row, col + 2].set_title(\"Prediction\")\n",
    "                axs[row, col + 2].axis(\"off\")\n",
    "\n",
    "            os.makedirs(self.out_dir, exist_ok=True)\n",
    "            save_path = os.path.join(self.out_dir, f\"val_preds_epoch{epoch+1:03d}.png\")\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.suptitle(f\"Validation Predictions After Epoch {epoch + 1}\", y=1.02)\n",
    "            plt.savefig(save_path, bbox_inches='tight', dpi=300)\n",
    "            plt.show()\n",
    "            plt.close(fig)\n",
    "\n",
    "            batches_seen += 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "import os\n",
    "from datetime import datetime\n",
    "import tensorflow as tf\n",
    "\n",
    "class DualCheckpointSaver(Callback):\n",
    "    def __init__(self, base_model, monitor='val_iou_score', mode='max',\n",
    "                 out_dir=\"checkpoints\", drive_dir=\"/content/drive/MyDrive/checkpoints\"):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        self.monitor = monitor\n",
    "        self.mode = mode\n",
    "        self.out_dir = out_dir\n",
    "        self.drive_dir = drive_dir\n",
    "        self.timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        self.best_value = -float('inf') if mode == 'max' else float('inf')\n",
    "\n",
    "        os.makedirs(self.out_dir, exist_ok=True)\n",
    "        os.makedirs(self.drive_dir, exist_ok=True)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if logs is None or self.monitor not in logs:\n",
    "            print(f\"âš ï¸ {self.monitor} not found in logs. Skipping checkpoint.\")\n",
    "            return\n",
    "\n",
    "        current = logs[self.monitor]\n",
    "        improved = (current > self.best_value) if self.mode == 'max' else (current < self.best_value)\n",
    "\n",
    "        if improved:\n",
    "            self.best_value = current\n",
    "            epoch_num = epoch + 1\n",
    "            model_name = f\"{self.base_model.name}_{self.timestamp}_epoch{epoch_num:03d}.keras\"\n",
    "\n",
    "            local_path = os.path.join(self.out_dir, model_name)\n",
    "            drive_path = os.path.join(self.drive_dir, model_name)\n",
    "\n",
    "            self.base_model.save(local_path)\n",
    "            tf.keras.models.save_model(self.base_model, drive_path)\n",
    "            print(f\"âœ… Saved improved model at epoch {epoch_num} with {self.monitor}: {current:.4f}\")\n",
    "        else:\n",
    "            print(f\"â­ï¸ Epoch {epoch+1}: {self.monitor} did not improve ({current:.4f})\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- Model Building ---\n",
    "def train_model(base_dir=\"/content/chipped_data/content/chipped_data\", out_dir=\"/content/figs\", \n",
    "                freeze=False, model_path=None,\n",
    "                input_type=\"rgb_elev\", model_type=\"unet\", tile_size=256,\n",
    "                batch_size=8, steps=None, epochs=10, train_time=20, verbose=1\n",
    "                ):\n",
    "    \n",
    "    assert input_type in INPUT_TYPE_CONFIG, f\"Unknown input type: {input_type}\"\n",
    "    num_channels = INPUT_TYPE_CONFIG[input_type][\"channels\"]\n",
    "\n",
    "    '''train_images = os.path.join(base_dir, \"train\", \"images\")\n",
    "    train_elev = os.path.join(base_dir, \"train\", \"elevations\")\n",
    "    train_labels = os.path.join(base_dir, \"train\", \"labels\")\n",
    "\n",
    "    eval_images = os.path.join(base_dir, \"raw\", \"images\")\n",
    "    eval_elev = os.path.join(base_dir, \"raw\", \"elevations\")\n",
    "    eval_labels = os.path.join(base_dir, \"raw\", \"labels\")'''\n",
    "\n",
    "\n",
    "    img_dir = os.path.join(base_dir, \"train\", \"images\")\n",
    "    elev_dir = os.path.join(base_dir, \"train\", \"elevations\")\n",
    "    label_dir = os.path.join(base_dir, \"train\", \"labels\")\n",
    "\n",
    "    '''\n",
    "    # Load metadata and define input shape\n",
    "    if model_type == \"segformer\":\n",
    "        input_shape = (tile_size, tile_size, num_channels)\n",
    "        eval_size = 256\n",
    "    else:\n",
    "        input_shape = (None, None, num_channels)\n",
    "        eval_size = 1024\n",
    "    '''\n",
    "\n",
    "    # Load metadata and define input shape\n",
    "    input_shape = (tile_size, tile_size, num_channels)\n",
    "    train_df = csv_to_df('train', 256)\n",
    "    val_df = csv_to_df('val', 256)\n",
    "    test_df = csv_to_df('test', 256)\n",
    "\n",
    "    # Compute steps\n",
    "    '''\n",
    "    eval_batch_size = batch_size // 2\n",
    "    val_steps = (len(val_df) // eval_batch_size) + (len(val_df) % eval_batch_size > 0)\n",
    "    test_steps = (len(test_df) // eval_batch_size) + (len(test_df) % eval_batch_size > 0)'''\n",
    "\n",
    "    # --- Streaming Data Generator ---\n",
    "\n",
    "    '''train_gen = StreamingDataGenerator(train_images, train_elev, train_labels,\n",
    "                                        split='train', df=train_df,\n",
    "                                        batch_size=batch_size, steps=steps, input_type=input_type,\n",
    "                                        shuffle=True, fixed=False, augment=True)\n",
    " \n",
    "    val_gen = StreamingDataGenerator(eval_images, eval_elev, eval_labels,\n",
    "                                     split='val', df=val_df,\n",
    "                                     batch_size=8, steps=val_steps,\n",
    "                                     input_type=input_type, shuffle=False, fixed=True, augment=False)\n",
    "\n",
    "\n",
    "    test_gen = StreamingDataGenerator(eval_images, eval_elev, eval_labels,\n",
    "                                      split='test', df=test_df,\n",
    "                                      batch_size=batch_size, steps=test_steps,\n",
    "                                      input_type=input_type, shuffle=False, fixed=True, augment=False)'''\n",
    "    \n",
    "\n",
    "    train_gen = build_tf_dataset(train_df, img_dir, elev_dir, label_dir,\n",
    "                                 input_type=input_type, split='train',\n",
    "                                 augment=True, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "    val_gen = build_tf_dataset(val_df, img_dir, elev_dir, label_dir,\n",
    "                                input_type=input_type, split='val',\n",
    "                                augment=False, shuffle=False, batch_size=batch_size)\n",
    "    \n",
    "    test_gen = build_tf_dataset(test_df, img_dir, elev_dir, label_dir,\n",
    "                            input_type=input_type, split='test',\n",
    "                            augment=False, shuffle=False, batch_size=batch_size)\n",
    "\n",
    "  \n",
    "    # --- Model ---\n",
    "    import segmentation_models as sm\n",
    "\n",
    "    if model_path is None or not os.path.exists(model_path):\n",
    "        if model_type == \"unet\":\n",
    "            print(\"ðŸ§ª Calling build_unet...\")\n",
    "            model = build_unet(input_shape=input_shape, num_classes=NUM_CLASSES)\n",
    "\n",
    "        elif model_type == \"multi_unet\":\n",
    "            print(\"ðŸ§ª Calling build_multi_unet...\")\n",
    "            model = build_multi_unet(input_shape=input_shape, num_classes=NUM_CLASSES)\n",
    "\n",
    "        elif model_type == \"unet_aux\":\n",
    "            print(\"ðŸ§ª Calling build_multi_unet_aux...\")\n",
    "            model = build_unet_aux(input_shape=input_shape, num_classes=NUM_CLASSES)\n",
    "\n",
    "        elif model_type == \"segformer\":\n",
    "            model = SegFormer_B2(input_shape=input_shape, num_classes=NUM_CLASSES)\n",
    "        \n",
    "        elif model_type == \"resnet34\":\n",
    "            model = sm.Unet(\n",
    "                backbone_name=\"resnet34\",               # or 'efficientnetb0', 'mobilenetv2', etc.\n",
    "                input_shape=input_shape,\n",
    "                classes=NUM_CLASSES,                  \n",
    "                activation='softmax', \n",
    "                encoder_weights='imagenet'              # Load ImageNet pre-trained weights\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model_type: {model_type}\")\n",
    "    \n",
    "    else:\n",
    "        print(f\"Loading Stage 1 model from {model_path} and unfreezing all layers...\")\n",
    "\n",
    "        custom_objects={\n",
    "            'DiceLoss': sm.losses.DiceLoss,\n",
    "            'CategoricalFocalLoss': sm.losses.CategoricalFocalLoss,\n",
    "            'MeanIoU': MeanIoUMetric\n",
    "        }\n",
    "        \n",
    "        model = tf.keras.models.load_model(\n",
    "            model_path,\n",
    "            custom_objects=custom_objects,\n",
    "            compile=True\n",
    "        )\n",
    "\n",
    "        for layer in model.layers:\n",
    "            layer.trainable = True\n",
    "\n",
    "\n",
    "    lr_schedule = TransformerLRSchedule(d_model=tile_size)\n",
    "    #optimizer = Adam(learning_rate=lr_schedule, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "    optimizer = Adam(learning_rate=5e-3)\n",
    "    miou_metric = MeanIoUMetric(num_classes=6)\n",
    "\n",
    "    metrics=[\n",
    "        miou_metric,\n",
    "        sm.metrics.IOUScore(threshold=None),\n",
    "        sm.metrics.FScore(threshold=None),\n",
    "        'categorical_accuracy',\n",
    "    ]\n",
    "\n",
    "    if freeze:\n",
    "        print(\"Stage 1: Freezing all layers except head...\")\n",
    "        for layer in model.layers:\n",
    "            layer.trainable = False\n",
    "        for layer in model.layers[-10:]:\n",
    "            layer.trainable = True\n",
    "\n",
    "        optimizer = Adam(learning_rate=1e-3)\n",
    "\n",
    "\n",
    "    # --- Compile Model ---\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=total_loss_with_smoothing,\n",
    "        metrics=metrics\n",
    "    )\n",
    "\n",
    "    #model.summary()\n",
    "\n",
    "\n",
    "    # --- Callbacks --- \n",
    "    time_limit = TimeLimitCallback(max_minutes=train_time)\n",
    "    early_stop = EarlyStopping(monitor='val_iou_score', patience=20, restore_best_weights=True, mode='max')\n",
    "    nan_terminate = TerminateOnNaN()\n",
    "    dual_ckpt = DualCheckpointSaver(\n",
    "        base_model=model,\n",
    "        monitor='val_iou_score',\n",
    "        mode='max'\n",
    "    )\n",
    "\n",
    "\n",
    "    callbacks = [\n",
    "        nan_terminate, time_limit, \n",
    "        StepTimer(),\n",
    "        dual_ckpt\n",
    "    ]\n",
    "\n",
    "    #LearningRateLogger(),\n",
    "    #ValidationPredictionLogger(val_gen, model, max_batches=1),\n",
    "\n",
    "    # --- Train Model ---\n",
    "    print(\"ðŸš€ Starting training...\")\n",
    "    history = model.fit(\n",
    "        train_gen, validation_data=val_gen,\n",
    "        epochs=epochs,\n",
    "        callbacks=callbacks,\n",
    "        verbose=verbose\n",
    "    )\n",
    "\n",
    "\n",
    "    # --- Evaluate Model ---\n",
    "    plot_training_curves(history, out_dir)\n",
    "    evaluate_on_test(model, test_gen, test_df, \"/content/figs\", img_dir, label_dir, tile_size, n_vis=5)\n",
    "    plot_class_distribution_from_dataset(train_gen)\n",
    "    measure_inference_time(model, test_gen, num_batches=steps)\n",
    "    print(\"ðŸš€ Training complete!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # --- Archive ---\n",
    "\n",
    "    '''\n",
    "    class_weights = compute_class_weights(train_gen)\n",
    "    loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n",
    "\n",
    "    def weighted_loss(y_true, y_pred):\n",
    "        weights = tf.reduce_sum(class_weights * y_true, axis=-1)\n",
    "        return tf.reduce_mean(weights * loss_fn(y_true, y_pred))\n",
    "\n",
    "    model.compile(optimizer='adam', loss=weighted_loss, metrics=['accuracy'])\n",
    "    print(\"âœ… Model compiled with class weights\")\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    from segmentation_models.metrics import iou_score as jaccard_coef\n",
    "    metrics = [\"accuracy\", jaccard_coef]\n",
    "    weights = [0.1666, 0.1666, 0.1666, 0.1666, 0.1666, 0.1666]\n",
    "    \n",
    "    dice_loss = dice_loss(class_weights = weights)\n",
    "    focal_loss = categorical_focal_loss()\n",
    "    total_loss = dice_loss + (1 * focal_loss)\n",
    "    \n",
    "    model.compile(optimizer=\"adam\", loss=\"total_loss\", metrics=metrics)\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    def jaccard_coef(y_true, y_pred):\n",
    "    y_true_flatten = K.flatten(y_true)\n",
    "    y_pred_flatten = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_flatten * y_pred_flatten)\n",
    "    final_coef_value = (intersection + 1.0) / (K.sum(y_true_flatten) + K.sum(y_pred_flatten) - intersection + 1.0)\n",
    "    return final_coef_value\n",
    "    '''"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPw4dXqbrOaTut/+CDo+nZ1",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
