{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TerminateOnNaN\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import gc\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.schedules import LearningRateSchedule\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "import segmentation_models as sm\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "\n",
    "INPUT_TYPE_CONFIG = {\n",
    "    \"rgb\": {\"description\": \"RGB only\", \"channels\": 3},\n",
    "    \"rgb_elev\": {\"description\": \"RGB + elevation\", \"channels\": 4}\n",
    "}\n",
    "\n",
    "COLOR_TO_CLASS = {\n",
    "    (230, 25, 75): 0,\n",
    "    (145, 30, 180): 1,\n",
    "    (60, 180, 75): 2,\n",
    "    (245, 130, 48): 3,\n",
    "    (255, 255, 255): 4,\n",
    "    (0, 130, 200): 5,\n",
    "    (255, 0, 255): 6 # Ignore pixel for visualisation\n",
    "}\n",
    "\n",
    "CLASS_TO_COLOR = {v: k for k, v in COLOR_TO_CLASS.items() if v < 6}  # Exclude ignore class\n",
    "NUM_CLASSES = 6\n",
    "COLOR_PALETTE = np.array(list(COLOR_TO_CLASS.keys()), dtype=np.uint8)\n",
    "COLOR_LOOKUP = {tuple(c): i for c, i in COLOR_TO_CLASS.items()}\n",
    "\n",
    "def decode_label_image(label_img):\n",
    "    h, w, _ = label_img.shape\n",
    "    label_map = np.zeros((h, w), dtype=np.uint8)\n",
    "    for y in range(h):\n",
    "        for x in range(w):\n",
    "            pixel = tuple(label_img[y, x])\n",
    "            if pixel not in COLOR_LOOKUP:\n",
    "                raise ValueError(f\"âŒ Unknown label colour {pixel} at ({y}, {x})\")\n",
    "            label_map[y, x] = COLOR_LOOKUP[pixel]\n",
    "    return label_map\n",
    "\n",
    "\n",
    "def filter_tile_ids_by_substring(image_dir, base_names):\n",
    "    return [f.replace('-ortho.png', '') for f in os.listdir(image_dir) if any(base in f for base in base_names)]\n",
    "\n",
    "\n",
    "\n",
    "def measure_inference_time(model, generator, num_batches=5):\n",
    "    import time\n",
    "    total_time = 0\n",
    "    total_images = 0\n",
    "\n",
    "    for i, (x_batch, _) in enumerate(generator.take(num_batches)):\n",
    "        start = time.time()\n",
    "        _ = model.predict(x_batch, verbose=0)\n",
    "        end = time.time()\n",
    "        total_time += (end - start)\n",
    "        total_images += x_batch.shape[0]\n",
    "\n",
    "    print(f\"ðŸ§  Inference time: {total_time:.2f} sec for {total_images} images\")\n",
    "    print(f\"â±ï¸ Avg inference time per image: {total_time / total_images:.4f} sec\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "def plot_training_curves(history, out_dir):\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "\n",
    "    # Define metric pairs to plot\n",
    "    metric_pairs = [\n",
    "        (\"loss\", \"val_loss\"),\n",
    "        (\"f1-score\", \"val_f1-score\"),\n",
    "        (\"iou_score\", \"val_iou_score\"),\n",
    "        (\"categorical_accuracy\", \"val_categorical_accuracy\")\n",
    "    ]\n",
    "\n",
    "    for train_metric, val_metric in metric_pairs:\n",
    "        if train_metric not in history.history or val_metric not in history.history:\n",
    "            print(f\"âš ï¸ Skipping {train_metric} â€” missing in history.\")\n",
    "            continue\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(history.history[train_metric], label=f\"Train {train_metric}\")\n",
    "        plt.plot(history.history[val_metric], label=f\"Val {val_metric}\")\n",
    "        plt.title(f\"{train_metric} over Epochs\")\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(train_metric)\n",
    "        plt.legend()\n",
    "        filename = os.path.join(out_dir, f\"{train_metric}_plot.png\")\n",
    "        plt.savefig(filename)\n",
    "        plt.close()\n",
    "        print(f\"âœ… Saved {filename}\")\n",
    "\n",
    "\n",
    "\n",
    "# --- Metrics ---\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TerminateOnNaN\n",
    "\n",
    "# Custom learning rate schedule\n",
    "class TransformerLRSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super().__init__()\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "        self.warmup_steps = tf.cast(warmup_steps, tf.float32)\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, tf.float32)\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * tf.pow(self.warmup_steps, -1.5)\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\n",
    "            \"d_model\": self.d_model.numpy(),  # convert back to Python float\n",
    "            \"warmup_steps\": self.warmup_steps.numpy()\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "# Loss function with label smoothing\n",
    "def apply_label_smoothing(y_true, smoothing=0.1):\n",
    "    num_classes = tf.cast(tf.shape(y_true)[-1], tf.float32)\n",
    "    return y_true * (1.0 - smoothing) + (smoothing / num_classes)\n",
    "\n",
    "\n",
    "os.environ[\"SM_FRAMEWORK\"] = \"tf.keras\"\n",
    "\n",
    "# Set class weights\n",
    "weights = [0.1666, 0.1666, 0.1666, 0.1666, 0.1666, 0.1666]\n",
    "\n",
    "# Raw losses from segmentation_models\n",
    "raw_dice = sm.losses.DiceLoss(class_weights=weights)\n",
    "raw_focal = sm.losses.CategoricalFocalLoss()\n",
    "\n",
    "# Final loss function with label smoothing applied\n",
    "def total_loss_with_smoothing(y_true, y_pred):\n",
    "    y_true_smoothed = apply_label_smoothing(y_true, smoothing=0.1)\n",
    "    return raw_dice(y_true_smoothed, y_pred) + raw_focal(y_true_smoothed, y_pred)\n",
    "\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.metrics import MeanIoU\n",
    "\n",
    "# --- Jaccard Index ---\n",
    "\n",
    "class MeanIoUMetric(tf.keras.metrics.MeanIoU):\n",
    "    def __init__(self, num_classes, name=\"mean_iou\", dtype=None):\n",
    "        super().__init__(num_classes=num_classes, name=name, dtype=dtype)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.argmax(y_true, axis=-1)\n",
    "        y_pred = tf.argmax(y_pred, axis=-1)\n",
    "        return super().update_state(y_true, y_pred, sample_weight)\n",
    "\n",
    "miou_metric = MeanIoUMetric(num_classes=6)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- Model Building ---\n",
    "def train_model(base_dir=\"/content/chipped_data/content/chipped_data\", out_dir=\"/content/figs\", \n",
    "                freeze=False, model_path=None,\n",
    "                input_type=\"rgb_elev\", model_type=\"unet\", tile_size=256,\n",
    "                batch_size=8, steps=None, epochs=10, train_time=20, verbose=1\n",
    "                ):\n",
    "    \n",
    "    assert input_type in INPUT_TYPE_CONFIG, f\"Unknown input type: {input_type}\"\n",
    "    num_channels = INPUT_TYPE_CONFIG[input_type][\"channels\"]\n",
    "\n",
    "    img_dir = os.path.join(base_dir, \"train\", \"images\")\n",
    "    elev_dir = os.path.join(base_dir, \"train\", \"elevations\")\n",
    "    label_dir = os.path.join(base_dir, \"train\", \"labels\")\n",
    "\n",
    "    # Load metadata and define input shape\n",
    "    input_shape = (tile_size, tile_size, num_channels)\n",
    "    train_df = csv_to_df('train', 256)\n",
    "    val_df = csv_to_df('val', 256)\n",
    "    test_df = csv_to_df('test', 256)\n",
    "\n",
    "    \n",
    "    # --- Streaming Data Generator ---\n",
    "    train_gen = build_tf_dataset(train_df, img_dir, elev_dir, label_dir,\n",
    "                                 input_type=input_type, split='train',\n",
    "                                 augment=True, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "    val_gen = build_tf_dataset(val_df, img_dir, elev_dir, label_dir,\n",
    "                                input_type=input_type, split='val',\n",
    "                                augment=False, shuffle=False, batch_size=batch_size)\n",
    "    \n",
    "    test_gen = build_tf_dataset(test_df, img_dir, elev_dir, label_dir,\n",
    "                            input_type=input_type, split='test',\n",
    "                            augment=False, shuffle=False, batch_size=batch_size)\n",
    "\n",
    "    \n",
    "    for x_batch, y_batch in test_gen.take(1):\n",
    "        y_np = np.argmax(y_batch.numpy(), axis=-1)\n",
    "        print(\"Unique labels in batch:\", np.unique(y_np))\n",
    "\n",
    "\n",
    "    # --- Model ---\n",
    "    import segmentation_models as sm\n",
    "\n",
    "    if model_path is None or not os.path.exists(model_path):\n",
    "        if model_type == \"unet\":\n",
    "            print(\"ðŸ§ª Calling build_unet...\")\n",
    "            model = build_unet(input_shape=input_shape, num_classes=NUM_CLASSES)\n",
    "\n",
    "        elif model_type == \"multi_unet\":\n",
    "            print(\"ðŸ§ª Calling build_multi_unet...\")\n",
    "            model = build_multi_unet(input_shape=input_shape, num_classes=NUM_CLASSES)\n",
    "\n",
    "        elif model_type == \"unet_aux\":\n",
    "            print(\"ðŸ§ª Calling build_multi_unet_aux...\")\n",
    "            model = build_unet_aux(input_shape=input_shape, num_classes=NUM_CLASSES)\n",
    "\n",
    "        elif model_type == \"B2\":\n",
    "            model = SegFormer_B2(input_shape=input_shape, num_classes=NUM_CLASSES)\n",
    "\n",
    "        elif model_type == \"B0\":\n",
    "            model = SegFormer_B0(input_shape=input_shape, num_classes=NUM_CLASSES)\n",
    "        \n",
    "        elif model_type == \"B5\":\n",
    "            model = SegFormer_B5(input_shape=input_shape, num_classes=NUM_CLASSES)\n",
    "\n",
    "        elif model_type == \"B4\":\n",
    "            model = SegFormer_B4(input_shape=input_shape, num_classes=NUM_CLASSES)\n",
    "\n",
    "        elif model_type == \"B1\":\n",
    "            model = SegFormer_B1(input_shape=input_shape, num_classes=NUM_CLASSES)\n",
    "        \n",
    "        elif model_type == \"resnet34\":\n",
    "            model = sm.Unet(\n",
    "                backbone_name=\"resnet34\",               # or 'efficientnetb0', 'mobilenetv2', etc.\n",
    "                input_shape=input_shape,\n",
    "                classes=NUM_CLASSES,                  \n",
    "                activation='softmax', \n",
    "                encoder_weights='imagenet'              # Load ImageNet pre-trained weights\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model_type: {model_type}\")\n",
    "    \n",
    "    else:\n",
    "        print(f\"Loading Stage 1 model from {model_path} and unfreezing all layers...\")\n",
    "\n",
    "        custom_objects={\n",
    "            'DiceLoss': sm.losses.DiceLoss,\n",
    "            'CategoricalFocalLoss': sm.losses.CategoricalFocalLoss,\n",
    "            'MeanIoU': MeanIoUMetric\n",
    "        }\n",
    "        \n",
    "        model = tf.keras.models.load_model(\n",
    "            model_path,\n",
    "            custom_objects=custom_objects,\n",
    "            compile=True\n",
    "        )\n",
    "\n",
    "        for layer in model.layers:\n",
    "            layer.trainable = True\n",
    "\n",
    "\n",
    "    #lr_schedule = TransformerLRSchedule(d_model=tile_size)\n",
    "    #optimizer = Adam(learning_rate=lr_schedule, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "    optimizer = Adam(learning_rate=5e-3)\n",
    "    miou_metric = MeanIoUMetric(num_classes=6)\n",
    "\n",
    "    metrics=[\n",
    "        miou_metric,\n",
    "        sm.metrics.IOUScore(threshold=None),\n",
    "        sm.metrics.FScore(threshold=None),\n",
    "        'categorical_accuracy',\n",
    "    ]\n",
    "\n",
    "    if freeze:\n",
    "        print(\"Stage 1: Freezing all layers except head...\")\n",
    "        for layer in model.layers:\n",
    "            layer.trainable = False\n",
    "        for layer in model.layers[-10:]:\n",
    "            layer.trainable = True\n",
    "\n",
    "        optimizer = Adam(learning_rate=1e-3)\n",
    "\n",
    "\n",
    "    # --- Compile Model ---\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=total_loss_with_smoothing,\n",
    "        metrics=metrics\n",
    "    )\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "\n",
    "    # --- Callbacks --- \n",
    "    from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "    \n",
    "    time_limit = TimeLimitCallback(max_minutes=train_time)\n",
    "    early_stop = EarlyStopping(monitor='val_iou_score', patience=32, restore_best_weights=True, mode='max')\n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_iou_score',      # or 'val_mean_iou' or any other metric you use\n",
    "        factor=0.5,        \n",
    "        patience=12,             \n",
    "        verbose=1,               \n",
    "        min_lr=5e-6              # donâ€™t go below 1e-6\n",
    "    )\n",
    "\n",
    "    nan_terminate = TerminateOnNaN()\n",
    "    dual_ckpt = DualCheckpointSaver(\n",
    "        base_model=model,\n",
    "        monitor='val_iou_score',\n",
    "        mode='max'\n",
    "    )\n",
    "\n",
    "\n",
    "    callbacks = [\n",
    "        reduce_lr,\n",
    "        time_limit,\n",
    "        early_stop,\n",
    "        nan_terminate, \n",
    "        StepTimer(),\n",
    "        dual_ckpt\n",
    "    ]\n",
    "\n",
    "    #LearningRateLogger(),\n",
    "    #ValidationPredictionLogger(val_gen, model, max_batches=1),\n",
    "\n",
    "    # --- Train Model ---\n",
    "    print(\"ðŸš€ Starting training...\")\n",
    "    history = model.fit(\n",
    "        train_gen, validation_data=val_gen,\n",
    "        epochs=epochs,\n",
    "        callbacks=callbacks,\n",
    "        verbose=verbose\n",
    "    )\n",
    "\n",
    "\n",
    "    # --- Evaluate Model ---\n",
    "    plot_training_curves(history, out_dir)\n",
    "    evaluate_on_test(model, test_gen, test_df, \"/content/figs\", img_dir, label_dir, tile_size, n_vis=5)\n",
    "    measure_inference_time(model, test_gen, num_batches=steps)\n",
    "    print(\"ðŸš€ Training complete!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # --- Archive ---\n",
    "\n",
    "    '''\n",
    "    class_weights = compute_class_weights(train_gen)\n",
    "    loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n",
    "\n",
    "    def weighted_loss(y_true, y_pred):\n",
    "        weights = tf.reduce_sum(class_weights * y_true, axis=-1)\n",
    "        return tf.reduce_mean(weights * loss_fn(y_true, y_pred))\n",
    "\n",
    "    model.compile(optimizer='adam', loss=weighted_loss, metrics=['accuracy'])\n",
    "    print(\"âœ… Model compiled with class weights\")\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    from segmentation_models.metrics import iou_score as jaccard_coef\n",
    "    metrics = [\"accuracy\", jaccard_coef]\n",
    "    weights = [0.1666, 0.1666, 0.1666, 0.1666, 0.1666, 0.1666]\n",
    "    \n",
    "    dice_loss = dice_loss(class_weights = weights)\n",
    "    focal_loss = categorical_focal_loss()\n",
    "    total_loss = dice_loss + (1 * focal_loss)\n",
    "    \n",
    "    model.compile(optimizer=\"adam\", loss=\"total_loss\", metrics=metrics)\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    def jaccard_coef(y_true, y_pred):\n",
    "    y_true_flatten = K.flatten(y_true)\n",
    "    y_pred_flatten = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_flatten * y_pred_flatten)\n",
    "    final_coef_value = (intersection + 1.0) / (K.sum(y_true_flatten) + K.sum(y_pred_flatten) - intersection + 1.0)\n",
    "    return final_coef_value\n",
    "    '''"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPw4dXqbrOaTut/+CDo+nZ1",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
