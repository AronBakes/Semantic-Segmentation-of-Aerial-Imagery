{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TerminateOnNaN\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import gc\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Conv2DTranspose, concatenate, BatchNormalization, Dropout, Lambda\n",
    "\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import segmentation_models as sm\n",
    "\n",
    "\n",
    "# --- Sanity ---\n",
    "def test_training_sanity():\n",
    "    print(\"✅ from training.ipynb\")\n",
    "\n",
    "# --- Visualisation ---\n",
    "def visualise_prediction(rgb, true_mask, pred_mask):\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(16, 5))\n",
    "    axs[0].imshow(rgb)\n",
    "    axs[0].set_title(\"RGB Image\")\n",
    "    axs[0].axis(\"off\")\n",
    "    axs[1].imshow(COLOR_PALETTE[true_mask])\n",
    "    axs[1].set_title(\"True Mask\")\n",
    "    axs[1].axis(\"off\")\n",
    "    axs[2].imshow(COLOR_PALETTE[pred_mask])\n",
    "    axs[2].set_title(\"Predicted Mask\")\n",
    "    axs[2].axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "INPUT_TYPE_CONFIG = {\n",
    "    \"1ch\": {\"description\": \"grayscale only\", \"channels\": 1},\n",
    "    \"2ch\": {\"description\": \"grayscale + elevation\", \"channels\": 2},\n",
    "    \"rgb\": {\"description\": \"RGB only\", \"channels\": 3},\n",
    "    \"rgb_elevation\": {\"description\": \"RGB + elevation\", \"channels\": 4}\n",
    "}\n",
    "\n",
    "COLOR_TO_CLASS = {\n",
    "    (230, 25, 75): 0,\n",
    "    (145, 30, 180): 1,\n",
    "    (60, 180, 75): 2,\n",
    "    (245, 130, 48): 3,\n",
    "    (255, 255, 255): 4,\n",
    "    (0, 130, 200): 5\n",
    "}\n",
    "\n",
    "CLASS_TO_COLOR = {v: k for k, v in COLOR_TO_CLASS.items()}\n",
    "NUM_CLASSES = len(COLOR_TO_CLASS)\n",
    "COLOR_PALETTE = np.array(list(COLOR_TO_CLASS.keys()), dtype=np.uint8)\n",
    "COLOR_LOOKUP = {tuple(c): i for c, i in COLOR_TO_CLASS.items()}\n",
    "\n",
    "class ClearMemory(tf.keras.callbacks.Callback):\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        gc.collect()\n",
    "        K.clear_session()\n",
    "\n",
    "def decode_label_image(label_img):\n",
    "    h, w, _ = label_img.shape\n",
    "    label_map = np.zeros((h, w), dtype=np.uint8)\n",
    "    for y in range(h):\n",
    "        for x in range(w):\n",
    "            pixel = tuple(label_img[y, x])\n",
    "            if pixel not in COLOR_LOOKUP:\n",
    "                raise ValueError(f\"❌ Unknown label colour {pixel} at ({y}, {x})\")\n",
    "            label_map[y, x] = COLOR_LOOKUP[pixel]\n",
    "    return label_map\n",
    "\n",
    "\n",
    "\n",
    "def train_model(base_dir=\"/content/chipped_data/content/chipped_data\", out_dir=\"/content/figs\", \n",
    "                input_type=\"rgb_elevation\", model_type=\"unet\", tile_size=256,\n",
    "                batch_size=8, steps=None, epochs=10, train_time=20, verbose=1\n",
    "                ):\n",
    "    \n",
    "    assert input_type in INPUT_TYPE_CONFIG, f\"Unknown input type: {input_type}\"\n",
    "    num_channels = INPUT_TYPE_CONFIG[input_type][\"channels\"]\n",
    "\n",
    "    print(f\"\\n🔧 Training {model_type.upper()} with input type: {input_type} ({num_channels} channels)\")\n",
    "    print(f\"🧪 Computed input shape: ({tile_size}, {tile_size}, {num_channels})\")\n",
    "\n",
    "    train_images = os.path.join(base_dir, \"train\", \"images\")\n",
    "    train_elev = os.path.join(base_dir, \"train\", \"elevations\")\n",
    "    train_labels = os.path.join(base_dir, \"train\", \"labels\")\n",
    "\n",
    "    val_images = os.path.join(base_dir, \"val\", \"images\")\n",
    "    val_elev = os.path.join(base_dir, \"val\", \"elevations\")\n",
    "    val_labels = os.path.join(base_dir, \"val\", \"labels\")\n",
    "\n",
    "    test_images = os.path.join(base_dir, \"test\", \"images\")\n",
    "    test_elev = os.path.join(base_dir, \"test\", \"elevations\")\n",
    "    test_labels = os.path.join(base_dir, \"test\", \"labels\")\n",
    "\n",
    "\n",
    "    # --- Generators ---\n",
    "    train_gen = StreamingDataGenerator(\n",
    "        train_images, train_elev, train_labels, \n",
    "        batch_size=batch_size, input_type=input_type, \n",
    "        shuffle=True, steps=steps, fixed=False, augment=True,\n",
    "        background_threshold=0.95\n",
    "        )\n",
    "    \n",
    "    val_gen = StreamingDataGenerator(\n",
    "        val_images, val_elev, val_labels, \n",
    "        batch_size=batch_size, steps=steps, input_type=input_type, \n",
    "        shuffle=False, fixed=True, augment=False\n",
    "        )\n",
    "    \n",
    "\n",
    "    # --- Model ---\n",
    "    if model_type == \"unet\":\n",
    "        print(\"🧪 Calling build_unet...\")\n",
    "        model = build_unet(input_shape=(tile_size, tile_size, num_channels), num_classes=NUM_CLASSES)\n",
    "\n",
    "    elif model_type == \"multi_unet\":\n",
    "        print(\"🧪 Calling build_multi_unet...\")\n",
    "        model = build_multi_unet(input_shape=(tile_size, tile_size, num_channels), num_classes=NUM_CLASSES)\n",
    "\n",
    "    elif model_type == \"unet_aux\":\n",
    "        print(\"🧪 Calling build_multi_unet_aux...\")\n",
    "        model = build_unet_aux(input_shape=(tile_size, tile_size, num_channels), num_classes=NUM_CLASSES)\n",
    "\n",
    "    elif model_type == \"segformer\":\n",
    "        model = build_segformer(input_shape=(tile_size, tile_size, num_channels), num_classes=NUM_CLASSES)\n",
    "    \n",
    "    elif model_type == \"model3\":\n",
    "        model = build_model_3(input_shape=(tile_size, tile_size, num_channels), num_classes=NUM_CLASSES)\n",
    "    \n",
    "    elif model_type == \"model0\":\n",
    "        model = build_model_0(input_shape=(tile_size, tile_size, num_channels), num_classes=NUM_CLASSES)\n",
    "    \n",
    "    elif model_type == \"model1\":\n",
    "        model = build_model_1(input_shape=(tile_size, tile_size, num_channels), num_classes=NUM_CLASSES)\n",
    "    \n",
    "    elif model_type == \"model2\":\n",
    "        model = build_model_2(input_shape=(tile_size, tile_size, num_channels), num_classes=NUM_CLASSES)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model_type: {model_type}\")\n",
    "    \n",
    "\n",
    "    print(f\"🧪 Final model input shape: {model.input_shape}\")\n",
    "\n",
    "\n",
    "    from tensorflow.keras.optimizers.schedules import LearningRateSchedule\n",
    "    from tensorflow.keras.optimizers import Adam\n",
    "    from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "\n",
    "    # Custom learning rate schedule\n",
    "    class TransformerLRSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "        def __init__(self, d_model, warmup_steps=4000):\n",
    "            super().__init__()\n",
    "            self.d_model = tf.cast(d_model, tf.float32)\n",
    "            self.warmup_steps = tf.cast(warmup_steps, tf.float32)\n",
    "\n",
    "        def __call__(self, step):\n",
    "            step = tf.cast(step, tf.float32)\n",
    "            arg1 = tf.math.rsqrt(step)\n",
    "            arg2 = step * tf.pow(self.warmup_steps, -1.5)\n",
    "            return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "        def get_config(self):\n",
    "            return {\n",
    "                \"d_model\": self.d_model.numpy(),  # convert back to Python float\n",
    "                \"warmup_steps\": self.warmup_steps.numpy()\n",
    "            }\n",
    "\n",
    "\n",
    "    # Instantiate learning rate schedule and optimizer\n",
    "    lr_schedule = TransformerLRSchedule(d_model=tile_size)\n",
    "    optimizer = Adam(learning_rate=lr_schedule, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "    # Loss function with label smoothing\n",
    "    # loss_fn = CategoricalCrossentropy(label_smoothing=0.1)\n",
    "\n",
    "    def apply_label_smoothing(y_true, smoothing=0.1):\n",
    "        num_classes = tf.cast(tf.shape(y_true)[-1], tf.float32)\n",
    "        return y_true * (1.0 - smoothing) + (smoothing / num_classes)\n",
    "\n",
    "\n",
    "    os.environ[\"SM_FRAMEWORK\"] = \"tf.keras\"\n",
    "\n",
    "    # Set class weights\n",
    "    weights = [0.1666, 0.1666, 0.1666, 0.1666, 0.1666, 0.1666]\n",
    "\n",
    "    '''\n",
    "    dice_loss = sm.losses.DiceLoss(class_weights = weights)\n",
    "    focal_loss = sm.losses.CategoricalFocalLoss()\n",
    "    total_loss = dice_loss + (1 * focal_loss)\n",
    "    '''\n",
    "\n",
    "    # Raw losses from segmentation_models\n",
    "    raw_dice = sm.losses.DiceLoss(class_weights=weights)\n",
    "    raw_focal = sm.losses.CategoricalFocalLoss()\n",
    "\n",
    "    # Final loss function with label smoothing applied\n",
    "    def total_loss_with_smoothing(y_true, y_pred):\n",
    "        y_true_smoothed = apply_label_smoothing(y_true, smoothing=0.1)\n",
    "        return raw_dice(y_true_smoothed, y_pred) + raw_focal(y_true_smoothed, y_pred)\n",
    "\n",
    "\n",
    "    # --- Jaccard Index ---\n",
    "    from tensorflow.keras.metrics import MeanIoU\n",
    "\n",
    "    # Custom wrapper for one-hot -> argmax conversion\n",
    "    class MeanIoUMetric(tf.keras.metrics.MeanIoU):\n",
    "        def __init__(self, num_classes, name=\"mean_iou\", dtype=None):\n",
    "            super().__init__(num_classes=num_classes, name=name, dtype=dtype)\n",
    "\n",
    "        def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "            y_true = tf.argmax(y_true, axis=-1)\n",
    "            y_pred = tf.argmax(y_pred, axis=-1)\n",
    "            return super().update_state(y_true, y_pred, sample_weight)\n",
    "\n",
    "    miou_metric = MeanIoUMetric(num_classes=6)\n",
    "\n",
    "    if model_type == \"unet_aux\":\n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss={'main_output': total_loss_with_smoothing, 'aux_output': total_loss_with_smoothing},\n",
    "            loss_weights={'main_output': 1.0, 'aux_output': 0.4},\n",
    "            metrics={'main_output': miou_metric, 'aux_output': miou_metric}\n",
    "        )\n",
    "    else:\n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss=total_loss_with_smoothing,\n",
    "            metrics=[miou_metric, 'categorical_accuracy', 'accuracy']\n",
    "        )\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "        \n",
    "    early_stop = EarlyStopping(monitor='val_mean_iou', patience=16, restore_best_weights=True, mode='max')\n",
    "\n",
    "    # reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=8, min_lr=1e-6, verbose=1)\n",
    "    # early_stop = EarlyStopping(monitor=total_loss, patience=16, restore_best_weights=True)\n",
    "\n",
    "    os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "    checkpoint = ModelCheckpoint(\"checkpoints/best_model.h5\", monitor='val_mean_iou', save_best_only=True)\n",
    "    nan_terminate = TerminateOnNaN()\n",
    "\n",
    "\n",
    "    class TimeLimitCallback(tf.keras.callbacks.Callback):\n",
    "        def __init__(self, max_minutes=20):\n",
    "            super().__init__()\n",
    "            self.max_duration = max_minutes * 60\n",
    "        def on_train_begin(self, logs=None):\n",
    "            self.start_time = tf.timestamp()\n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            elapsed = tf.timestamp() - self.start_time\n",
    "            if elapsed > self.max_duration:\n",
    "                print(f\"⏱️ Training time exceeded {self.max_duration // 60} minutes. Stopping early.\")\n",
    "                self.model.stop_training = True\n",
    "\n",
    "\n",
    "    from collections import defaultdict\n",
    "    CLASS_NAMES = ['Building', 'Clutter', 'Vegetation', 'Water', 'Background', 'Car']\n",
    "\n",
    "    class DistributionLogger(tf.keras.callbacks.Callback):\n",
    "        def __init__(self, generator, name=\"Training\", max_batches=16):\n",
    "            super().__init__()\n",
    "            self.generator = generator\n",
    "            self.name = name\n",
    "            self.max_batches = max_batches\n",
    "            self.cumulative_class_counts = defaultdict(int)\n",
    "\n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            batch_class_counts = defaultdict(int)\n",
    "            batches_seen = 0\n",
    "\n",
    "            for batch_images, batch_labels in self.generator:\n",
    "                if batches_seen >= self.max_batches:\n",
    "                    break\n",
    "                unique, counts = np.unique(np.argmax(batch_labels, axis=-1), return_counts=True)\n",
    "                for u, c in zip(unique, counts):\n",
    "                    batch_class_counts[u] += c\n",
    "                    self.cumulative_class_counts[u] += c\n",
    "                batches_seen += 1\n",
    "\n",
    "            total_pixels = sum(batch_class_counts.values())\n",
    "            total_images = batches_seen * self.generator.batch_size\n",
    "\n",
    "            print(f\"\\n📊 {self.name} Distribution After Epoch {epoch + 1} ({total_images:,} images):\")\n",
    "            for cls in sorted(batch_class_counts):\n",
    "                count = batch_class_counts[cls]\n",
    "                percent = 100.0 * count / total_pixels\n",
    "                print(f\"  Class {cls} ({CLASS_NAMES[cls]}): {count:,} px ({percent:.2f}%)\")\n",
    "\n",
    "        def on_train_end(self, logs=None):\n",
    "            total_pixels = sum(self.cumulative_class_counts.values())\n",
    "\n",
    "            print(\"\\n📊 Final Cumulative Training Class Distribution:\")\n",
    "            print(f\"  Total pixels: {total_pixels:,} px\")\n",
    "            for cls in sorted(self.cumulative_class_counts):\n",
    "                count = self.cumulative_class_counts[cls]\n",
    "                percent = 100.0 * count / total_pixels\n",
    "                print(f\"  Class {cls} ({CLASS_NAMES[cls]}): {count:,} px ({percent:.2f}%)\")\n",
    "\n",
    "            plot_class_distribution(self.cumulative_class_counts)\n",
    "\n",
    "\n",
    "    class LearningRateLogger(tf.keras.callbacks.Callback):\n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            lr = self.model.optimizer._decayed_lr(tf.float32).numpy()\n",
    "            print(f\"Learning Rate at epoch {epoch + 1}: {lr:.6f}\")\n",
    "\n",
    "\n",
    "    time_limit = TimeLimitCallback(max_minutes=train_time)\n",
    "\n",
    "    callbacks = [checkpoint, early_stop, nan_terminate, time_limit, ClearMemory(),\n",
    "                 DistributionLogger(train_gen, name=\"Training\", max_batches=steps),\n",
    "                 LearningRateLogger()]\n",
    "\n",
    "    print(\"🚀 Starting training...\")\n",
    "    history = model.fit(\n",
    "        train_gen, validation_data=val_gen,\n",
    "        epochs=epochs,\n",
    "        callbacks=callbacks,\n",
    "        verbose=verbose\n",
    "    )\n",
    "\n",
    "\n",
    "    # --- Evaluate Model ---\n",
    "    from tqdm import trange\n",
    "    \n",
    "    # 📈 Plotting Training Curves for Mean IoU and val loss \n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    epochs = range(1, len(loss) + 1)\n",
    "    plt.plot(epochs, loss, 'y', label=\"Training Loss\")\n",
    "    plt.plot(epochs, val_loss, 'r', label=\"Validation Loss\")\n",
    "    plt.title(\"Training Vs Validation Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(out_dir, \"loss_plot.png\"))\n",
    "    plt.show()\n",
    "\n",
    "    mean_iou = history.history['mean_iou']\n",
    "    val_mean_iou = history.history['val_mean_iou']\n",
    "    epochs = range(1, len(mean_iou) + 1)\n",
    "\n",
    "    plt.plot(epochs, mean_iou, 'b', label=\"Training mIoU\")\n",
    "    plt.plot(epochs, val_mean_iou, 'g', label=\"Validation mIoU\")\n",
    "    plt.title(\"Training vs Validation Mean IoU\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"mIoU\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(out_dir, \"mIoU_plot.png\"))\n",
    "    plt.show()\n",
    "\n",
    "    # 📈 Plotting Random Outputs, Confusion Matrix, Classification Report and mIoU\n",
    "    test_gen = StreamingDataGenerator(test_images, test_elev, test_labels,\n",
    "                                   batch_size=batch_size, steps=steps, input_type=input_type,\n",
    "                                   shuffle=False,\n",
    "                                   fixed=True, augment=False)\n",
    "\n",
    "    evaluate_on_test(model, test_gen, n_vis=30)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # --- Archive ---\n",
    "\n",
    "    '''\n",
    "    class_weights = compute_class_weights(train_gen)\n",
    "    loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n",
    "\n",
    "    def weighted_loss(y_true, y_pred):\n",
    "        weights = tf.reduce_sum(class_weights * y_true, axis=-1)\n",
    "        return tf.reduce_mean(weights * loss_fn(y_true, y_pred))\n",
    "\n",
    "    model.compile(optimizer='adam', loss=weighted_loss, metrics=['accuracy'])\n",
    "    print(\"✅ Model compiled with class weights\")\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    from segmentation_models.metrics import iou_score as jaccard_coef\n",
    "    metrics = [\"accuracy\", jaccard_coef]\n",
    "    weights = [0.1666, 0.1666, 0.1666, 0.1666, 0.1666, 0.1666]\n",
    "    \n",
    "    dice_loss = dice_loss(class_weights = weights)\n",
    "    focal_loss = categorical_focal_loss()\n",
    "    total_loss = dice_loss + (1 * focal_loss)\n",
    "    \n",
    "    model.compile(optimizer=\"adam\", loss=\"total_loss\", metrics=metrics)\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    def jaccard_coef(y_true, y_pred):\n",
    "    y_true_flatten = K.flatten(y_true)\n",
    "    y_pred_flatten = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_flatten * y_pred_flatten)\n",
    "    final_coef_value = (intersection + 1.0) / (K.sum(y_true_flatten) + K.sum(y_pred_flatten) - intersection + 1.0)\n",
    "    return final_coef_value\n",
    "    '''"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPw4dXqbrOaTut/+CDo+nZ1",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
