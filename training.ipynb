{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TerminateOnNaN\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import gc\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.schedules import LearningRateSchedule\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "import segmentation_models as sm\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- Sanity ---\n",
    "def test_training_sanity():\n",
    "    print(\"‚úÖ from training.ipynb\")\n",
    "\n",
    "\n",
    "val_files = [\n",
    "    \"1476907971_CHADGRISMOPENPIPELINE\",\n",
    "    \"dabec5e872_E8AD935CEDINSPIRE\",\n",
    "    \"c6d131e346_536DE05ED2OPENPIPELINE\",\n",
    "    \"57426ebe1e_84B52814D2OPENPIPELINE\",\n",
    "    \"1726eb08ef_60693DB04DINSPIRE\",\n",
    "    \"9170479165_625EDFBAB6OPENPIPELINE\",\n",
    "    \"520947aa07_8FCB044F58OPENPIPELINE\",\n",
    "    \"cc4b443c7d_A9CBEF2C97INSPIRE\",\n",
    "    \"12fa5e614f_53197F206FOPENPIPELINE\",\n",
    "    \"2ef3a4994a_0CCD105428INSPIRE\",\n",
    "]\n",
    "\n",
    "test_files = [\n",
    "    \"1d4fbe33f3_F1BE1D4184INSPIRE\",\n",
    "    \"f9f43e5144_1DB9E6F68BINSPIRE\",\n",
    "    \"25f1c24f30_EB81FE6E2BOPENPIPELINE\",\n",
    "    \"a1af86939f_F1BE1D4184OPENPIPELINE\",\n",
    "    \"1553541487_APIGENERATED\",\n",
    "    \"74d7796531_EB81FE6E2BOPENPIPELINE\",\n",
    "    \"8710b98ea0_06E6522D6DINSPIRE\",\n",
    "    \"c644f91210_27E21B7F30OPENPIPELINE\",\n",
    "    \"d9161f7e18_C05BA1BC72OPENPIPELINE\", \n",
    "]\n",
    "\n",
    "\n",
    "INPUT_TYPE_CONFIG = {\n",
    "    \"1ch\": {\"description\": \"grayscale only\", \"channels\": 1},\n",
    "    \"2ch\": {\"description\": \"grayscale + elevation\", \"channels\": 2},\n",
    "    \"rgb\": {\"description\": \"RGB only\", \"channels\": 3},\n",
    "    \"rgb_elevation\": {\"description\": \"RGB + elevation\", \"channels\": 4}\n",
    "}\n",
    "\n",
    "COLOR_TO_CLASS = {\n",
    "    (230, 25, 75): 0,\n",
    "    (145, 30, 180): 1,\n",
    "    (60, 180, 75): 2,\n",
    "    (245, 130, 48): 3,\n",
    "    (255, 255, 255): 4,\n",
    "    (0, 130, 200): 5\n",
    "}\n",
    "\n",
    "CLASS_TO_COLOR = {v: k for k, v in COLOR_TO_CLASS.items()}\n",
    "NUM_CLASSES = len(COLOR_TO_CLASS)\n",
    "COLOR_PALETTE = np.array(list(COLOR_TO_CLASS.keys()), dtype=np.uint8)\n",
    "COLOR_LOOKUP = {tuple(c): i for c, i in COLOR_TO_CLASS.items()}\n",
    "\n",
    "def decode_label_image(label_img):\n",
    "    h, w, _ = label_img.shape\n",
    "    label_map = np.zeros((h, w), dtype=np.uint8)\n",
    "    for y in range(h):\n",
    "        for x in range(w):\n",
    "            pixel = tuple(label_img[y, x])\n",
    "            if pixel not in COLOR_LOOKUP:\n",
    "                raise ValueError(f\"‚ùå Unknown label colour {pixel} at ({y}, {x})\")\n",
    "            label_map[y, x] = COLOR_LOOKUP[pixel]\n",
    "    return label_map\n",
    "\n",
    "\n",
    "def filter_tile_ids_by_substring(image_dir, base_names):\n",
    "    return [f.replace('-ortho.png', '') for f in os.listdir(image_dir) if any(base in f for base in base_names)]\n",
    "\n",
    "\n",
    "def train_model(base_dir=\"/content/chipped_data/content/chipped_data\", out_dir=\"/content/figs\", \n",
    "                input_type=\"rgb_elevation\", model_type=\"unet\", tile_size=256,\n",
    "                batch_size=8, steps=None, epochs=10, train_time=20, verbose=1\n",
    "                ):\n",
    "    \n",
    "    assert input_type in INPUT_TYPE_CONFIG, f\"Unknown input type: {input_type}\"\n",
    "    num_channels = INPUT_TYPE_CONFIG[input_type][\"channels\"]\n",
    "\n",
    "    train_images = os.path.join(base_dir, \"train\", \"images\")\n",
    "    train_elev = os.path.join(base_dir, \"train\", \"elevations\")\n",
    "    train_labels = os.path.join(base_dir, \"train\", \"labels\")\n",
    "\n",
    "    eval_images = os.path.join(base_dir, \"raw\", \"images\")\n",
    "    eval_elev = os.path.join(base_dir, \"raw\", \"elevations\")\n",
    "    eval_labels = os.path.join(base_dir, \"raw\", \"labels\")\n",
    "\n",
    "    val_images = os.path.join(base_dir, \"raw\", \"images\")\n",
    "    val_elev = os.path.join(base_dir, \"raw\", \"elevations\")\n",
    "    val_labels = os.path.join(base_dir, \"raw\", \"labels\")\n",
    "\n",
    "    test_images = os.path.join(base_dir, \"raw\", \"images\")\n",
    "    test_elev = os.path.join(base_dir, \"raw\", \"elevations\")\n",
    "    test_labels = os.path.join(base_dir, \"raw\", \"labels\")\n",
    "\n",
    "\n",
    "\n",
    "    # --- Streaming Data Generator ---\n",
    "    train_gen = StreamingDataGenerator(\n",
    "        train_images, train_elev, train_labels,\n",
    "        split='train',\n",
    "        val_files=val_files,\n",
    "        test_files=test_files,\n",
    "        batch_size=batch_size,\n",
    "        input_type=input_type,\n",
    "        shuffle=True,\n",
    "        steps=steps,\n",
    "        fixed=False,\n",
    "        augment=True,\n",
    "        metadata_csv_path=\"/content/chipped_data/content/chipped_data/train_metadata.csv\"\n",
    "    )\n",
    "\n",
    "    val_tile_ids = filter_tile_ids_by_substring(os.path.join(base_dir, \"train\", \"images\"), val_files)\n",
    "    test_tile_ids = filter_tile_ids_by_substring(os.path.join(base_dir, \"train\", \"images\"), test_files)\n",
    "\n",
    "    val_tile_ids = filter_tile_ids_by_substring(os.path.join(base_dir, \"raw\", \"images\"), val_files)\n",
    "    test_tile_ids = filter_tile_ids_by_substring(os.path.join(base_dir, \"raw\", \"images\"), test_files)\n",
    "\n",
    "\n",
    "    # val_tile_ids = [f.replace('-ortho.png', '') for f in os.listdir(val_images) if any(f.startswith(v) for v in val_files)]\n",
    "    val_gen = StreamingDataGenerator(eval_images, eval_elev, eval_labels,\n",
    "                                     split='val', val_files=val_tile_ids, test_files=test_files,\n",
    "                                     batch_size=8, steps=len(val_tile_ids) // 8 + 1,\n",
    "                                     input_type=input_type, shuffle=False, fixed=True, augment=False)\n",
    "\n",
    "    # test_tile_ids = [f.replace('-ortho.png', '') for f in os.listdir(test_images) if any(f.startswith(v) for v in test_files)]\n",
    "    test_gen = StreamingDataGenerator(eval_images, eval_elev, eval_labels,\n",
    "                                      split='test', val_files=val_files, test_files=test_tile_ids,\n",
    "                                      batch_size=8, steps=len(test_tile_ids) // 8 + 1,\n",
    "                                      input_type=input_type, shuffle=False, fixed=True, augment=False)\n",
    "\n",
    "\n",
    "    #input_shape = (tile_size, tile_size, num_channels)\n",
    "    input_shape = (None, None, num_channels)\n",
    "\n",
    "    # --- Model ---\n",
    "    if model_type == \"unet\":\n",
    "        print(\"üß™ Calling build_unet...\")\n",
    "        model = build_unet(input_shape=input_shape, num_classes=NUM_CLASSES)\n",
    "\n",
    "    elif model_type == \"multi_unet\":\n",
    "        print(\"üß™ Calling build_multi_unet...\")\n",
    "        model = build_multi_unet(input_shape=input_shape, num_classes=NUM_CLASSES)\n",
    "\n",
    "    elif model_type == \"unet_aux\":\n",
    "        print(\"üß™ Calling build_multi_unet_aux...\")\n",
    "        model = build_unet_aux(input_shape=input_shape, num_classes=NUM_CLASSES)\n",
    "\n",
    "    elif model_type == \"segformer\":\n",
    "        model = build_segformer(input_shape=input_shape, num_classes=NUM_CLASSES)\n",
    "    \n",
    "    elif model_type == \"CRF\":\n",
    "        model = build_crf(input_shape=input_shape, num_classes=NUM_CLASSES)\n",
    "    \n",
    "    elif model_type == \"resnet34\":\n",
    "        import segmentation_models as sm\n",
    "        model = sm.Unet(\n",
    "            backbone_name=\"resnet34\",          # or 'efficientnetb0', 'mobilenetv2', etc.\n",
    "            input_shape=input_shape,\n",
    "            classes=NUM_CLASSES,                  \n",
    "            activation='softmax', \n",
    "            encoder_weights='imagenet'         # Load ImageNet pre-trained weights\n",
    "        )\n",
    "    \n",
    "    elif model_type == \"model1\":\n",
    "        model = build_model_1(input_shape=input_shape, num_classes=NUM_CLASSES)\n",
    "    \n",
    "    elif model_type == \"model2\":\n",
    "        model = build_model_2(input_shape=input_shape, num_classes=NUM_CLASSES)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model_type: {model_type}\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # --- Metrics ---\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TerminateOnNaN\n",
    "    \n",
    "    # Custom learning rate schedule\n",
    "    class TransformerLRSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "        def __init__(self, d_model, warmup_steps=4000):\n",
    "            super().__init__()\n",
    "            self.d_model = tf.cast(d_model, tf.float32)\n",
    "            self.warmup_steps = tf.cast(warmup_steps, tf.float32)\n",
    "\n",
    "        def __call__(self, step):\n",
    "            step = tf.cast(step, tf.float32)\n",
    "            arg1 = tf.math.rsqrt(step)\n",
    "            arg2 = step * tf.pow(self.warmup_steps, -1.5)\n",
    "            return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "        def get_config(self):\n",
    "            return {\n",
    "                \"d_model\": self.d_model.numpy(),  # convert back to Python float\n",
    "                \"warmup_steps\": self.warmup_steps.numpy()\n",
    "            }\n",
    "\n",
    "\n",
    "    # Instantiate learning rate schedule and optimizer\n",
    "    lr_schedule = TransformerLRSchedule(d_model=tile_size)\n",
    "    optimizer = Adam(learning_rate=lr_schedule, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "    # Loss function with label smoothing\n",
    "    # loss_fn = CategoricalCrossentropy(label_smoothing=0.1)\n",
    "\n",
    "    def apply_label_smoothing(y_true, smoothing=0.1):\n",
    "        num_classes = tf.cast(tf.shape(y_true)[-1], tf.float32)\n",
    "        return y_true * (1.0 - smoothing) + (smoothing / num_classes)\n",
    "\n",
    "\n",
    "    os.environ[\"SM_FRAMEWORK\"] = \"tf.keras\"\n",
    "\n",
    "    # Set class weights\n",
    "    weights = [0.1666, 0.1666, 0.1666, 0.1666, 0.1666, 0.1666]\n",
    "\n",
    "    '''\n",
    "    dice_loss = sm.losses.DiceLoss(class_weights = weights)\n",
    "    focal_loss = sm.losses.CategoricalFocalLoss()\n",
    "    total_loss = dice_loss + (1 * focal_loss)\n",
    "    '''\n",
    "\n",
    "    # Raw losses from segmentation_models\n",
    "    raw_dice = sm.losses.DiceLoss(class_weights=weights)\n",
    "    raw_focal = sm.losses.CategoricalFocalLoss()\n",
    "\n",
    "    # Final loss function with label smoothing applied\n",
    "    def total_loss_with_smoothing(y_true, y_pred):\n",
    "        y_true_smoothed = apply_label_smoothing(y_true, smoothing=0.1)\n",
    "        return raw_dice(y_true_smoothed, y_pred) + raw_focal(y_true_smoothed, y_pred)\n",
    "\n",
    "\n",
    "\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.metrics import MeanIoU\n",
    "\n",
    "    # --- Jaccard Index ---\n",
    "    '''\n",
    "    class MeanIoUMetric(tf.keras.metrics.MeanIoU):\n",
    "        def __init__(self, num_classes, name=\"mean_iou\", dtype=None):\n",
    "            super().__init__(num_classes=num_classes, name=name, dtype=dtype)\n",
    "\n",
    "        def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "            y_true = tf.argmax(y_true, axis=-1)\n",
    "            y_pred = tf.argmax(y_pred, axis=-1)\n",
    "            return super().update_state(y_true, y_pred, sample_weight)\n",
    "\n",
    "    miou_metric = MeanIoUMetric(num_classes=6)\n",
    "    '''\n",
    "\n",
    "    class MaskedMeanIoU(tf.keras.metrics.Metric):\n",
    "        def __init__(self, num_classes, name=\"masked_mean_iou\", **kwargs):\n",
    "            super(MaskedMeanIoU, self).__init__(name=name, **kwargs)\n",
    "            self.num_classes = num_classes\n",
    "            self.total_cm = self.add_weight(\n",
    "                name=\"total_confusion_matrix\",\n",
    "                shape=(num_classes, num_classes),\n",
    "                initializer=\"zeros\",\n",
    "                dtype=tf.float32,\n",
    "            )\n",
    "\n",
    "        def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "            y_true = tf.argmax(y_true, axis=-1)\n",
    "            y_pred = tf.argmax(y_pred, axis=-1)\n",
    "\n",
    "            # MASK: Ignore pixels labeled with 6 (your ignore class)\n",
    "            mask = tf.not_equal(y_true, 6)\n",
    "            y_true = tf.boolean_mask(y_true, mask)\n",
    "            y_pred = tf.boolean_mask(y_pred, mask)\n",
    "\n",
    "            current_cm = tf.math.confusion_matrix(y_true, y_pred, num_classes=self.num_classes, dtype=tf.float32)\n",
    "            self.total_cm.assign_add(current_cm)\n",
    "\n",
    "        def result(self):\n",
    "            sum_over_row = tf.reduce_sum(self.total_cm, axis=0)\n",
    "            sum_over_col = tf.reduce_sum(self.total_cm, axis=1)\n",
    "            true_positives = tf.linalg.diag_part(self.total_cm)\n",
    "            denominator = sum_over_row + sum_over_col - true_positives\n",
    "\n",
    "            iou = tf.math.divide_no_nan(true_positives, denominator)\n",
    "            return tf.reduce_mean(iou)\n",
    "\n",
    "        def reset_states(self):\n",
    "            tf.keras.backend.set_value(self.total_cm, tf.zeros((self.num_classes, self.num_classes)))\n",
    "\n",
    "        \n",
    "    miou_metric = MaskedMeanIoU(num_classes=NUM_CLASSES)\n",
    "    metrics=[\n",
    "        miou_metric,\n",
    "        sm.metrics.IOUScore(threshold=None),\n",
    "        sm.metrics.FScore(threshold=None),\n",
    "        'categorical_accuracy',\n",
    "        'accuracy'\n",
    "    ]\n",
    "\n",
    "    # --- Compile Model ---\n",
    "    if model_type == \"unet_aux\":\n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss={'main_output': total_loss_with_smoothing, 'aux_output': total_loss_with_smoothing},\n",
    "            loss_weights={'main_output': 1.0, 'aux_output': 0.4},\n",
    "            metrics={'main_output': miou_metric, 'aux_output': miou_metric}\n",
    "        )\n",
    "\n",
    "    elif model_type == \"resnet34\":\n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss=total_loss_with_smoothing,\n",
    "            metrics=metrics\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss=total_loss_with_smoothing,\n",
    "            metrics=metrics\n",
    "        )\n",
    "\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "\n",
    "    # --- Callbacks --- \n",
    "    class ClearMemory(tf.keras.callbacks.Callback):\n",
    "        def on_train_batch_end(self, batch, logs=None):\n",
    "            gc.collect()\n",
    "            K.clear_session()\n",
    "\n",
    "    class LearningRateLogger(tf.keras.callbacks.Callback):\n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            lr = self.model.optimizer.learning_rate.numpy()\n",
    "            print(f\"üìâ Learning Rate at epoch {epoch + 1}: {lr:.6f}\")\n",
    "\n",
    "    class TimeLimitCallback(tf.keras.callbacks.Callback):\n",
    "        def __init__(self, max_minutes=20):\n",
    "            super().__init__()\n",
    "            self.max_duration = max_minutes * 60\n",
    "        def on_train_begin(self, logs=None):\n",
    "            self.start_time = tf.timestamp()\n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            elapsed = tf.timestamp() - self.start_time\n",
    "            if elapsed > self.max_duration:\n",
    "                print(f\"‚è±Ô∏è Training time exceeded {self.max_duration // 60} minutes. Stopping early.\")\n",
    "                self.model.stop_training = True\n",
    "\n",
    "    time_limit = TimeLimitCallback(max_minutes=train_time)\n",
    "    early_stop = EarlyStopping(monitor='val_mean_iou', patience=12, restore_best_weights=True, mode='max')\n",
    "    nan_terminate = TerminateOnNaN()\n",
    "\n",
    "    # Create both output folders\n",
    "    os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "    os.makedirs(\"/content/drive/MyDrive/segmentation_checkpoints\", exist_ok=True)\n",
    "\n",
    "    # Checkpoint in Colab workspace\n",
    "    checkpoint_local = ModelCheckpoint(\n",
    "        \"checkpoints/best_model.h5\",\n",
    "        monitor='val_mean_iou',\n",
    "        save_best_only=True\n",
    "    )\n",
    "\n",
    "    # Checkpoint in Google Drive\n",
    "    checkpoint_drive = ModelCheckpoint(\n",
    "        \"/content/drive/MyDrive/segmentation_checkpoints/best_model.h5\",\n",
    "        monitor='val_mean_iou',\n",
    "        save_best_only=True\n",
    "    )\n",
    "\n",
    "    CLASS_NAMES = ['Building', 'Clutter', 'Vegetation', 'Water', 'Background', 'Car']\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    class DistributionLogger(tf.keras.callbacks.Callback):\n",
    "        def __init__(self, generator, name=\"Training\", max_batches=16, visualise_samples=2):\n",
    "            super().__init__()\n",
    "            self.generator = generator\n",
    "            self.name = name\n",
    "            self.max_batches = max_batches\n",
    "            self.visualise_samples = visualise_samples\n",
    "            self.cumulative_class_counts = defaultdict(int)\n",
    "\n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            batch_class_counts = defaultdict(int)\n",
    "            all_samples = []\n",
    "            batches_seen = 0\n",
    "\n",
    "            for batch_images, batch_labels in self.generator:\n",
    "                if batches_seen >= self.max_batches:\n",
    "                    break\n",
    "\n",
    "                batch_preds = np.argmax(batch_labels, axis=-1)\n",
    "                unique, counts = np.unique(batch_preds, return_counts=True)\n",
    "\n",
    "                for u, c in zip(unique, counts):\n",
    "                    batch_class_counts[u] += c\n",
    "                    self.cumulative_class_counts[u] += c\n",
    "\n",
    "                for img, label in zip(batch_images, batch_preds):\n",
    "                    all_samples.append((img, label))\n",
    "\n",
    "                batches_seen += 1\n",
    "\n",
    "            total_pixels = sum(batch_class_counts.values())\n",
    "            total_images = batches_seen * self.generator.batch_size\n",
    "\n",
    "            print(f\"üìä {self.name} Distribution After Epoch {epoch + 1} ({total_images:,} images):\")\n",
    "            for cls in sorted(batch_class_counts):\n",
    "                count = batch_class_counts[cls]\n",
    "                percent = 100.0 * count / total_pixels\n",
    "                print(f\"  Class {cls} ({CLASS_NAMES[cls]}): {count:,} px ({percent:.2f}%)\")\n",
    "\n",
    "            self._plot_random_samples(all_samples, epoch)\n",
    "\n",
    "        def _plot_random_samples(self, all_samples, epoch):\n",
    "            if len(all_samples) < self.visualise_samples:\n",
    "                return\n",
    "\n",
    "            samples_to_show = random.sample(all_samples, self.visualise_samples)\n",
    "            fig, axs = plt.subplots(self.visualise_samples, 2, figsize=(8, 4 * self.visualise_samples))\n",
    "\n",
    "            for i, (image, label) in enumerate(samples_to_show):\n",
    "                img = (image * 255).astype(np.uint8)\n",
    "                axs[i, 0].imshow(img if img.shape[-1] == 3 else img[:, :, 0], cmap='gray')\n",
    "                axs[i, 0].set_title(\"Input Image\")\n",
    "                axs[i, 0].axis('off')\n",
    "\n",
    "                label_rgb = np.zeros((label.shape[0], label.shape[1], 3), dtype=np.uint8)\n",
    "                for class_id, color in CLASS_TO_COLOR.items():\n",
    "                    label_rgb[label == class_id] = color\n",
    "                axs[i, 1].imshow(label_rgb)\n",
    "                axs[i, 1].set_title(\"Label\")\n",
    "                axs[i, 1].axis('off')\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.suptitle(f\"üîç Sample Batches from Epoch {epoch + 1}\", y=1.02)\n",
    "            plt.show()\n",
    "\n",
    "        def on_train_end(self, logs=None):\n",
    "            total_pixels = sum(self.cumulative_class_counts.values())\n",
    "            print(\"üìä Final Cumulative Training Class Distribution:\")\n",
    "            print(f\"Total pixels: {total_pixels:,} px\")\n",
    "            for cls in sorted(self.cumulative_class_counts):\n",
    "                count = self.cumulative_class_counts[cls]\n",
    "                percent = 100.0 * count / total_pixels\n",
    "                print(f\"  Class {cls} ({CLASS_NAMES[cls]}): {count:,} px ({percent:.2f}%)\")\n",
    "\n",
    "            plot_class_distribution(self.cumulative_class_counts)\n",
    "\n",
    "\n",
    "    class ValidationPredictionLogger(tf.keras.callbacks.Callback):\n",
    "        def __init__(self, val_gen, model, max_batches=1):\n",
    "            super().__init__()\n",
    "            self.val_gen = val_gen\n",
    "            self.model = model\n",
    "            self.max_batches = max_batches\n",
    "\n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            batches_seen = 0\n",
    "            for batch_images, batch_labels in self.val_gen:\n",
    "                if batches_seen >= self.max_batches:\n",
    "                    break\n",
    "                preds = self.model.predict(batch_images)\n",
    "                preds_argmax = np.argmax(preds, axis=-1)\n",
    "                true_argmax = np.argmax(batch_labels, axis=-1)\n",
    "\n",
    "                fig, axs = plt.subplots(len(batch_images), 3, figsize=(10, 3 * len(batch_images)))\n",
    "                for i in range(len(batch_images)):\n",
    "                    axs[i, 0].imshow((batch_images[i] * 255).astype(np.uint8))\n",
    "                    axs[i, 0].set_title(\"Input\")\n",
    "                    axs[i, 0].axis('off')\n",
    "\n",
    "                    true_rgb = np.zeros((*true_argmax[i].shape, 3), dtype=np.uint8)\n",
    "                    pred_rgb = np.zeros((*preds_argmax[i].shape, 3), dtype=np.uint8)\n",
    "                    for cid, col in CLASS_TO_COLOR.items():\n",
    "                        true_rgb[true_argmax[i] == cid] = col\n",
    "                        pred_rgb[preds_argmax[i] == cid] = col\n",
    "\n",
    "                    axs[i, 1].imshow(true_rgb)\n",
    "                    axs[i, 1].set_title(\"Ground Truth\")\n",
    "                    axs[i, 1].axis('off')\n",
    "                    axs[i, 2].imshow(pred_rgb)\n",
    "                    axs[i, 2].set_title(\"Prediction\")\n",
    "                    axs[i, 2].axis('off')\n",
    "\n",
    "                plt.tight_layout()\n",
    "                plt.suptitle(f\"üîç Validation Predictions After Epoch {epoch + 1}\", y=1.02)\n",
    "                plt.show()\n",
    "                batches_seen += 1\n",
    "\n",
    "    class StepTimer(tf.keras.callbacks.Callback):\n",
    "        def on_train_begin(self, logs=None):\n",
    "            self.total_time = 0.0\n",
    "            self.total_steps = 0\n",
    "\n",
    "        def on_train_batch_begin(self, batch, logs=None):\n",
    "            self.start_time = tf.timestamp()\n",
    "\n",
    "        def on_train_batch_end(self, batch, logs=None):\n",
    "            elapsed = tf.timestamp() - self.start_time\n",
    "            self.total_time += elapsed\n",
    "            self.total_steps += 1\n",
    "\n",
    "        def on_train_end(self, logs=None):\n",
    "            avg_step_time = self.total_time / self.total_steps\n",
    "            print(f\"üïí Average training step time: {avg_step_time:.4f} sec\")\n",
    "\n",
    "\n",
    "    callbacks = [\n",
    "        checkpoint_local, checkpoint_drive, \n",
    "        early_stop, nan_terminate, time_limit, \n",
    "        ClearMemory(), \n",
    "        LearningRateLogger(),\n",
    "        StepTimer(),\n",
    "        DistributionLogger(train_gen, name=\"Training\", max_batches=steps, visualise_samples=4),\n",
    "        ValidationPredictionLogger(val_gen, model, max_batches=1)\n",
    "    ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # --- Evaluate Model ---\n",
    "    print(\"üöÄ Starting training...\")\n",
    "    history = model.fit(\n",
    "        train_gen, validation_data=val_gen,\n",
    "        epochs=epochs,\n",
    "        callbacks=callbacks,\n",
    "        verbose=verbose\n",
    "    )\n",
    "\n",
    "    from tqdm import trange\n",
    "    \n",
    "    # üìà Plotting Training Curves for Mean IoU and val loss \n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    epochs = range(1, len(loss) + 1)\n",
    "    plt.plot(epochs, loss, 'y', label=\"Training Loss\")\n",
    "    plt.plot(epochs, val_loss, 'r', label=\"Validation Loss\")\n",
    "    plt.title(\"Training Vs Validation Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(out_dir, \"loss_plot.png\"))\n",
    "    plt.show()\n",
    "\n",
    "    '''\n",
    "    mean_iou = history.history['masked_mean_iou']\n",
    "    val_mean_iou = history.history['val_masked_mean_iou']\n",
    "    epochs = range(1, len(mean_iou) + 1)\n",
    "\n",
    "    plt.plot(epochs, masked_mean_iou, 'b', label=\"Training mIoU\")\n",
    "    plt.plot(epochs, val_masked_mean_iou, 'g', label=\"Validation mIoU\")\n",
    "    plt.title(\"Training vs Validation Mean IoU\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"mIoU\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(out_dir, \"mIoU_plot.png\"))\n",
    "    plt.show()\n",
    "    '''\n",
    "\n",
    "    # üìà Plotting Random Outputs, Confusion Matrix, Classification Report and mIoU\n",
    "    evaluate_on_test(model, test_gen, n_vis=9)\n",
    "\n",
    "    \n",
    "    def measure_inference_time(model, generator, num_batches=5):\n",
    "        import time\n",
    "        total_time = 0\n",
    "        total_images = 0\n",
    "\n",
    "        for i, (x_batch, _) in enumerate(generator):\n",
    "            if i >= num_batches:\n",
    "                break\n",
    "            start = time.time()\n",
    "            _ = model.predict(x_batch, verbose=0)\n",
    "            end = time.time()\n",
    "            total_time += (end - start)\n",
    "            total_images += x_batch.shape[0]\n",
    "\n",
    "\n",
    "    print(f\"üß† Inference time: {total_time:.2f} sec for {total_images} images\")\n",
    "    print(f\"‚è±Ô∏è Avg inference time per image: {total_time / total_images:.4f} sec\")\n",
    "    measure_inference_time(model, test_gen, num_batches=steps)\n",
    "    print(\"üöÄ Training complete!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # --- Archive ---\n",
    "\n",
    "    '''\n",
    "    class_weights = compute_class_weights(train_gen)\n",
    "    loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n",
    "\n",
    "    def weighted_loss(y_true, y_pred):\n",
    "        weights = tf.reduce_sum(class_weights * y_true, axis=-1)\n",
    "        return tf.reduce_mean(weights * loss_fn(y_true, y_pred))\n",
    "\n",
    "    model.compile(optimizer='adam', loss=weighted_loss, metrics=['accuracy'])\n",
    "    print(\"‚úÖ Model compiled with class weights\")\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    from segmentation_models.metrics import iou_score as jaccard_coef\n",
    "    metrics = [\"accuracy\", jaccard_coef]\n",
    "    weights = [0.1666, 0.1666, 0.1666, 0.1666, 0.1666, 0.1666]\n",
    "    \n",
    "    dice_loss = dice_loss(class_weights = weights)\n",
    "    focal_loss = categorical_focal_loss()\n",
    "    total_loss = dice_loss + (1 * focal_loss)\n",
    "    \n",
    "    model.compile(optimizer=\"adam\", loss=\"total_loss\", metrics=metrics)\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    def jaccard_coef(y_true, y_pred):\n",
    "    y_true_flatten = K.flatten(y_true)\n",
    "    y_pred_flatten = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_flatten * y_pred_flatten)\n",
    "    final_coef_value = (intersection + 1.0) / (K.sum(y_true_flatten) + K.sum(y_pred_flatten) - intersection + 1.0)\n",
    "    return final_coef_value\n",
    "    '''"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPw4dXqbrOaTut/+CDo+nZ1",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
