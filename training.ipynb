{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TerminateOnNaN\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import gc\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.schedules import LearningRateSchedule\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "import segmentation_models as sm\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- Sanity ---\n",
    "def test_training_sanity():\n",
    "    print(\"‚úÖ from training.ipynb\")\n",
    "\n",
    "\n",
    "\n",
    "INPUT_TYPE_CONFIG = {\n",
    "    \"1ch\": {\"description\": \"grayscale only\", \"channels\": 1},\n",
    "    \"2ch\": {\"description\": \"grayscale + elevation\", \"channels\": 2},\n",
    "    \"rgb\": {\"description\": \"RGB only\", \"channels\": 3},\n",
    "    \"rgb_elev\": {\"description\": \"RGB + elevation\", \"channels\": 4}\n",
    "}\n",
    "\n",
    "COLOR_TO_CLASS = {\n",
    "    (230, 25, 75): 0,\n",
    "    (145, 30, 180): 1,\n",
    "    (60, 180, 75): 2,\n",
    "    (245, 130, 48): 3,\n",
    "    (255, 255, 255): 4,\n",
    "    (0, 130, 200): 5,\n",
    "    (255, 0, 255): 6 # Ignore pixel for visualisation\n",
    "}\n",
    "\n",
    "CLASS_TO_COLOR = {v: k for k, v in COLOR_TO_CLASS.items() if v < 6}  # Exclude ignore class\n",
    "NUM_CLASSES = 6\n",
    "COLOR_PALETTE = np.array(list(COLOR_TO_CLASS.keys()), dtype=np.uint8)\n",
    "COLOR_LOOKUP = {tuple(c): i for c, i in COLOR_TO_CLASS.items()}\n",
    "\n",
    "def decode_label_image(label_img):\n",
    "    h, w, _ = label_img.shape\n",
    "    label_map = np.zeros((h, w), dtype=np.uint8)\n",
    "    for y in range(h):\n",
    "        for x in range(w):\n",
    "            pixel = tuple(label_img[y, x])\n",
    "            if pixel not in COLOR_LOOKUP:\n",
    "                raise ValueError(f\"‚ùå Unknown label colour {pixel} at ({y}, {x})\")\n",
    "            label_map[y, x] = COLOR_LOOKUP[pixel]\n",
    "    return label_map\n",
    "\n",
    "\n",
    "def filter_tile_ids_by_substring(image_dir, base_names):\n",
    "    return [f.replace('-ortho.png', '') for f in os.listdir(image_dir) if any(base in f for base in base_names)]\n",
    "\n",
    "\n",
    "def measure_inference_time(model, generator, num_batches=5):\n",
    "    import time\n",
    "    total_time = 0\n",
    "    total_images = 0\n",
    "\n",
    "    for i, (x_batch, _) in enumerate(generator):\n",
    "        if i >= num_batches:\n",
    "            break\n",
    "        start = time.time()\n",
    "        _ = model.predict(x_batch, verbose=0)\n",
    "        end = time.time()\n",
    "        total_time += (end - start)\n",
    "        total_images += x_batch.shape[0]\n",
    "\n",
    "    print(f\"üß† Inference time: {total_time:.2f} sec for {total_images} images\")\n",
    "    print(f\"‚è±Ô∏è Avg inference time per image: {total_time / total_images:.4f} sec\")\n",
    "\n",
    "\n",
    "from tqdm import trange\n",
    "def plot_training_curves(history, out_dir):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import os\n",
    "\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "\n",
    "    # Define metrics to plot\n",
    "    metrics = ['loss', 'accuracy', 'masked_mean_iou', 'f1-score', 'iou_score', 'categorical_accuracy']\n",
    "\n",
    "    # üìà Plotting Training Curves\n",
    "    for metric in metrics:\n",
    "        plt.figure()\n",
    "        plt.plot(history.history[metric], label=\"Training \" + metric)\n",
    "        plt.plot(history.history[\"val_\" + metric], label=\"Validation \" + metric)\n",
    "        plt.title(\"Training and Validation \" + metric)\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(metric)\n",
    "        plt.legend()\n",
    "        plt.savefig(os.path.join(out_dir, metric + \"_plot.png\"))\n",
    "        plt.close()\n",
    "\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(out_dir, \"loss_plot.png\"))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# --- Metrics ---\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TerminateOnNaN\n",
    "\n",
    "# Custom learning rate schedule\n",
    "class TransformerLRSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super().__init__()\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "        self.warmup_steps = tf.cast(warmup_steps, tf.float32)\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, tf.float32)\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * tf.pow(self.warmup_steps, -1.5)\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\n",
    "            \"d_model\": self.d_model.numpy(),  # convert back to Python float\n",
    "            \"warmup_steps\": self.warmup_steps.numpy()\n",
    "        }\n",
    "\n",
    "\n",
    "# Instantiate learning rate schedule and optimizer\n",
    "lr_schedule = TransformerLRSchedule(d_model=256)\n",
    "optimizer = Adam(learning_rate=lr_schedule, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "# Loss function with label smoothing\n",
    "def apply_label_smoothing(y_true, smoothing=0.1):\n",
    "    num_classes = tf.cast(tf.shape(y_true)[-1], tf.float32)\n",
    "    return y_true * (1.0 - smoothing) + (smoothing / num_classes)\n",
    "\n",
    "\n",
    "os.environ[\"SM_FRAMEWORK\"] = \"tf.keras\"\n",
    "\n",
    "# Set class weights\n",
    "weights = [0.1666, 0.1666, 0.1666, 0.1666, 0.1666, 0.1666]\n",
    "\n",
    "# Raw losses from segmentation_models\n",
    "raw_dice = sm.losses.DiceLoss(class_weights=weights)\n",
    "raw_focal = sm.losses.CategoricalFocalLoss()\n",
    "\n",
    "# Final loss function with label smoothing applied\n",
    "def total_loss_with_smoothing(y_true, y_pred):\n",
    "    y_true_smoothed = apply_label_smoothing(y_true, smoothing=0.1)\n",
    "    return raw_dice(y_true_smoothed, y_pred) + raw_focal(y_true_smoothed, y_pred)\n",
    "\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.metrics import MeanIoU\n",
    "\n",
    "# --- Jaccard Index ---\n",
    "'''\n",
    "class MeanIoUMetric(tf.keras.metrics.MeanIoU):\n",
    "    def __init__(self, num_classes, name=\"mean_iou\", dtype=None):\n",
    "        super().__init__(num_classes=num_classes, name=name, dtype=dtype)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.argmax(y_true, axis=-1)\n",
    "        y_pred = tf.argmax(y_pred, axis=-1)\n",
    "        return super().update_state(y_true, y_pred, sample_weight)\n",
    "\n",
    "miou_metric = MeanIoUMetric(num_classes=6)\n",
    "'''\n",
    "\n",
    "class MaskedMeanIoU(tf.keras.metrics.Metric):\n",
    "    def __init__(self, num_classes, name=\"masked_mean_iou\", **kwargs):\n",
    "        super(MaskedMeanIoU, self).__init__(name=name, **kwargs)\n",
    "        self.num_classes = num_classes\n",
    "        self.total_cm = self.add_weight(\n",
    "            name=\"total_confusion_matrix\",\n",
    "            shape=(num_classes, num_classes),\n",
    "            initializer=\"zeros\",\n",
    "            dtype=tf.float32,\n",
    "        )\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.argmax(y_true, axis=-1)\n",
    "        y_pred = tf.argmax(y_pred, axis=-1)\n",
    "\n",
    "        # MASK: Ignore pixels labeled with 6 (your ignore class)\n",
    "        mask = tf.not_equal(y_true, 6)\n",
    "        y_true = tf.boolean_mask(y_true, mask)\n",
    "        y_pred = tf.boolean_mask(y_pred, mask)\n",
    "\n",
    "        current_cm = tf.math.confusion_matrix(y_true, y_pred, num_classes=self.num_classes, dtype=tf.float32)\n",
    "        self.total_cm.assign_add(current_cm)\n",
    "\n",
    "    def result(self):\n",
    "        sum_over_row = tf.reduce_sum(self.total_cm, axis=0)\n",
    "        sum_over_col = tf.reduce_sum(self.total_cm, axis=1)\n",
    "        true_positives = tf.linalg.diag_part(self.total_cm)\n",
    "        denominator = sum_over_row + sum_over_col - true_positives\n",
    "\n",
    "        iou = tf.math.divide_no_nan(true_positives, denominator)\n",
    "        return tf.reduce_mean(iou)\n",
    "\n",
    "    def reset_states(self):\n",
    "        tf.keras.backend.set_value(self.total_cm, tf.zeros((self.num_classes, self.num_classes)))\n",
    "\n",
    "\n",
    "# --- Callbacks ---\n",
    "class ClearMemory(tf.keras.callbacks.Callback):\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        gc.collect()\n",
    "        K.clear_session()\n",
    "\n",
    "class LearningRateLogger(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        lr = self.model.optimizer.learning_rate.numpy()\n",
    "        print(f\"üìâ Learning Rate at epoch {epoch + 1}: {lr:.6f}\")\n",
    "\n",
    "class TimeLimitCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, max_minutes=20):\n",
    "        super().__init__()\n",
    "        self.max_duration = max_minutes * 60\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.start_time = tf.timestamp()\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        elapsed = tf.timestamp() - self.start_time\n",
    "        if elapsed > self.max_duration:\n",
    "            print(f\"‚è±Ô∏è Training time exceeded {self.max_duration // 60} minutes. Stopping early.\")\n",
    "            self.model.stop_training = True\n",
    "\n",
    "            \n",
    "CLASS_NAMES = ['Building', 'Clutter', 'Vegetation', 'Water', 'Background', 'Car']\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class DistributionLogger(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, generator, name=\"Training\", max_batches=16, visualise_samples=2):\n",
    "        super().__init__()\n",
    "        self.generator = generator\n",
    "        self.name = name\n",
    "        self.max_batches = max_batches\n",
    "        self.visualise_samples = visualise_samples\n",
    "        self.cumulative_class_counts = defaultdict(int)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        batch_class_counts = defaultdict(int)\n",
    "        all_samples = []\n",
    "        batches_seen = 0\n",
    "\n",
    "        for batch_images, batch_labels in self.generator:\n",
    "            if batches_seen >= self.max_batches:\n",
    "                break\n",
    "\n",
    "            batch_preds = np.argmax(batch_labels, axis=-1)\n",
    "            unique, counts = np.unique(batch_preds, return_counts=True)\n",
    "\n",
    "            for u, c in zip(unique, counts):\n",
    "                batch_class_counts[u] += c\n",
    "                self.cumulative_class_counts[u] += c\n",
    "\n",
    "            for img, label in zip(batch_images, batch_preds):\n",
    "                all_samples.append((img, label))\n",
    "\n",
    "            batches_seen += 1\n",
    "\n",
    "        total_pixels = sum(batch_class_counts.values())\n",
    "        total_images = batches_seen * self.generator.batch_size\n",
    "\n",
    "        print(f\"üìä {self.name} Distribution After Epoch {epoch + 1} ({total_images:,} images):\")\n",
    "        for cls in sorted(batch_class_counts):\n",
    "            count = batch_class_counts[cls]\n",
    "            percent = 100.0 * count / total_pixels\n",
    "            print(f\"  Class {cls} ({CLASS_NAMES[cls]}): {count:,} px ({percent:.2f}%)\")\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        total_pixels = sum(self.cumulative_class_counts.values())\n",
    "        print(\"üìä Final Cumulative Training Class Distribution:\")\n",
    "        print(f\"Total pixels: {total_pixels:,} px\")\n",
    "        for cls in sorted(self.cumulative_class_counts):\n",
    "            count = self.cumulative_class_counts[cls]\n",
    "            percent = 100.0 * count / total_pixels\n",
    "            print(f\"  Class {cls} ({CLASS_NAMES[cls]}): {count:,} px ({percent:.2f}%)\")\n",
    "\n",
    "        plot_class_distribution(self.cumulative_class_counts)\n",
    "\n",
    "\n",
    "class ValidationPredictionLogger(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, val_gen, user_model, max_batches=1):\n",
    "        super().__init__()\n",
    "        self.val_gen = val_gen\n",
    "        self.user_model = user_model\n",
    "        self.max_batches = max_batches\n",
    "        self.ignore_color = (255, 0, 255)\n",
    "        self.class_to_color = {\n",
    "            0: (230, 25, 75),    # Building\n",
    "            1: (145, 30, 180),   # Clutter\n",
    "            2: (60, 180, 75),    # Vegetation\n",
    "            3: (245, 130, 48),   # Water\n",
    "            4: (255, 255, 255),  # Background\n",
    "            5: (0, 130, 200),    # Car\n",
    "        }\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None): \n",
    "        if (epoch + 1) % 5 != 0:\n",
    "            return\n",
    "\n",
    "        batches_seen = 0\n",
    "        for batch_images, batch_labels in self.val_gen:\n",
    "            if batches_seen >= self.max_batches:\n",
    "                break\n",
    "            preds = self.user_model.predict(batch_images)\n",
    "            preds_argmax = np.argmax(preds, axis=-1)\n",
    "            true_argmax = np.argmax(batch_labels, axis=-1)\n",
    "\n",
    "            fig, axs = plt.subplots(len(batch_images), 3, figsize=(10, 3 * len(batch_images)))\n",
    "            for i in range(len(batch_images)):\n",
    "                axs[i, 0].imshow((batch_images[i] * 255).astype(np.uint8))\n",
    "                axs[i, 0].set_title(\"Input\")\n",
    "                axs[i, 0].axis('off')\n",
    "\n",
    "                h, w = true_argmax[i].shape\n",
    "                true_rgb = np.zeros((h, w, 3), dtype=np.uint8)\n",
    "                pred_rgb = np.zeros((h, w, 3), dtype=np.uint8)\n",
    "\n",
    "                # Decode classes\n",
    "                for cid, col in self.class_to_color.items():\n",
    "                    true_rgb[true_argmax[i] == cid] = col\n",
    "                    pred_rgb[preds_argmax[i] == cid] = col\n",
    "\n",
    "                # üü£ Highlight ignore pixels (zero-vectors in one-hot)\n",
    "                ignore_mask = np.all(batch_labels[i] == 0, axis=-1)\n",
    "                true_rgb[ignore_mask] = self.ignore_color\n",
    "                pred_rgb[ignore_mask] = self.ignore_color\n",
    "\n",
    "                axs[i, 1].imshow(true_rgb)\n",
    "                axs[i, 1].set_title(\"Ground Truth\")\n",
    "                axs[i, 1].axis('off')\n",
    "\n",
    "                axs[i, 2].imshow(pred_rgb)\n",
    "                axs[i, 2].set_title(\"Prediction\")\n",
    "                axs[i, 2].axis('off')\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.suptitle(f\"Validation Predictions After Epoch {epoch + 1}\", y=1.02)\n",
    "            plt.show()\n",
    "            batches_seen += 1\n",
    "\n",
    "\n",
    "class StepTimer(tf.keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.total_time = 0.0\n",
    "        self.total_steps = 0\n",
    "\n",
    "    def on_train_batch_begin(self, batch, logs=None):\n",
    "        self.start_time = tf.timestamp()\n",
    "\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        elapsed = tf.timestamp() - self.start_time\n",
    "        self.total_time += elapsed\n",
    "        self.total_steps += 1\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        avg_step_time = self.total_time / self.total_steps\n",
    "        print(f\"üïí Average training step time: {avg_step_time:.4f} sec\")\n",
    "\n",
    "\n",
    "\n",
    "# Create both output folders\n",
    "os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "os.makedirs(\"/content/drive/MyDrive/segmentation_checkpoints\", exist_ok=True)\n",
    "\n",
    "# Checkpoint in Colab workspace\n",
    "checkpoint_local = ModelCheckpoint(\n",
    "    \"checkpoints/best_model.h5\",\n",
    "    monitor='val_mean_iou',\n",
    "    save_best_only=True\n",
    ")\n",
    "\n",
    "# Checkpoint in Google Drive\n",
    "checkpoint_drive = ModelCheckpoint(\n",
    "    \"/content/drive/MyDrive/segmentation_checkpoints/best_model.h5\",\n",
    "    monitor='val_mean_iou',\n",
    "    save_best_only=True\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# --- Model Building ---\n",
    "#from segmentation_models import Unet, MultiUnet, UnetAux, Segformer, CRF\n",
    "\n",
    "def train_model(base_dir=\"/content/chipped_data/content/chipped_data\", out_dir=\"/content/figs\", \n",
    "                freeze=False, model_path=None,\n",
    "                input_type=\"rgb_elev\", model_type=\"unet\", tile_size=256,\n",
    "                batch_size=8, steps=None, epochs=10, train_time=20, verbose=1\n",
    "                ):\n",
    "    \n",
    "    assert input_type in INPUT_TYPE_CONFIG, f\"Unknown input type: {input_type}\"\n",
    "    num_channels = INPUT_TYPE_CONFIG[input_type][\"channels\"]\n",
    "\n",
    "    train_images = os.path.join(base_dir, \"train\", \"images\")\n",
    "    train_elev = os.path.join(base_dir, \"train\", \"elevations\")\n",
    "    train_labels = os.path.join(base_dir, \"train\", \"labels\")\n",
    "\n",
    "    eval_images = os.path.join(base_dir, \"raw\", \"images\")\n",
    "    eval_elev = os.path.join(base_dir, \"raw\", \"elevations\")\n",
    "    eval_labels = os.path.join(base_dir, \"raw\", \"labels\")\n",
    "\n",
    "    val_images = os.path.join(base_dir, \"raw\", \"images\")\n",
    "    val_elev = os.path.join(base_dir, \"raw\", \"elevations\")\n",
    "    val_labels = os.path.join(base_dir, \"raw\", \"labels\")\n",
    "\n",
    "    test_images = os.path.join(base_dir, \"raw\", \"images\")\n",
    "    test_elev = os.path.join(base_dir, \"raw\", \"elevations\")\n",
    "    test_labels = os.path.join(base_dir, \"raw\", \"labels\")\n",
    "\n",
    "\n",
    "    # Load metadata\n",
    "    train_df = csv_to_df('train')\n",
    "    val_df = csv_to_df('val')\n",
    "    test_df = csv_to_df('test')\n",
    "\n",
    "    # Compute steps\n",
    "    eval_batch_size = batch_size // 2\n",
    "    val_steps = len(val_df) // eval_batch_size + (len(val_df) % eval_batch_size > 0)\n",
    "    test_steps = len(test_df) // eval_batch_size + (len(test_df) % eval_batch_size > 0)\n",
    "\n",
    "    # --- Streaming Data Generator ---\n",
    "    train_gen = StreamingDataGenerator(\n",
    "        train_images, train_elev, train_labels,\n",
    "        split='train', df=train_df,\n",
    "        batch_size=batch_size,\n",
    "        input_type=input_type,\n",
    "        shuffle=True,\n",
    "        steps=steps,\n",
    "        fixed=False,\n",
    "        augment=True,\n",
    "    )\n",
    " \n",
    "\n",
    "    val_gen = StreamingDataGenerator(eval_images, eval_elev, eval_labels,\n",
    "                                     split='val', df=val_df,\n",
    "                                     batch_size=8, steps=val_steps,\n",
    "                                     input_type=input_type, shuffle=False, fixed=True, augment=False)\n",
    "\n",
    "\n",
    "    test_gen = StreamingDataGenerator(eval_images, eval_elev, eval_labels,\n",
    "                                      split='test', df=test_df,\n",
    "                                      batch_size=batch_size, steps=test_steps,\n",
    "                                      input_type=input_type, shuffle=False, fixed=True, augment=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #input_shape = (tile_size, tile_size, num_channels)\n",
    "    input_shape = (None, None, num_channels)\n",
    "\n",
    "    # --- Model ---\n",
    "    if model_path is None or not os.path.exists(model_path):\n",
    "        if model_type == \"unet\":\n",
    "            print(\"üß™ Calling build_unet...\")\n",
    "            model = build_unet(input_shape=input_shape, num_classes=NUM_CLASSES)\n",
    "\n",
    "        elif model_type == \"multi_unet\":\n",
    "            print(\"üß™ Calling build_multi_unet...\")\n",
    "            model = build_multi_unet(input_shape=input_shape, num_classes=NUM_CLASSES)\n",
    "\n",
    "        elif model_type == \"unet_aux\":\n",
    "            print(\"üß™ Calling build_multi_unet_aux...\")\n",
    "            model = build_unet_aux(input_shape=input_shape, num_classes=NUM_CLASSES)\n",
    "\n",
    "        elif model_type == \"segformer\":\n",
    "            model = SegFormer_B2(input_shape=input_shape, num_classes=NUM_CLASSES)\n",
    "        \n",
    "        elif model_type == \"resnet34\":\n",
    "            import segmentation_models as sm\n",
    "            model = sm.Unet(\n",
    "                backbone_name=\"resnet34\",          # or 'efficientnetb0', 'mobilenetv2', etc.\n",
    "                input_shape=input_shape,\n",
    "                classes=NUM_CLASSES,                  \n",
    "                activation='softmax', \n",
    "                encoder_weights='imagenet'         # Load ImageNet pre-trained weights\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model_type: {model_type}\")\n",
    "    \n",
    "    else:\n",
    "        import segmentation_models as sm\n",
    "        print(f\"Loading Stage 1 model from {model_path} and unfreezing all layers...\")\n",
    "\n",
    "        custom_objects={\n",
    "            'DiceLoss': sm.losses.DiceLoss,\n",
    "            'CategoricalFocalLoss': sm.losses.CategoricalFocalLoss,\n",
    "            'MaskedMeanIoU': MaskedMeanIoU\n",
    "        }\n",
    "        \n",
    "        model = tf.keras.models.load_model(\n",
    "            model_path,\n",
    "            custom_objects=custom_objects,\n",
    "            compile=True\n",
    "        )\n",
    "\n",
    "        for layer in model.layers:\n",
    "            layer.trainable = True\n",
    "\n",
    "\n",
    "    lr_schedule = TransformerLRSchedule(d_model=tile_size)\n",
    "    optimizer = Adam(learning_rate=lr_schedule, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "    miou_metric = MaskedMeanIoU(num_classes=NUM_CLASSES)\n",
    "    metrics=[\n",
    "        miou_metric,\n",
    "        sm.metrics.IOUScore(threshold=None),\n",
    "        sm.metrics.FScore(threshold=None),\n",
    "        'categorical_accuracy',\n",
    "        'accuracy'\n",
    "    ]\n",
    "\n",
    "    if freeze:\n",
    "        print(\"Stage 1: Freezing all layers except head...\")\n",
    "        for layer in model.layers:\n",
    "            layer.trainable = False\n",
    "        for layer in model.layers[-10:]:\n",
    "            layer.trainable = True\n",
    "\n",
    "        optimizer = Adam(learning_rate=1e-3)\n",
    "\n",
    "\n",
    "    # --- Compile Model ---\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=total_loss_with_smoothing,\n",
    "        metrics=metrics\n",
    "    )\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "\n",
    "    # --- Callbacks --- \n",
    "    time_limit = TimeLimitCallback(max_minutes=train_time)\n",
    "    early_stop = EarlyStopping(monitor='val_mean_iou', patience=12, restore_best_weights=True, mode='max')\n",
    "    nan_terminate = TerminateOnNaN()\n",
    "\n",
    "    callbacks = [\n",
    "        checkpoint_local, checkpoint_drive, \n",
    "        early_stop, nan_terminate, time_limit, \n",
    "        ClearMemory(), \n",
    "        LearningRateLogger(),\n",
    "        StepTimer(),\n",
    "        DistributionLogger(train_gen, name=\"Training\", max_batches=steps),\n",
    "        ValidationPredictionLogger(val_gen, model, max_batches=1)\n",
    "    ]\n",
    "\n",
    "\n",
    "    # --- Train Model ---\n",
    "    print(\"üöÄ Starting training...\")\n",
    "\n",
    "    history = model.fit(\n",
    "        train_gen, validation_data=val_gen,\n",
    "        epochs=epochs,\n",
    "        callbacks=callbacks,\n",
    "        verbose=verbose\n",
    "    )\n",
    "\n",
    "    from datetime import datetime\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    model_name = f\"unet_resnet34_stage1_{timestamp}\"\n",
    "    model_name\n",
    "\n",
    "    model.save(f\"/content/checkpoints/{model_name}.keras\")\n",
    "    print(f\"model saved to /content/checkpoints/{model_name}.keras\")\n",
    "\n",
    "\n",
    "\n",
    "    # --- Evaluate Model ---\n",
    "    plot_training_curves(history, out_dir)\n",
    "    evaluate_on_test(model, test_gen, n_vis=9)\n",
    "    measure_inference_time(model, test_gen, num_batches=steps)\n",
    "    print(\"üöÄ Training complete!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def reconstruct_prediction_canvas(df, tile_size, image_dir, label_dir, pred_dir):\n",
    "    import numpy as np\n",
    "    import cv2\n",
    "    import os\n",
    "\n",
    "    # Determine canvas size from tile coordinates\n",
    "    x_coords = df['x'].values\n",
    "    y_coords = df['y'].values\n",
    "    max_x = x_coords.max() + tile_size\n",
    "    max_y = y_coords.max() + tile_size\n",
    "    min_x = x_coords.min()\n",
    "    min_y = y_coords.min()\n",
    "\n",
    "    canvas_shape = (max_y - min_y, max_x - min_x, 3)\n",
    "    img_canvas = np.full(canvas_shape, (255, 0, 255), dtype=np.uint8)  # Magenta default\n",
    "    label_canvas = np.full(canvas_shape, (255, 0, 255), dtype=np.uint8)\n",
    "    pred_canvas = np.full(canvas_shape, (255, 0, 255), dtype=np.uint8)\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        tile_id = row['tile_id']\n",
    "        x, y = row['x'], row['y']\n",
    "        x_offset = x - min_x\n",
    "        y_offset = y - min_y\n",
    "\n",
    "        try:\n",
    "            img_path = os.path.join(image_dir, tile_id + '-ortho.png')\n",
    "            label_path = os.path.join(label_dir, tile_id + '-label.png')\n",
    "            pred_path = os.path.join(pred_dir, tile_id + '.png')\n",
    "\n",
    "            if os.path.exists(img_path):\n",
    "                rgb = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n",
    "                img_canvas[y_offset:y_offset+tile_size, x_offset:x_offset+tile_size] = rgb\n",
    "\n",
    "            if os.path.exists(label_path):\n",
    "                label_rgb = cv2.cvtColor(cv2.imread(label_path), cv2.COLOR_BGR2RGB)\n",
    "                label_canvas[y_offset:y_offset+tile_size, x_offset:x_offset+tile_size] = label_rgb\n",
    "\n",
    "            if os.path.exists(pred_path):\n",
    "                pred_rgb = cv2.cvtColor(cv2.imread(pred_path), cv2.COLOR_BGR2RGB)\n",
    "                pred_canvas[y_offset:y_offset+tile_size, x_offset:x_offset+tile_size] = pred_rgb\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Failed to load tile {tile_id}: {e}\")\n",
    "            continue\n",
    "\n",
    "    return img_canvas, label_canvas, pred_canvas\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # --- Archive ---\n",
    "\n",
    "    '''\n",
    "    class_weights = compute_class_weights(train_gen)\n",
    "    loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n",
    "\n",
    "    def weighted_loss(y_true, y_pred):\n",
    "        weights = tf.reduce_sum(class_weights * y_true, axis=-1)\n",
    "        return tf.reduce_mean(weights * loss_fn(y_true, y_pred))\n",
    "\n",
    "    model.compile(optimizer='adam', loss=weighted_loss, metrics=['accuracy'])\n",
    "    print(\"‚úÖ Model compiled with class weights\")\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    from segmentation_models.metrics import iou_score as jaccard_coef\n",
    "    metrics = [\"accuracy\", jaccard_coef]\n",
    "    weights = [0.1666, 0.1666, 0.1666, 0.1666, 0.1666, 0.1666]\n",
    "    \n",
    "    dice_loss = dice_loss(class_weights = weights)\n",
    "    focal_loss = categorical_focal_loss()\n",
    "    total_loss = dice_loss + (1 * focal_loss)\n",
    "    \n",
    "    model.compile(optimizer=\"adam\", loss=\"total_loss\", metrics=metrics)\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    def jaccard_coef(y_true, y_pred):\n",
    "    y_true_flatten = K.flatten(y_true)\n",
    "    y_pred_flatten = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_flatten * y_pred_flatten)\n",
    "    final_coef_value = (intersection + 1.0) / (K.sum(y_true_flatten) + K.sum(y_pred_flatten) - intersection + 1.0)\n",
    "    return final_coef_value\n",
    "    '''"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPw4dXqbrOaTut/+CDo+nZ1",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
