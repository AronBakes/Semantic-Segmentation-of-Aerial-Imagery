{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yfkCy82a_1IM"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "# Set segmentation models to use tf.keras backend\n",
    "os.environ[\"SM_FRAMEWORK\"] = \"tf.keras\"\n",
    "\n",
    "\n",
    "# --- Constants ---\n",
    "TILE_SIZE = 512  # Size of each training chip in pixels (512x512)\n",
    "IGNORE_COLOR = (255, 0, 255) # The specific RGB color for ignored regions (magenta)\n",
    "\n",
    "# Mapping from RGB color (as tuple) to integer class ID\n",
    "COLOR_TO_CLASS = {\n",
    "    (230, 25, 75): 0,    # Building\n",
    "    (60, 180, 75): 1,    # Vegetation\n",
    "    (0, 130, 200): 2,    # Water\n",
    "    (255, 255, 255): 3,  # Background\n",
    "    (245, 130, 48): 4,   # Car\n",
    "    (128, 128, 128): 5,  # Road\n",
    "}\n",
    "CLASS_NAMES = ['Building', 'Vegetation', 'Water', 'Background', 'Car', 'Road']\n",
    "class_cols = ['0: Building', '1: Vegetation', '2: Water', '3: Background', '4: Car', '5: Road']  # Column names for class pixel counts in the metadata CSV\n",
    "#class_cols = {f\"dist_{i}:{name}\" for i, name in enumerate(CLASS_NAMES)}\n",
    "\n",
    "\n",
    "# Inverse mapping from integer class ID to RGB color (as tuple)\n",
    "NUM_CLASSES = len(COLOR_TO_CLASS)\n",
    "CLASS_TO_COLOR = {v: k for k, v in COLOR_TO_CLASS.items() if v < NUM_CLASSES} # Ensure it matches NUM_CLASSES\n",
    "COLOR_PALETTE = np.array(list(COLOR_TO_CLASS.keys()), dtype=np.uint8)\n",
    "COLOR_LOOKUP = {tuple(c): i for c, i in COLOR_TO_CLASS.items()}\n",
    "\n",
    "\n",
    "# --- Constants ---\n",
    "INPUT_TYPE_CONFIG = {\n",
    "    \"rgb\": {\"description\": \"RGB only\", \"channels\": 3},\n",
    "    \"rgb_elev\": {\"description\": \"RGB + elevation\", \"channels\": 4},\n",
    "    \"rgb_elev_slope\": {\"description\": \"RGB + elevation + slope\", \"channels\": 5}\n",
    "}\n",
    "\n",
    "\n",
    "base_dir=\"/content/chipped_data/chipped_data\"\n",
    "out_dir=\"/content/figs\"\n",
    "\n",
    "img_dir = os.path.join(base_dir, \"train\", \"images\")\n",
    "elev_dir = os.path.join(base_dir, \"train\", \"elevations\")\n",
    "slope_dir = os.path.join(base_dir, \"train\", \"slopes\")\n",
    "label_dir = os.path.join(base_dir, \"train\", \"labels\")\n",
    "\n",
    "metadata_path = os.path.join(base_dir, \"train_metadata.csv\")\n",
    "\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "# --- Jaccard Index (IoU) Metric ---\n",
    "class MeanIoUMetric(tf.keras.metrics.MeanIoU):\n",
    "    \"\"\"Mean Intersection-over-Union (mIoU) metric for multi-class segmentation.\n",
    "\n",
    "    This subclass overrides the update_state method to work directly with\n",
    "    one-hot encoded ground truth and softmax predictions by converting them\n",
    "    to integer class labels using `argmax`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes: int, name: str = \"mean_iou\", dtype=None):\n",
    "        \"\"\"Initialises the MeanIoUMetric.\n",
    "\n",
    "        Args:\n",
    "            num_classes (int): Total number of segmentation classes.\n",
    "            name (str): Optional name for the metric.\n",
    "            dtype (Optional): Optional data type.\n",
    "        \"\"\"\n",
    "        super().__init__(num_classes=num_classes, name=name, dtype=dtype)\n",
    "\n",
    "    def update_state(self, y_true: tf.Tensor, y_pred: tf.Tensor, sample_weight=None):\n",
    "        \"\"\"Updates the confusion matrix with batch predictions.\n",
    "\n",
    "        Args:\n",
    "            y_true (tf.Tensor): One-hot encoded ground truth labels.\n",
    "            y_pred (tf.Tensor): Softmax predictions from the model.\n",
    "            sample_weight (Optional): Optional tensor for sample weights.\n",
    "\n",
    "        Returns:\n",
    "            tf.Operation: Update operation.\n",
    "        \"\"\"\n",
    "        y_true = tf.argmax(y_true, axis=-1)\n",
    "        y_pred = tf.argmax(y_pred, axis=-1)\n",
    "        return super().update_state(y_true, y_pred, sample_weight)\n",
    "\n",
    "\n",
    "class TransformerLRSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    \"\"\"Custom learning rate schedule based on the Transformer paper.\n",
    "\n",
    "    The schedule increases the learning rate linearly for the first `warmup_steps`,\n",
    "    and then decreases it proportionally to the inverse square root of the step number.\n",
    "\n",
    "    This is commonly used in training Transformer models.\n",
    "\n",
    "    Attributes:\n",
    "        d_model (tf.Tensor): The dimensionality of the model.\n",
    "        warmup_steps (tf.Tensor): Number of steps to linearly increase the learning rate.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int, warmup_steps: int = 4000):\n",
    "        \"\"\"Initialises the TransformerLRSchedule.\n",
    "\n",
    "        Args:\n",
    "            d_model (int): The model dimensionality (e.g., 512).\n",
    "            warmup_steps (int): Number of warm-up steps. Default is 4000.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "        self.warmup_steps = tf.cast(warmup_steps, tf.float32)\n",
    "\n",
    "    def __call__(self, step: tf.Tensor) -> tf.Tensor:\n",
    "        \"\"\"Computes the learning rate at a given training step.\n",
    "\n",
    "        Args:\n",
    "            step (tf.Tensor): The current training step.\n",
    "\n",
    "        Returns:\n",
    "            tf.Tensor: The calculated learning rate for this step.\n",
    "        \"\"\"\n",
    "        step = tf.cast(step, tf.float32)\n",
    "\n",
    "        # Inverse square root decay and warmup scaling\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * tf.pow(self.warmup_steps, -1.5)\n",
    "\n",
    "        # Apply the min schedule\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "    def get_config(self) -> dict:\n",
    "        \"\"\"Returns the config of the learning rate schedule for serialization.\n",
    "\n",
    "        Returns:\n",
    "            dict: Configuration dictionary with d_model and warmup_steps.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"d_model\": self.d_model.numpy(),\n",
    "            \"warmup_steps\": self.warmup_steps.numpy(),\n",
    "        }\n",
    "\n",
    "\n",
    "def decode_label_image(label_img: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Converts a colour-coded label image into a class ID map.\n",
    "\n",
    "    Args:\n",
    "        label_img (np.ndarray): A (H, W, 3) RGB label image where each unique colour\n",
    "            represents a class, and colours are defined in COLOR_LOOKUP.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A (H, W) array of class IDs corresponding to each pixel.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If an unknown colour is encountered in the label image.\n",
    "    \"\"\"\n",
    "    h, w, _ = label_img.shape\n",
    "    label_map = np.zeros((h, w), dtype=np.uint8)\n",
    "\n",
    "    for y in range(h):\n",
    "        for x in range(w):\n",
    "            pixel = tuple(label_img[y, x])\n",
    "            if pixel not in COLOR_LOOKUP:\n",
    "                raise ValueError(f\"Unknown label colour {pixel} at ({y}, {x})\")\n",
    "            label_map[y, x] = COLOR_LOOKUP[pixel]\n",
    "\n",
    "    return label_map\n",
    "\n",
    "\n",
    "def apply_label_smoothing(y_true: tf.Tensor, smoothing: float = 0.1) -> tf.Tensor:\n",
    "    \"\"\"Applies label smoothing to a one-hot encoded tensor.\n",
    "\n",
    "    Args:\n",
    "        y_true (tf.Tensor): One-hot encoded labels of shape (..., num_classes).\n",
    "        smoothing (float): Smoothing factor in the range [0, 1].\n",
    "\n",
    "    Returns:\n",
    "        tf.Tensor: Smoothed label tensor of the same shape.\n",
    "    \"\"\" \n",
    "    num_classes = tf.cast(tf.shape(y_true)[-1], tf.float32)\n",
    "    return y_true * (1.0 - smoothing) + (smoothing / num_classes)\n",
    "\n",
    "\n",
    "def plot_augmented_grid_from_dataset(\n",
    "    tf_dataset: tf.data.Dataset,\n",
    "    input_type: str,  # 'rgb' or 'rgb_elev'\n",
    "    n_rows: int = 3,\n",
    "    n_cols: int = 4,\n",
    "    title: str = \"Augmented Training Chips\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Plots a grid of RGB + label masks from a tf.data.Dataset, using CLASS_TO_COLOR for display.\n",
    "    Layout matches the style of visualise_prediction_grid, but without predictions.\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "\n",
    "    print(f\"Fetching one batch for {n_rows * n_cols} RGB + label pairs...\")\n",
    "\n",
    "    try:\n",
    "        batch = next(iter(tf_dataset.take(1)), None)\n",
    "\n",
    "        if batch is None:\n",
    "            print(\"Warning: Dataset is empty. Skipping plot.\")\n",
    "            return\n",
    "\n",
    "        images, labels_one_hot = batch\n",
    "        rgb_images = images[:, :, :, :3].numpy() if input_type == 'rgb_elev' else images.numpy()\n",
    "        rgb_images = np.clip(rgb_images, 0.0, 1.0)\n",
    "\n",
    "        label_ids = tf.argmax(labels_one_hot, axis=-1).numpy()\n",
    "        ignore_mask = tf.reduce_all(labels_one_hot == 0, axis=-1).numpy()\n",
    "\n",
    "        total = n_rows * n_cols\n",
    "        batch_size = rgb_images.shape[0]\n",
    "        total = min(total, batch_size)\n",
    "\n",
    "        fig, axs = plt.subplots(n_rows, n_cols * 2, figsize=(n_cols * 5.6, n_rows * 2.6))\n",
    "\n",
    "        for i in range(total):\n",
    "            rgb = rgb_images[i]\n",
    "            mask = label_ids[i]\n",
    "            ignore = ignore_mask[i]\n",
    "\n",
    "            # Create RGB mask image from class IDs\n",
    "            label_rgb = np.zeros((*mask.shape, 3), dtype=np.uint8)\n",
    "            for class_id, color in CLASS_TO_COLOR.items():\n",
    "                label_rgb[mask == class_id] = color\n",
    "            label_rgb[ignore] = (255, 0, 255)  # Magenta for ignored pixels\n",
    "\n",
    "            row = i // n_cols\n",
    "            col = (i % n_cols) * 2\n",
    "\n",
    "            axs[row, col].imshow(rgb)\n",
    "            axs[row, col].set_title(\"RGB\")\n",
    "            axs[row, col].axis(\"off\")\n",
    "\n",
    "            axs[row, col + 1].imshow(label_rgb)\n",
    "            axs[row, col + 1].set_title(\"Label\")\n",
    "            axs[row, col + 1].axis(\"off\")\n",
    "\n",
    "        # Hide any unused axes\n",
    "        for j in range(total, n_rows * n_cols):\n",
    "            row = j // n_cols\n",
    "            col = (j % n_cols) * 2\n",
    "            axs[row, col].axis(\"off\")\n",
    "            axs[row, col + 1].axis(\"off\")\n",
    "\n",
    "        #plt.suptitle(title, fontsize=16, y=1.02)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        plt.close(fig)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during plotting: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def filter_tile_ids_by_substring(image_dir: str, base_names: list[str]) -> list[str]:\n",
    "    \"\"\"Filters filenames in a directory to extract tile IDs that match given substrings.\n",
    "\n",
    "    Looks for files ending in '-ortho.png' and checks if any given base name is present\n",
    "    in the filename. If matched, strips the '-ortho.png' suffix and returns the base ID.\n",
    "\n",
    "    Args:\n",
    "        image_dir (str): Path to the directory containing image files.\n",
    "        base_names (list[str]): List of substrings to match in the filenames.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: A list of tile IDs (filenames without '-ortho.png') that contain\n",
    "        any of the specified base substrings.\n",
    "    \"\"\"\n",
    "    return [\n",
    "        f.replace('-ortho.png', '')\n",
    "        for f in os.listdir(image_dir)\n",
    "        if any(base in f for base in base_names)\n",
    "    ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- Global Configuration and Constants ---\n",
    "\n",
    "# NEW: Explicitly define which scenes belong to which dataset split.\n",
    "# The csv_to_df function will now ONLY use files from these lists.\n",
    "TRAIN_SCENES = [\n",
    "    '1726eb08ef_60693DB04DINSPIRE', '1df70e7340_4413A67E91INSPIRE', '2552eb56dd_2AABB46C86OPENPIPELINE', \n",
    "    '25f1c24f30_EB81FE6E2BOPENPIPELINE', '2ef883f08d_F317F9C1DFOPENPIPELINE',\n",
    "    '420d6b69b8_84B52814D2OPENPIPELINE', '520947aa07_8FCB044F58OPENPIPELINE', \n",
    "    '57426ebe1e_84B52814D2OPENPIPELINE', '5fa39d6378_DB9FF730D9OPENPIPELINE', '6f93b9026b_F1BFB8B17DOPENPIPELINE', \n",
    "    '74d7796531_EB81FE6E2BOPENPIPELINE', '84410645db_8D20F02042OPENPIPELINE', \n",
    "    '888432f840_80E7FD39EBINSPIRE', '9170479165_625EDFBAB6OPENPIPELINE', 'a1af86939f_F1BE1D4184OPENPIPELINE', \n",
    "    'b61673f780_4413A67E91INSPIRE', 'b705d0cc9c_E5F5E0E316OPENPIPELINE', 'b771104de5_7E02A41EBEOPENPIPELINE',\n",
    "    'c37dbfae2f_84B52814D2OPENPIPELINE', 'c6d131e346_536DE05ED2OPENPIPELINE', 'c8a7031e5f_32156F5DC2INSPIRE', \n",
    "    'cc4b443c7d_A9CBEF2C97INSPIRE', 'd06b2c67d2_2A62B67B52OPENPIPELINE', 'd9161f7e18_C05BA1BC72OPENPIPELINE', \n",
    "    'e87da4ebdb_29FEA32BC7INSPIRE', 'ebffe540d0_7BA042D858OPENPIPELINE', 'ec09336a6f_06BA0AF311OPENPIPELINE', \n",
    "    'f0747ed88d_E74C0DD8FDOPENPIPELINE', 'f4dd768188_NOLANOPENPIPELINE', 'f56b6b2232_2A62B67B52OPENPIPELINE', \n",
    "    'f971256246_MIKEINSPIRE'                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
    "]\n",
    "\n",
    "VAL_SCENES = [\n",
    "    \"c644f91210_27E21B7F30OPENPIPELINE\", \"f9f43e5144_1DB9E6F68BINSPIRE\", '8710b98ea0_06E6522D6DINSPIRE', '84410645db_8D20F02042OPENPIPELINE',\n",
    "    '1553541487_APIGENERATED', '1553541585_APIGENERATED', 'fc5837dcf8_7CD52BE09EINSPIRE','1d056881e8_29FEA32BC7INSPIRE'\n",
    "    'ec09336a6f_06BA0AF311OPENPIPELINE',\n",
    "    '15efe45820_D95DF0B1F4INSPIRE',\n",
    "    '107f24d6e9_F1BE1D4184INSPIRE',\n",
    "    '1d4fbe33f3_F1BE1D4184INSPIRE',\n",
    "    '2ef3a4994a_0CCD105428INSPIRE',\n",
    "    '34fbf7c2bd_E8AD935CEDINSPIRE',\n",
    "    \"f9f43e5144_1DB9E6F68BINSPIRE\",\n",
    "    \"3502e187b2_23071E4605OPENPIPELINE\",\n",
    "    \"d9161f7e18_C05BA1BC72OPENPIPELINE\",\n",
    "    \"551063e3c5_8FCB044F58INSPIRE\",\n",
    "    \"39e77bedd0_729FB913CDOPENPIPELINE\",\n",
    "    'c2e8370ca3_3340CAC7AEOPENPIPELINE', \n",
    "]\n",
    "\n",
    "TEST_SCENES = [\n",
    "    \"1d4fbe33f3_F1BE1D4184INSPIRE\",\n",
    "    '1476907971_CHADGRISMOPENPIPELINE',\n",
    "    '6f93b9026b_F1BFB8B17DOPENPIPELINE',\n",
    "    '12fa5e614f_53197F206FOPENPIPELINE',\n",
    "    '11cdce7802_B6A62F8BE0INSPIRE',\n",
    "    '7c719dfcc0_310490364FINSPIRE',\n",
    "    '7008b80b00_FF24A4975DINSPIRE',     \n",
    "    'dabec5e872_E8AD935CEDINSPIRE',\n",
    "    '130a76ebe1_68B40B480AOPENPIPELINE',\n",
    "    'fc5837dcf8_7CD52BE09EINSPIRE',\n",
    "    'f9f43e5144_1DB9E6F68BINSPIRE',\n",
    "    '1553627230_APIGENERATED',\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# List of all available file prefixes (tile_ids without coordinate suffix)\n",
    "# These are used to identify the source files for splitting the dataset.\n",
    "all_files = [\n",
    "    '107f24d6e9_F1BE1D4184INSPIRE', '11cdce7802_B6A62F8BE0INSPIRE', '12fa5e614f_53197F206FOPENPIPELINE', '130a76ebe1_68B40B480AOPENPIPELINE', \n",
    "    '1476907971_CHADGRISMOPENPIPELINE', '1553541487_APIGENERATED', '1553541585_APIGENERATED', '1553627230_APIGENERATED', '15efe45820_D95DF0B1F4INSPIRE', \n",
    "    '1726eb08ef_60693DB04DINSPIRE', '1d056881e8_29FEA32BC7INSPIRE', '1d4fbe33f3_F1BE1D4184INSPIRE', '1df70e7340_4413A67E91INSPIRE', '2552eb56dd_2AABB46C86OPENPIPELINE', \n",
    "    '25f1c24f30_EB81FE6E2BOPENPIPELINE', '2ef3a4994a_0CCD105428INSPIRE', '2ef883f08d_F317F9C1DFOPENPIPELINE', '34fbf7c2bd_E8AD935CEDINSPIRE', \n",
    "    '3502e187b2_23071E4605OPENPIPELINE', '39e77bedd0_729FB913CDOPENPIPELINE', '420d6b69b8_84B52814D2OPENPIPELINE', '520947aa07_8FCB044F58OPENPIPELINE', \n",
    "    '551063e3c5_8FCB044F58INSPIRE', '57426ebe1e_84B52814D2OPENPIPELINE', '5fa39d6378_DB9FF730D9OPENPIPELINE', '6f93b9026b_F1BFB8B17DOPENPIPELINE', \n",
    "    '7008b80b00_FF24A4975DINSPIRE', '74d7796531_EB81FE6E2BOPENPIPELINE', '7c719dfcc0_310490364FINSPIRE', '84410645db_8D20F02042OPENPIPELINE', \n",
    "    '8710b98ea0_06E6522D6DINSPIRE', '888432f840_80E7FD39EBINSPIRE', '9170479165_625EDFBAB6OPENPIPELINE', 'a1af86939f_F1BE1D4184OPENPIPELINE', \n",
    "    'b61673f780_4413A67E91INSPIRE', 'b705d0cc9c_E5F5E0E316OPENPIPELINE', 'b771104de5_7E02A41EBEOPENPIPELINE', 'c2e8370ca3_3340CAC7AEOPENPIPELINE', \n",
    "    'c37dbfae2f_84B52814D2OPENPIPELINE', 'c644f91210_27E21B7F30OPENPIPELINE', 'c6d131e346_536DE05ED2OPENPIPELINE', 'c8a7031e5f_32156F5DC2INSPIRE', \n",
    "    'cc4b443c7d_A9CBEF2C97INSPIRE', 'd06b2c67d2_2A62B67B52OPENPIPELINE', 'd9161f7e18_C05BA1BC72OPENPIPELINE', 'dabec5e872_E8AD935CEDINSPIRE', \n",
    "    'e87da4ebdb_29FEA32BC7INSPIRE', 'ebffe540d0_7BA042D858OPENPIPELINE', 'ec09336a6f_06BA0AF311OPENPIPELINE', \n",
    "    'f0747ed88d_E74C0DD8FDOPENPIPELINE', 'f4dd768188_NOLANOPENPIPELINE', 'f56b6b2232_2A62B67B52OPENPIPELINE', \n",
    "    'f971256246_MIKEINSPIRE', 'f9f43e5144_1DB9E6F68BINSPIRE', 'fc5837dcf8_7CD52BE09EINSPIRE'\n",
    "]\n",
    "\n",
    "# Prefixes for files designated for the validation set\n",
    "val_files = [\n",
    "    \"c644f91210_27E21B7F30OPENPIPELINE\",\n",
    "    \"f9f43e5144_1DB9E6F68BINSPIRE\",\n",
    "    \"1d056881e8_29FEA32BC7INSPIRE\",\n",
    "    \"3502e187b2_23071E4605OPENPIPELINE\",\n",
    "    \"d9161f7e18_C05BA1BC72OPENPIPELINE\",\n",
    "    \"c8a7031e5f_32156F5DC2INSPIRE\",\n",
    "    \"551063e3c5_8FCB044F58INSPIRE\",\n",
    "    \"fc5837dcf8_7CD52BE09EINSPIRE\",\n",
    "    \"39e77bedd0_729FB913CDOPENPIPELINE\",\n",
    "]\n",
    "\n",
    "# Prefixes for files designated for the test set\n",
    "test_files = [\n",
    "    \"25f1c24f30_EB81FE6E2BOPENPIPELINE\",\n",
    "    \"1d4fbe33f3_F1BE1D4184INSPIRE\",\n",
    "    \"15efe45820_D95DF0B1F4INSPIRE\",\n",
    "    \"c6d131e346_536DE05ED2OPENPIPELINE\",\n",
    "    \"12fa5e614f_53197F206FOPENPIPELINE\",\n",
    "    \"5fa39d6378_DB9FF730D9OPENPIPELINE\",\n",
    "    \"ebffe540d0_7BA042D858OPENPIPELINE\",\n",
    "    \"8710b98ea0_06E6522D6DINSPIRE\",\n",
    "    \"84410645db_8D20F02042OPENPIPELINE\",\n",
    "    \"a1af86939f_F1BE1D4184OPENPIPELINE\"\n",
    "]\n",
    "\n",
    "\n",
    "# List of specific tile IDs to select for detailed visualization (e.g., problematic chips)\n",
    "# These are the full tile_id strings including x_y coordinates\n",
    "specific_tile_ids = [\n",
    "    # Group 1\n",
    "    \"25f1c24f30_EB81FE6E2BOPENPIPELINE_3456_1280\", \"25f1c24f30_EB81FE6E2BOPENPIPELINE_3584_8320\",\n",
    "    \"25f1c24f30_EB81FE6E2BOPENPIPELINE_896_2816\", \"25f1c24f30_EB81FE6E2BOPENPIPELINE_3840_4736\",\n",
    "    \"25f1c24f30_EB81FE6E2BOPENPIPELINE_3968_384\", \"25f1c24f30_EB81FE6E2BOPENPIPELINE_4736_512\",\n",
    "    \"25f1c24f30_EB81FE6E2BOPENPIPELINE_4736_768\", \"25f1c24f30_EB81FE6E2BOPENPIPELINE_1024_5888\",\n",
    "    \"25f1c24f30_EB81FE6E2BOPENPIPELINE_896_5888\", \"25f1c24f30_EB81FE6E2BOPENPIPELINE_1024_6016\",\n",
    "\n",
    "    # Group 2\n",
    "    \"1d4fbe33f3_F1BE1D4184INSPIRE_2560_4864\", \"1d4fbe33f3_F1BE1D4184INSPIRE_896_3584\",\n",
    "    \"1d4fbe33f3_F1BE1D4184INSPIRE_768_3584\", \"1d4fbe33f3_F1BE1D4184INSPIRE_896_3712\",\n",
    "    \"1d4fbe33f3_F1BE1D4184INSPIRE_1280_2432\", \"1d4fbe33f3_F1BE1D4184INSPIRE_1536_4608\",\n",
    "    \"1d4fbe33f3_F1BE1D4184INSPIRE_1152_2432\", \"1d4fbe33f3_F1BE1D4184INSPIRE_1664_4864\",\n",
    "    \"1d4fbe33f3_F1BE1D4184INSPIRE_1664_4736\", \"1d4fbe33f3_F1BE1D4184INSPIRE_1408_1280\",\n",
    "    \"1d4fbe33f3_F1BE1D4184INSPIRE_1152_4864\", \"1d4fbe33f3_F1BE1D4184INSPIRE_1280_2432\",\n",
    "    \"1d4fbe33f3_F1BE1D4184INSPIRE_1408_1408\", \"1d4fbe33f3_F1BE1D4184INSPIRE_1536_4736\",\n",
    "    \"1d4fbe33f3_F1BE1D4184INSPIRE_384_1152\",\n",
    "\n",
    "    # Group 3\n",
    "    \"15efe45820_D95DF0B1F4INSPIRE_4736_9472\", \"15efe45820_D95DF0B1F4INSPIRE_9600_6016\",\n",
    "    \"15efe45820_D95DF0B1F4INSPIRE_5888_6272\", \"15efe45820_D95DF0B1F4INSPIRE_7168_7936\",\n",
    "    \"15efe45820_D95DF0B1F4INSPIRE_6016_6272\", \"15efe45820_D95DF0B1F4INSPIRE_8704_1024\",\n",
    "    \"15efe45820_D95DF0B1F4INSPIRE_7040_6912\", \"15efe45820_D95DF0B1F4INSPIRE_8064_3968\",\n",
    "    \"15efe45820_D95DF0B1F4INSPIRE_2688_2048\", \"15efe45820_D95DF0B1F4INSPIRE_7680_1920\",\n",
    "    \"15efe45820_D95DF0B1F4INSPIRE_6272_10624\", \"15efe45820_D95DF0B1F4INSPIRE_6784_6784\",\n",
    "    \"15efe45820_D95DF0B1F4INSPIRE_6528_8576\",\n",
    "\n",
    "    # Group 4\n",
    "    \"c6d131e346_536DE05ED2OPENPIPELINE_128_896\", \"c6d131e346_536DE05ED2OPENPIPELINE_256_768\",\n",
    "    \"c6d131e346_536DE05ED2OPENPIPELINE_256_896\", \"c6d131e346_536DE05ED2OPENPIPELINE_1792_512\",\n",
    "    \"c6d131e346_536DE05ED2OPENPIPELINE_1792_640\", \"c6d131e346_536DE05ED2OPENPIPELINE_256_640\",\n",
    "    \"c6d131e346_536DE05ED2OPENPIPELINE_128_640\", \"c6d131e346_536DE05ED2OPENPIPELINE_128_128\",\n",
    "    \"c6d131e346_536DE05ED2OPENPIPELINE_256_128\", \"c6d131e346_536DE05ED2OPENPIPELINE_256_512\",\n",
    "    \"c6d131e346_536DE05ED2OPENPIPELINE_2688_2176\", \"c6d131e346_536DE05ED2OPENPIPELINE_2560_2176\",\n",
    "    \"c6d131e346_536DE05ED2OPENPIPELINE_2688_2048\", \"c6d131e346_536DE05ED2OPENPIPELINE_2560_2048\",\n",
    "    \"c6d131e346_536DE05ED2OPENPIPELINE_2688_2304\", \"c6d131e346_536DE05ED2OPENPIPELINE_2560_2304\",\n",
    "    \"c6d131e346_536DE05ED2OPENPIPELINE_2816_2176\", \"c6d131e346_536DE05ED2OPENPIPELINE_2816_2048\",\n",
    "    \"c6d131e346_536DE05ED2OPENPIPELINE_2816_2304\", \"c6d131e346_536DE05ED2OPENPIPELINE_2304_2560\",\n",
    "    \"c6d131e346_536DE05ED2OPENPIPELINE_2304_2688\", \"c6d131e346_536DE05ED2OPENPIPELINE_2432_2688\",\n",
    "    \"c6d131e346_536DE05ED2OPENPIPELINE_2432_2560\", \"c6d131e346_536DE05ED2OPENPIPELINE_2176_2560\",\n",
    "    \"c6d131e346_536DE05ED2OPENPIPELINE_2176_2688\",\n",
    "\n",
    "    # Group 5\n",
    "    \"12fa5e614f_53197F206FOPENPIPELINE_384_3072\", \"12fa5e614f_53197F206FOPENPIPELINE_512_3072\",\n",
    "    \"12fa5e614f_53197F206FOPENPIPELINE_256_3200\", \"12fa5e614f_53197F206FOPENPIPELINE_1024_3712\",\n",
    "    \"12fa5e614f_53197F206FOPENPIPELINE_384_3200\", \"12fa5e614f_53197F206FOPENPIPELINE_640_3072\",\n",
    "    \"12fa5e614f_53197F206FOPENPIPELINE_256_3328\", \"12fa5e614f_53197F206FOPENPIPELINE_256_3072\",\n",
    "    \"12fa5e614f_53197F206FOPENPIPELINE_3200_1152\", \"12fa5e614f_53197F206FOPENPIPELINE_1152_2688\",\n",
    "    \"12fa5e614f_53197F206FOPENPIPELINE_1536_2432\", \"12fa5e614f_53197F206FOPENPIPELINE_1280_2560\",\n",
    "    \"12fa5e614f_53197F206FOPENPIPELINE_1536_2048\", \"12fa5e614f_53197F206FOPENPIPELINE_512_3840\",\n",
    "    \"12fa5e614f_53197F206FOPENPIPELINE_512_3712\", \"12fa5e614f_53197F206FOPENPIPELINE_1664_2304\",\n",
    "    \"12fa5e614f_53197F206FOPENPIPELINE_384_3456\", \"12fa5e614f_53197F206FOPENPIPELINE_384_3328\",\n",
    "    \"12fa5e614f_53197F206FOPENPIPELINE_1280_3584\", \"12fa5e614f_53197F206FOPENPIPELINE_384_3584\",\n",
    "    \"12fa5e614f_53197F206FOPENPIPELINE_3072_1152\", \"12fa5e614f_53197F206FOPENPIPELINE_3456_1024\",\n",
    "\n",
    "    # Group 6\n",
    "    \"5fa39d6378_DB9FF730D9OPENPIPELINE_3072_2688\", \"5fa39d6378_DB9FF730D9OPENPIPELINE_1024_6784\",\n",
    "    \"5fa39d6378_DB9FF730D9OPENPIPELINE_3712_2816\", \"5fa39d6378_DB9FF730D9OPENPIPELINE_3200_2688\",\n",
    "    \"5fa39d6378_DB9FF730D9OPENPIPELINE_2944_2816\", \"5fa39d6378_DB9FF730D9OPENPIPELINE_4224_3072\",\n",
    "    \"5fa39d6378_DB9FF730D9OPENPIPELINE_3328_4992\", \"5fa39d6378_DB9FF730D9OPENPIPELINE_1024_6528\",\n",
    "    \"5fa39d6378_DB9FF730D9OPENPIPELINE_3840_5888\", \"5fa39d6378_DB9FF730D9OPENPIPELINE_2816_4224\",\n",
    "    \"5fa39d6378_DB9FF730D9OPENPIPELINE_5760_1920\", \"5fa39d6378_DB9FF730D9OPENPIPELINE_3328_2816\",\n",
    "    \"5fa39d6378_DB9FF730D9OPENPIPELINE_4352_4864\", \"5fa39d6378_DB9FF730D9OPENPIPELINE_3072_6912\",\n",
    "    \"5fa39d6378_DB9FF730D9OPENPIPELINE_4096_3328\", \"5fa39d6378_DB9FF730D9OPENPIPELINE_2816_3968\",\n",
    "    \"5fa39d6378_DB9FF730D9OPENPIPELINE_5888_1920\", \"5fa39d6378_DB9FF730D9OPENPIPELINE_1280_2432\",\n",
    "    \"5fa39d6378_DB9FF730D9OPENPIPELINE_3584_2560\", \"5fa39d6378_DB9FF730D9OPENPIPELINE_1280_5632\",\n",
    "    \"5fa39d6378_DB9FF730D9OPENPIPELINE_1280_5504\", \"5fa39d6378_DB9FF730D9OPENPIPELINE_1408_5504\",\n",
    "    \"5fa39d6378_DB9FF730D9OPENPIPELINE_1408_5632\", \"5fa39d6378_DB9FF730D9OPENPIPELINE_4608_4480\",\n",
    "    \"5fa39d6378_DB9FF730D9OPENPIPELINE_4608_4352\", \"5fa39d6378_DB9FF730D9OPENPIPELINE_4736_4480\",\n",
    "    \"5fa39d6378_DB9FF730D9OPENPIPELINE_2816_6400\", \"5fa39d6378_DB9FF730D9OPENPIPELINE_2944_6400\",\n",
    "    \"5fa39d6378_DB9FF730D9OPENPIPELINE_1280_5760\", \"5fa39d6378_DB9FF730D9OPENPIPELINE_2816_6528\",\n",
    "    \"5fa39d6378_DB9FF730D9OPENPIPELINE_2944_6528\", \"5fa39d6378_DB9FF730D9OPENPIPELINE_1408_5760\",\n",
    "    \"5fa39d6378_DB9FF730D9OPENPIPELINE_4608_4608\", \"5fa39d6378_DB9FF730D9OPENPIPELINE_4480_4352\",\n",
    "    \"5fa39d6378_DB9FF730D9OPENPIPELINE_4736_4608\", \"5fa39d6378_DB9FF730D9OPENPIPELINE_4480_4480\",\n",
    "    \"5fa39d6378_DB9FF730D9OPENPIPELINE_1280_5376\", \"5fa39d6378_DB9FF730D9OPENPIPELINE_1408_5376\",\n",
    "    \"5fa39d6378_DB9FF730D9OPENPIPELINE_4736_4352\", \"5fa39d6378_DB9FF730D9OPENPIPELINE_2816_6272\",\n",
    "    \"5fa39d6378_DB9FF730D9OPENPIPELINE_2944_6272\", \"5fa39d6378_DB9FF730D9OPENPIPELINE_1152_5760\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- Class Weights ---\n",
    "def compute_class_weights(generator):\n",
    "    pixel_counts = np.zeros(NUM_CLASSES, dtype=np.int64)\n",
    "\n",
    "    for _, labels in generator:\n",
    "        flat = np.argmax(labels, axis=-1).flatten()\n",
    "        counts = np.bincount(flat, minlength=NUM_CLASSES)\n",
    "        pixel_counts[:len(counts)] += counts\n",
    "\n",
    "    total = np.sum(pixel_counts)\n",
    "    weights = total / (NUM_CLASSES * np.maximum(pixel_counts, 1))\n",
    "    weights = weights / np.sum(weights) * NUM_CLASSES\n",
    "    return tf.constant(weights, dtype=tf.float32)\n",
    "\n",
    "\n",
    "\n",
    "# --- Distribution ---\n",
    "def plot_class_distribution(class_counts, title=\"Class Distribution\"):\n",
    "    if isinstance(class_counts, dict):\n",
    "        pixel_counts = [class_counts.get(i, 0) for i in range(NUM_CLASSES)]\n",
    "    else:\n",
    "        pixel_counts = list(class_counts)\n",
    "\n",
    "    total_pixels = sum(pixel_counts)\n",
    "    if total_pixels == 0:\n",
    "        print(\"No pixels counted for class distribution plot.\")\n",
    "        return\n",
    "\n",
    "    class_names = [f\"{i}: {name}\" for i, name in enumerate(CLASS_NAMES)]\n",
    "    colours = [np.array(CLASS_TO_COLOR[i]) / 255.0 for i in range(NUM_CLASSES)]\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    bars = plt.bar(class_names, pixel_counts, color=colours, edgecolor='black')\n",
    "\n",
    "    max_height = max(pixel_counts)\n",
    "    plt.ylim(0, max_height * 1.15)  # Add 15% headroom above the tallest bar\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Class\")\n",
    "    plt.ylabel(\"Pixel Count\")\n",
    "    plt.grid(True, axis='y', linestyle='--', alpha=0.5)\n",
    "\n",
    "    for bar, count in zip(bars, pixel_counts):\n",
    "        percent = 100.0 * count / total_pixels\n",
    "        plt.text(\n",
    "            bar.get_x() + bar.get_width() / 2,\n",
    "            bar.get_height() + max_height * 0.01,\n",
    "            f\"{count:,}\\n({percent:.2f}%)\",\n",
    "            ha='center',\n",
    "            va='bottom',\n",
    "            fontsize=9\n",
    "        )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(out_dir, \"epoch_dist.png\"))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def print_class_distribution(generator, title=\"Class Distribution\", max_batches=32):\n",
    "    pixel_counts = np.zeros(NUM_CLASSES, dtype=np.int64)\n",
    "    total_batches = 0\n",
    "\n",
    "    print(f\"\\n{title}\")\n",
    "    print(f\"Starting distribution scan (max {max_batches} batches)...\")\n",
    "\n",
    "    for i, (_, labels) in enumerate(generator):\n",
    "        if labels.size == 0:\n",
    "            print(f\"Skipping empty batch at index {i}\")\n",
    "            continue\n",
    "        \n",
    "        flat = np.argmax(labels, axis=-1).flatten()\n",
    "        counts = np.bincount(flat, minlength=NUM_CLASSES)\n",
    "        pixel_counts[:len(counts)] += counts\n",
    "        total_batches += 1\n",
    "\n",
    "        if total_batches >= max_batches:\n",
    "            print(f\"Processed {total_batches} batches. Stopping early.\")\n",
    "            break\n",
    "\n",
    "    total_pixels = np.sum(pixel_counts)\n",
    "    if total_pixels == 0:\n",
    "        print(\"No valid pixels found.\")\n",
    "        return\n",
    "\n",
    "    class_names = [\n",
    "        \"0: Building\", \"1: Clutter\", \"2: Vegetation\",\n",
    "        \"3: Water\", \"4: Background\", \"5: Car\"\n",
    "    ]\n",
    "\n",
    "    print(f\"\\nTotal pixels processed: {total_pixels:,}\")\n",
    "    print(\"Pixel Distribution (percentages):\\n\")\n",
    "    for i in range(NUM_CLASSES):\n",
    "        pct = (pixel_counts[i] / total_pixels) * 100\n",
    "        print(f\"Class {class_names[i]:<14} : {pct:6.2f}% ({pixel_counts[i]:,} px)\")\n",
    "\n",
    "    print(\"\\nDone.\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- Loss Functions (Unused) ---\n",
    "\n",
    "weights = np.array([0.1666, 0.1666, 0.1666, 0.1666, 0.1666, 0.1666], dtype=np.float32)\n",
    "\n",
    "# Dice Loss\n",
    "def dice_loss(y_true, y_pred, smooth=1e-6):\n",
    "    # Ensure inputs are float32\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    \n",
    "    # Flatten predictions and ground truth\n",
    "    y_true_f = tf.reshape(y_true, [-1, 6])  # Shape: [batch_size * pixels, 6]\n",
    "    y_pred_f = tf.reshape(y_pred, [-1, 6])\n",
    "    \n",
    "    # Compute intersection and union per class\n",
    "    intersection = tf.reduce_sum(y_true_f * y_pred_f, axis=0)  # Sum over pixels, per class\n",
    "    denominator = tf.reduce_sum(y_true_f, axis=0) + tf.reduce_sum(y_pred_f, axis=0) + smooth\n",
    "    \n",
    "    # Dice score per class\n",
    "    dice = (2.0 * intersection + smooth) / denominator\n",
    "    \n",
    "    # Apply class weights and compute mean loss\n",
    "    weighted_dice = weights * dice\n",
    "    dice_loss = 1.0 - tf.reduce_mean(weighted_dice)\n",
    "    \n",
    "    return dice_loss\n",
    "\n",
    "# Categorical Focal Loss\n",
    "def categorical_focal_loss(gamma=2.0):\n",
    "    def focal_loss(y_true, y_pred):\n",
    "        # Ensure inputs are float32\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        y_pred = tf.cast(y_pred, tf.float32)\n",
    "        \n",
    "        # Clip predictions to avoid log(0)\n",
    "        y_pred = tf.clip_by_value(y_pred, 1e-7, 1.0 - 1e-7)\n",
    "        \n",
    "        # Compute cross-entropy\n",
    "        ce = -y_true * tf.math.log(y_pred)\n",
    "        \n",
    "        # Focal factor: (1 - p_t)^gamma\n",
    "        focal_factor = tf.pow(1.0 - y_pred, gamma)\n",
    "        \n",
    "        # Weighted focal loss\n",
    "        loss = focal_factor * ce\n",
    "        \n",
    "        # Mean over classes and pixels\n",
    "        return tf.reduce_mean(loss)\n",
    "    \n",
    "    return focal_loss\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOIvhFb8l65g0rsyWayDedf",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
