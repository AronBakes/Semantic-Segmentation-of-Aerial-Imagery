{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yfkCy82a_1IM"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# --- Color and Class Constants ---\n",
    "COLOR_TO_CLASS = {\n",
    "    (230, 25, 75): 0,       # BUILDING\n",
    "    (145, 30, 180): 1,      # CLUTTER\n",
    "    (60, 180, 75): 2,       # VEGETATION\n",
    "    (245, 130, 48): 3,      # WATER\n",
    "    (255, 255, 255): 4,     # GROUND\n",
    "    (0, 130, 200): 5        # CAR\n",
    "}\n",
    "\n",
    "CLASS_TO_COLOR = {v: k for k, v in COLOR_TO_CLASS.items()}\n",
    "NUM_CLASSES = len(COLOR_TO_CLASS)\n",
    "COLOR_PALETTE = np.array(list(COLOR_TO_CLASS.keys()), dtype=np.uint8)\n",
    "COLOR_LOOKUP = {tuple(c): i for c, i in COLOR_TO_CLASS.items()}\n",
    "CLASS_NAMES = ['Building', 'Clutter', 'Vegetation', 'Water', 'Background', 'Car']\n",
    "class_names = CLASS_NAMES  # For compatibility with existing code\n",
    "\n",
    "\n",
    "# --- Constants ---\n",
    "INPUT_TYPE_CONFIG = {\n",
    "    \"rgb\": {\"description\": \"RGB only\", \"channels\": 3},\n",
    "    \"rgb_elev\": {\"description\": \"RGB + elevation\", \"channels\": 5}\n",
    "}\n",
    "\n",
    "\n",
    "base_dir=\"/content/chipped_data/content/chipped_data\"\n",
    "out_dir=\"/content/figs\"\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def filter_tile_ids_by_substring(image_dir: str, base_names: list[str]) -> list[str]:\n",
    "    \"\"\"Filters filenames in a directory to extract tile IDs that match given substrings.\n",
    "\n",
    "    Looks for files ending in '-ortho.png' and checks if any given base name is present\n",
    "    in the filename. If matched, strips the '-ortho.png' suffix and returns the base ID.\n",
    "\n",
    "    Args:\n",
    "        image_dir (str): Path to the directory containing image files.\n",
    "        base_names (list[str]): List of substrings to match in the filenames.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: A list of tile IDs (filenames without '-ortho.png') that contain\n",
    "        any of the specified base substrings.\n",
    "    \"\"\"\n",
    "    return [\n",
    "        f.replace('-ortho.png', '')\n",
    "        for f in os.listdir(image_dir)\n",
    "        if any(base in f for base in base_names)\n",
    "    ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# List of all available file prefixes (tile_ids without coordinate suffix)\n",
    "# These are used to identify the source files for splitting the dataset.\n",
    "all_files = [\n",
    "    '107f24d6e9_F1BE1D4184INSPIRE', '11cdce7802_B6A62F8BE0INSPIRE', '12fa5e614f_53197F206FOPENPIPELINE', '130a76ebe1_68B40B480AOPENPIPELINE', \n",
    "    '1476907971_CHADGRISMOPENPIPELINE', '1553541487_APIGENERATED', '1553541585_APIGENERATED', '1553627230_APIGENERATED', '15efe45820_D95DF0B1F4INSPIRE', \n",
    "    '1726eb08ef_60693DB04DINSPIRE', '1d056881e8_29FEA32BC7INSPIRE', '1d4fbe33f3_F1BE1D4184INSPIRE', '1df70e7340_4413A67E91INSPIRE', '2552eb56dd_2AABB46C86OPENPIPELINE', \n",
    "    '25f1c24f30_EB81FE6E2BOPENPIPELINE', '2ef3a4994a_0CCD105428INSPIRE', '2ef883f08d_F317F9C1DFOPENPIPELINE', '34fbf7c2bd_E8AD935CEDINSPIRE', \n",
    "    '3502e187b2_23071E4605OPENPIPELINE', '39e77bedd0_729FB913CDOPENPIPELINE', '420d6b69b8_84B52814D2OPENPIPELINE', '520947aa07_8FCB044F58OPENPIPELINE', \n",
    "    '551063e3c5_8FCB044F58INSPIRE', '57426ebe1e_84B52814D2OPENPIPELINE', '5fa39d6378_DB9FF730D9OPENPIPELINE', '6f93b9026b_F1BFB8B17DOPENPIPELINE', \n",
    "    '7008b80b00_FF24A4975DINSPIRE', '74d7796531_EB81FE6E2BOPENPIPELINE', '7c719dfcc0_310490364FINSPIRE', '84410645db_8D20F02042OPENPIPELINE', \n",
    "    '8710b98ea0_06E6522D6DINSPIRE', '888432f840_80E7FD39EBINSPIRE', '9170479165_625EDFBAB6OPENPIPELINE', 'a1af86939f_F1BE1D4184OPENPIPELINE', \n",
    "    'b61673f780_4413A67E91INSPIRE', 'b705d0cc9c_E5F5E0E316OPENPIPELINE', 'b771104de5_7E02A41EBEOPENPIPELINE', 'c2e8370ca3_3340CAC7AEOPENPIPELINE', \n",
    "    'c37dbfae2f_84B52814D2OPENPIPELINE', 'c644f91210_27E21B7F30OPENPIPELINE', 'c6d131e346_536DE05ED2OPENPIPELINE', 'c8a7031e5f_32156F5DC2INSPIRE', \n",
    "    'cc4b443c7d_A9CBEF2C97INSPIRE', 'd06b2c67d2_2A62B67B52OPENPIPELINE', 'd9161f7e18_C05BA1BC72OPENPIPELINE', 'dabec5e872_E8AD935CEDINSPIRE', \n",
    "    'e87da4ebdb_29FEA32BC7INSPIRE', 'ebffe540d0_7BA042D858OPENPIPELINE', 'ec09336a6f_06BA0AF311OPENPIPELINE', \n",
    "    'f0747ed88d_E74C0DD8FDOPENPIPELINE', 'f4dd768188_NOLANOPENPIPELINE', 'f56b6b2232_2A62B67B52OPENPIPELINE', \n",
    "    'f971256246_MIKEINSPIRE', 'f9f43e5144_1DB9E6F68BINSPIRE', 'fc5837dcf8_7CD52BE09EINSPIRE'\n",
    "]\n",
    "\n",
    "# Prefixes for files designated for the validation set\n",
    "val_files = [\n",
    "    \"c644f91210_27E21B7F30OPENPIPELINE\",\n",
    "    \"f9f43e5144_1DB9E6F68BINSPIRE\",\n",
    "    \"1d056881e8_29FEA32BC7INSPIRE\",\n",
    "    \"3502e187b2_23071E4605OPENPIPELINE\",\n",
    "    \"d9161f7e18_C05BA1BC72OPENPIPELINE\",\n",
    "    \"c8a7031e5f_32156F5DC2INSPIRE\",\n",
    "    \"551063e3c5_8FCB044F58INSPIRE\",\n",
    "    \"fc5837dcf8_7CD52BE09EINSPIRE\",\n",
    "    \"39e77bedd0_729FB913CDOPENPIPELINE\",\n",
    "]\n",
    "\n",
    "# Prefixes for files designated for the test set\n",
    "test_files = [\n",
    "    \"25f1c24f30_EB81FE6E2BOPENPIPELINE\",\n",
    "    \"1d4fbe33f3_F1BE1D4184INSPIRE\",\n",
    "    \"15efe45820_D95DF0B1F4INSPIRE\",\n",
    "    \"c6d131e346_536DE05ED2OPENPIPELINE\",\n",
    "    \"12fa5e614f_53197F206FOPENPIPELINE\",\n",
    "    \"5fa39d6378_DB9FF730D9OPENPIPELINE\",\n",
    "    \"ebffe540d0_7BA042D858OPENPIPELINE\",\n",
    "    \"8710b98ea0_06E6522D6DINSPIRE\",\n",
    "    \"84410645db_8D20F02042OPENPIPELINE\",\n",
    "    \"a1af86939f_F1BE1D4184OPENPIPELINE\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# List of specific tile IDs to select for detailed visualization (e.g., problematic chips)\n",
    "# These are the full tile_id strings including x_y coordinates\n",
    "specific_tile_ids = [\n",
    "    # Group 1\n",
    "    \"25f1c24f30_EB81FE6E2BOPENPIPELINE_3456_1280\", \"25f1c24f30_EB81FE6E2BOPENPIPELINE_3584_8320\",\n",
    "    \"25f1c24f30_EB81FE6E2BOPENPIPELINE_896_2816\", \"25f1c24f30_EB81FE6E2BOPENPIPELINE_3840_4736\",\n",
    "    \"25f1c24f30_EB81FE6E2BOPENPIPELINE_3968_384\", \"25f1c24f30_EB81FE6E2BOPENPIPELINE_4736_512\",\n",
    "    \"25f1c24f30_EB81FE6E2BOPENPIPELINE_4736_768\", \"25f1c24f30_EB81FE6E2BOPENPIPELINE_1024_5888\",\n",
    "    \"25f1c24f30_EB81FE6E2BOPENPIPELINE_896_5888\", \"25f1c24f30_EB81FE6E2BOPENPIPELINE_1024_6016\",\n",
    "\n",
    "    # Group 2\n",
    "    \"1d4fbe33f3_F1BE1D4184INSPIRE_2560_4864\", \"1d4fbe33f3_F1BE1D4184INSPIRE_896_3584\",\n",
    "    \"1d4fbe33f3_F1BE1D4184INSPIRE_768_3584\", \"1d4fbe33f3_F1BE1D4184INSPIRE_896_3712\",\n",
    "    \"1d4fbe33f3_F1BE1D4184INSPIRE_1280_2432\", \"1d4fbe33f3_F1BE1D4184INSPIRE_1536_4608\",\n",
    "    \"1d4fbe33f3_F1BE1D4184INSPIRE_1152_2432\", \"1d4fbe33f3_F1BE1D4184INSPIRE_1664_4864\",\n",
    "    \"1d4fbe33f3_F1BE1D4184INSPIRE_1664_4736\", \"1d4fbe33f3_F1BE1D4184INSPIRE_1408_1280\",\n",
    "    \"1d4fbe33f3_F1BE1D4184INSPIRE_1152_4864\", \"1d4fbe33f3_F1BE1D4184INSPIRE_1280_2432\",\n",
    "    \"1d4fbe33f3_F1BE1D4184INSPIRE_1408_1408\", \"1d4fbe33f3_F1BE1D4184INSPIRE_1536_4736\",\n",
    "    \"1d4fbe33f3_F1BE1D4184INSPIRE_384_1152\",\n",
    "\n",
    "    # Group 3\n",
    "    \"15efe45820_D95DF0B1F4INSPIRE_4736_9472\", \"15efe45820_D95DF0B1F4INSPIRE_9600_6016\",\n",
    "    \"15efe45820_D95DF0B1F4INSPIRE_5888_6272\", \"15efe45820_D95DF0B1F4INSPIRE_7168_7936\",\n",
    "    \"15efe45820_D95DF0B1F4INSPIRE_6016_6272\", \"15efe45820_D95DF0B1F4INSPIRE_8704_1024\",\n",
    "    \"15efe45820_D95DF0B1F4INSPIRE_7040_6912\", \"15efe45820_D95DF0B1F4INSPIRE_8064_3968\",\n",
    "    \"15efe45820_D95DF0B1F4INSPIRE_2688_2048\", \"15efe45820_D95DF0B1F4INSPIRE_7680_1920\",\n",
    "    \"15efe45820_D95DF0B1F4INSPIRE_6272_10624\", \"15efe45820_D95DF0B1F4INSPIRE_6784_6784\",\n",
    "    \"15efe45820_D95DF0B1F4INSPIRE_6528_8576\",\n",
    "\n",
    "    # Group 4\n",
    "    \"c6d131e346_536DE05ED2OPENPIPELINE_128_896\", \"c6d131e346_536DE05ED2OPENPIPELINE_256_768\",\n",
    "    \"c6d131e346_536DE05ED2OPENPIPELINE_256_896\", \"c6d131e346_536DE05ED2OPENPIPELINE_1792_512\",\n",
    "    \"c6d131e346_536DE05ED2OPENPIPELINE_1792_640\", \"c6d131e346_536DE05ED2OPENPIPELINE_256_640\",\n",
    "    \"c6d131e346_536DE05ED2OPENPIPELINE_128_640\", \"c6d131e346_536DE05ED2OPENPIPELINE_128_128\",\n",
    "    \"c6d131e346_536DE05ED2OPENPIPELINE_256_128\", \"c6d131e346_536DE05ED2OPENPIPELINE_256_512\",\n",
    "    \"c6d131e346_536DE05ED2OPENPIPELINE_2688_2176\", \"c6d131e346_536DE05ED2OPENPIPELINE_2560_2176\",\n",
    "    \"c6d131e346_536DE05ED2OPENPIPELINE_2688_2048\", \"c6d131e346_536DE05ED2OPENPIPELINE_2560_2048\",\n",
    "    \"c6d131e346_536DE05ED2OPENPIPELINE_2688_2304\", \"c6d131e346_536DE05ED2OPENPIPELINE_2560_2304\",\n",
    "    \"c6d131e346_536DE05ED2OPENPIPELINE_2816_2176\", \"c6d131e346_536DE05ED2OPENPIPELINE_2816_2048\",\n",
    "    \"c6d131e346_536DE05ED2OPENPIPELINE_2816_2304\", \"c6d131e346_536DE05ED2OPENPIPELINE_2304_2560\",\n",
    "    \"c6d131e346_536DE05ED2OPENPIPELINE_2304_2688\", \"c6d131e346_536DE05ED2OPENPIPELINE_2432_2688\",\n",
    "    \"c6d131e346_536DE05ED2OPENPIPELINE_2432_2560\", \"c6d131e346_536DE05ED2OPENPIPELINE_2176_2560\",\n",
    "    \"c6d131e346_536DE05ED2OPENPIPELINE_2176_2688\",\n",
    "\n",
    "    # Group 5\n",
    "    \"12fa5e614f_53197F206FOPENPIPELINE_384_3072\", \"12fa5e614f_53197F206FOPENPIPELINE_512_3072\",\n",
    "    \"12fa5e614f_53197F206FOPENPIPELINE_256_3200\", \"12fa5e614f_53197F206FOPENPIPELINE_1024_3712\",\n",
    "    \"12fa5e614f_53197F206FOPENPIPELINE_384_3200\", \"12fa5e614f_53197F206FOPENPIPELINE_640_3072\",\n",
    "    \"12fa5e614f_53197F206FOPENPIPELINE_256_3328\", \"12fa5e614f_53197F206FOPENPIPELINE_256_3072\",\n",
    "    \"12fa5e614f_53197F206FOPENPIPELINE_3200_1152\", \"12fa5e614f_53197F206FOPENPIPELINE_1152_2688\",\n",
    "    \"12fa5e614f_53197F206FOPENPIPELINE_1536_2432\", \"12fa5e614f_53197F206FOPENPIPELINE_1280_2560\",\n",
    "    \"12fa5e614f_53197F206FOPENPIPELINE_1536_2048\", \"12fa5e614f_53197F206FOPENPIPELINE_512_3840\",\n",
    "    \"12fa5e614f_53197F206FOPENPIPELINE_512_3712\", \"12fa5e614f_53197F206FOPENPIPELINE_1664_2304\",\n",
    "    \"12fa5e614f_53197F206FOPENPIPELINE_384_3456\", \"12fa5e614f_53197F206FOPENPIPELINE_384_3328\",\n",
    "    \"12fa5e614f_53197F206FOPENPIPELINE_1280_3584\", \"12fa5e614f_53197F206FOPENPIPELINE_384_3584\",\n",
    "    \"12fa5e614f_53197F206FOPENPIPELINE_3072_1152\", \"12fa5e614f_53197F206FOPENPIPELINE_3456_1024\",\n",
    "\n",
    "    # Group 6\n",
    "    \"5fa39d6378_DB9FF730D9OPENPIPELINE_3072_2688\", \"5fa39d6378_DB9FF730D9OPENPIPELINE_1024_6784\",\n",
    "    \"5fa39d6378_DB9FF730D9OPENPIPELINE_3712_2816\", \"5fa39d6378_DB9FF730D9OPENPIPELINE_3200_2688\",\n",
    "    \"5fa39d6378_DB9FF730D9OPENPIPELINE_2944_2816\", \"5fa39d6378_DB9FF730D9OPENPIPELINE_4224_3072\",\n",
    "    \"5fa39d6378_DB9FF730D9OPENPIPELINE_3328_4992\", \"5fa39d6378_DB9FF730D9OPENPIPELINE_1024_6528\",\n",
    "    \"5fa39d6378_DB9FF730D9OPENPIPELINE_3840_5888\", \"5fa39d6378_DB9FF730D9OPENPIPELINE_2816_4224\",\n",
    "    \"5fa39d6378_DB9FF730D9OPENPIPELINE_5760_1920\", \"5fa39d6378_DB9FF730D9OPENPIPELINE_3328_2816\",\n",
    "    \"5fa39d6378_DB9FF730D9OPENPIPELINE_4352_4864\", \"5fa39d6378_DB9FF730D9OPENPIPELINE_3072_6912\",\n",
    "    \"5fa39d6378_DB9FF730D9OPENPIPELINE_4096_3328\", \"5fa39d6378_DB9FF730D9OPENPIPELINE_2816_3968\",\n",
    "    \"5fa39d6378_DB9FF730D9OPENPIPELINE_5888_1920\", \"5fa39d6378_DB9FF730D9OPENPIPELINE_1280_2432\",\n",
    "    \"5fa39d6378_DB9FF730D9OPENPIPELINE_3584_2560\", \"5fa39d6378_DB9FF730D9OPENPIPELINE_1280_5632\",\n",
    "    \"5fa39d6378_DB9FF730D9OPENPIPELINE_1280_5504\", \"5fa39d6378_DB9FF730D9OPENPIPELINE_1408_5504\",\n",
    "    \"5fa39d6378_DB9FF730D9OPENPIPELINE_1408_5632\", \"5fa39d6378_DB9FF730D9OPENPIPELINE_4608_4480\",\n",
    "    \"5fa39d6378_DB9FF730D9OPENPIPELINE_4608_4352\", \"5fa39d6378_DB9FF730D9OPENPIPELINE_4736_4480\",\n",
    "    \"5fa39d6378_DB9FF730D9OPENPIPELINE_2816_6400\", \"5fa39d6378_DB9FF730D9OPENPIPELINE_2944_6400\",\n",
    "    \"5fa39d6378_DB9FF730D9OPENPIPELINE_1280_5760\", \"5fa39d6378_DB9FF730D9OPENPIPELINE_2816_6528\",\n",
    "    \"5fa39d6378_DB9FF730D9OPENPIPELINE_2944_6528\", \"5fa39d6378_DB9FF730D9OPENPIPELINE_1408_5760\",\n",
    "    \"5fa39d6378_DB9FF730D9OPENPIPELINE_4608_4608\", \"5fa39d6378_DB9FF730D9OPENPIPELINE_4480_4352\",\n",
    "    \"5fa39d6378_DB9FF730D9OPENPIPELINE_4736_4608\", \"5fa39d6378_DB9FF730D9OPENPIPELINE_4480_4480\",\n",
    "    \"5fa39d6378_DB9FF730D9OPENPIPELINE_1280_5376\", \"5fa39d6378_DB9FF730D9OPENPIPELINE_1408_5376\",\n",
    "    \"5fa39d6378_DB9FF730D9OPENPIPELINE_4736_4352\", \"5fa39d6378_DB9FF730D9OPENPIPELINE_2816_6272\",\n",
    "    \"5fa39d6378_DB9FF730D9OPENPIPELINE_2944_6272\", \"5fa39d6378_DB9FF730D9OPENPIPELINE_1152_5760\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- Visualisation ---\n",
    "\n",
    "'''\n",
    "def visualise_prediction(rgb, true_mask_onehot, pred_mask):\n",
    "    \"\"\"\n",
    "    Visualise prediction with magenta overlay for ignore regions.\n",
    "    \"\"\"\n",
    "    IGNORE_COLOR = (255, 0, 255)\n",
    "    CLASS_TO_COLOR = {\n",
    "        0: (230, 25, 75),    # Building\n",
    "        1: (145, 30, 180),   # Clutter\n",
    "        2: (60, 180, 75),    # Vegetation\n",
    "        3: (245, 130, 48),   # Water\n",
    "        4: (255, 255, 255),  # Background\n",
    "        5: (0, 130, 200),    # Car\n",
    "    }\n",
    "\n",
    "    ignore_mask = np.all(true_mask_onehot == 0, axis=-1)  # 🟣 Detect ignored pixels\n",
    "\n",
    "    true_mask = np.argmax(true_mask_onehot, axis=-1)      # 🔢 Decode only after ignore_mask is made\n",
    "\n",
    "    h, w = true_mask.shape\n",
    "    true_rgb = np.zeros((h, w, 3), dtype=np.uint8)\n",
    "    pred_rgb = np.zeros((h, w, 3), dtype=np.uint8)\n",
    "\n",
    "    for cid, col in CLASS_TO_COLOR.items():\n",
    "        true_rgb[true_mask == cid] = col\n",
    "        pred_rgb[pred_mask == cid] = col\n",
    "\n",
    "    # 🟣 Overlay ignored regions\n",
    "    true_rgb[ignore_mask] = IGNORE_COLOR\n",
    "    pred_rgb[ignore_mask] = IGNORE_COLOR\n",
    "\n",
    "    # --- Plot ---\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(16, 5))\n",
    "    axs[0].imshow(rgb)\n",
    "    axs[0].set_title(\"RGB Image\")\n",
    "    axs[0].axis(\"off\")\n",
    "    axs[1].imshow(true_rgb)\n",
    "    axs[1].set_title(\"True Mask\")\n",
    "    axs[1].axis(\"off\")\n",
    "    axs[2].imshow(pred_rgb)\n",
    "    axs[2].set_title(\"Predicted Mask\")\n",
    "    axs[2].axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- Class Weights ---\n",
    "def compute_class_weights(generator):\n",
    "    pixel_counts = np.zeros(NUM_CLASSES, dtype=np.int64)\n",
    "\n",
    "    for _, labels in generator:\n",
    "        flat = np.argmax(labels, axis=-1).flatten()\n",
    "        counts = np.bincount(flat, minlength=NUM_CLASSES)\n",
    "        pixel_counts[:len(counts)] += counts\n",
    "\n",
    "    total = np.sum(pixel_counts)\n",
    "    weights = total / (NUM_CLASSES * np.maximum(pixel_counts, 1))\n",
    "    weights = weights / np.sum(weights) * NUM_CLASSES\n",
    "    return tf.constant(weights, dtype=tf.float32)\n",
    "\n",
    "\n",
    "\n",
    "# --- Distribution ---\n",
    "def plot_class_distribution(class_counts, title=\"Class Distribution\"):\n",
    "    if isinstance(class_counts, dict):\n",
    "        pixel_counts = [class_counts.get(i, 0) for i in range(NUM_CLASSES)]\n",
    "    else:\n",
    "        pixel_counts = list(class_counts)\n",
    "\n",
    "    total_pixels = sum(pixel_counts)\n",
    "    if total_pixels == 0:\n",
    "        print(\"No pixels counted for class distribution plot.\")\n",
    "        return\n",
    "\n",
    "    class_names = [f\"{i}: {name}\" for i, name in enumerate(CLASS_NAMES)]\n",
    "    colours = [np.array(CLASS_TO_COLOR[i]) / 255.0 for i in range(NUM_CLASSES)]\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    bars = plt.bar(class_names, pixel_counts, color=colours, edgecolor='black')\n",
    "\n",
    "    max_height = max(pixel_counts)\n",
    "    plt.ylim(0, max_height * 1.15)  # Add 15% headroom above the tallest bar\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Class\")\n",
    "    plt.ylabel(\"Pixel Count\")\n",
    "    plt.grid(True, axis='y', linestyle='--', alpha=0.5)\n",
    "\n",
    "    for bar, count in zip(bars, pixel_counts):\n",
    "        percent = 100.0 * count / total_pixels\n",
    "        plt.text(\n",
    "            bar.get_x() + bar.get_width() / 2,\n",
    "            bar.get_height() + max_height * 0.01,\n",
    "            f\"{count:,}\\n({percent:.2f}%)\",\n",
    "            ha='center',\n",
    "            va='bottom',\n",
    "            fontsize=9\n",
    "        )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(out_dir, \"epoch_dist.png\"))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def print_class_distribution(generator, title=\"Class Distribution\", max_batches=32):\n",
    "    pixel_counts = np.zeros(NUM_CLASSES, dtype=np.int64)\n",
    "    total_batches = 0\n",
    "\n",
    "    print(f\"\\n📊 {title}\")\n",
    "    print(f\"🔍 Starting distribution scan (max {max_batches} batches)...\")\n",
    "\n",
    "    for i, (_, labels) in enumerate(generator):\n",
    "        if labels.size == 0:\n",
    "            print(f\"⚠️ Skipping empty batch at index {i}\")\n",
    "            continue\n",
    "        \n",
    "        flat = np.argmax(labels, axis=-1).flatten()\n",
    "        counts = np.bincount(flat, minlength=NUM_CLASSES)\n",
    "        pixel_counts[:len(counts)] += counts\n",
    "        total_batches += 1\n",
    "\n",
    "        if total_batches >= max_batches:\n",
    "            print(f\"✅ Processed {total_batches} batches. Stopping early.\")\n",
    "            break\n",
    "\n",
    "    total_pixels = np.sum(pixel_counts)\n",
    "    if total_pixels == 0:\n",
    "        print(\"❌ No valid pixels found.\")\n",
    "        return\n",
    "\n",
    "    class_names = [\n",
    "        \"0: Building\", \"1: Clutter\", \"2: Vegetation\",\n",
    "        \"3: Water\", \"4: Background\", \"5: Car\"\n",
    "    ]\n",
    "\n",
    "    print(f\"\\n🧮 Total pixels processed: {total_pixels:,}\")\n",
    "    print(\"📈 Pixel Distribution (percentages):\\n\")\n",
    "    for i in range(NUM_CLASSES):\n",
    "        pct = (pixel_counts[i] / total_pixels) * 100\n",
    "        print(f\"Class {class_names[i]:<14} : {pct:6.2f}% ({pixel_counts[i]:,} px)\")\n",
    "\n",
    "    print(\"\\n✅ Done.\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# =================================================================== \n",
    "# ------------------------------------------------------------------- \n",
    "# =================================================================== \n",
    "# ------------------------------------------------------------------- \n",
    "# =================================================================== \n",
    "# -------------------------------------------------------------------\n",
    "# =================================================================== \n",
    "\n",
    "\n",
    "\n",
    "# Class weights for 6 classes (0 to 5), based on imbalance\n",
    "weights = np.array([0.2374, 0.2374, 0.0356, 0.2374, 0.0148, 0.2374], dtype=np.float32)\n",
    "\n",
    "# Dice Loss\n",
    "def dice_loss(y_true, y_pred, smooth=1e-6):\n",
    "    # Ensure inputs are float32\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    \n",
    "    # Flatten predictions and ground truth\n",
    "    y_true_f = tf.reshape(y_true, [-1, 6])  # Shape: [batch_size * pixels, 6]\n",
    "    y_pred_f = tf.reshape(y_pred, [-1, 6])\n",
    "    \n",
    "    # Compute intersection and union per class\n",
    "    intersection = tf.reduce_sum(y_true_f * y_pred_f, axis=0)  # Sum over pixels, per class\n",
    "    denominator = tf.reduce_sum(y_true_f, axis=0) + tf.reduce_sum(y_pred_f, axis=0) + smooth\n",
    "    \n",
    "    # Dice score per class\n",
    "    dice = (2.0 * intersection + smooth) / denominator\n",
    "    \n",
    "    # Apply class weights and compute mean loss\n",
    "    weighted_dice = weights * dice\n",
    "    dice_loss = 1.0 - tf.reduce_mean(weighted_dice)\n",
    "    \n",
    "    return dice_loss\n",
    "\n",
    "# Categorical Focal Loss\n",
    "def categorical_focal_loss(gamma=2.0):\n",
    "    def focal_loss(y_true, y_pred):\n",
    "        # Ensure inputs are float32\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        y_pred = tf.cast(y_pred, tf.float32)\n",
    "        \n",
    "        # Clip predictions to avoid log(0)\n",
    "        y_pred = tf.clip_by_value(y_pred, 1e-7, 1.0 - 1e-7)\n",
    "        \n",
    "        # Compute cross-entropy\n",
    "        ce = -y_true * tf.math.log(y_pred)\n",
    "        \n",
    "        # Focal factor: (1 - p_t)^gamma\n",
    "        focal_factor = tf.pow(1.0 - y_pred, gamma)\n",
    "        \n",
    "        # Weighted focal loss\n",
    "        loss = focal_factor * ce\n",
    "        \n",
    "        # Mean over classes and pixels\n",
    "        return tf.reduce_mean(loss)\n",
    "    \n",
    "    return focal_loss\n",
    "\n",
    "# Combined Loss\n",
    "def combined_loss(y_true, y_pred):\n",
    "    return dice_loss(y_true, y_pred) + categorical_focal_loss(gamma=2.0)(y_true, y_pred)\n",
    "\n",
    "# Example: Compile model\n",
    "# model = your_model  # e.g., U-Net with 6 classes\n",
    "# model.compile(optimizer='adam', loss=combined_loss, metrics=['accuracy'])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOIvhFb8l65g0rsyWayDedf",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
