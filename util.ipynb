{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yfkCy82a_1IM"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import warnings\n",
    "import logging\n",
    "import random\n",
    "import time\n",
    "import cv2\n",
    "import math\n",
    "import json\n",
    "import shutil\n",
    "import glob\n",
    "import re\n",
    "import itertools\n",
    "import copy\n",
    "import collections\n",
    "import h5py\n",
    "import pickle\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import MeanIoU\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.lines as mlines\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "from tensorflow.keras import layers, models \n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Conv2DTranspose, Dropout, concatenate, Input\n",
    "\n",
    "\n",
    "\n",
    "# --- Color and Class Constants ---\n",
    "COLOR_TO_CLASS = {\n",
    "    (230, 25, 75): 0,       # BUILDING\n",
    "    (145, 30, 180): 1,      # CLUTTER\n",
    "    (60, 180, 75): 2,       # VEGETATION\n",
    "    (245, 130, 48): 3,      # WATER\n",
    "    (255, 255, 255): 4,     # GROUND\n",
    "    (0, 130, 200): 5        # CAR\n",
    "}\n",
    "\n",
    "CLASS_TO_COLOR = {v: k for k, v in COLOR_TO_CLASS.items()}\n",
    "NUM_CLASSES = len(COLOR_TO_CLASS)\n",
    "COLOR_PALETTE = np.array(list(COLOR_TO_CLASS.keys()), dtype=np.uint8)\n",
    "COLOR_LOOKUP = {tuple(c): i for c, i in COLOR_TO_CLASS.items()}\n",
    "CLASS_NAMES = ['Building', 'Clutter', 'Vegetation', 'Water', 'Background', 'Car']\n",
    "class_names = CLASS_NAMES  # For compatibility with existing code\n",
    "\n",
    "\n",
    "# --- Constants ---\n",
    "INPUT_TYPE_CONFIG = {\n",
    "    \"rgb\": {\"description\": \"RGB only\", \"channels\": 3},\n",
    "    \"rgb_elev\": {\"description\": \"RGB + elevation\", \"channels\": 5}\n",
    "}\n",
    "\n",
    "\n",
    "base_dir=\"/content/chipped_data/content/chipped_data\"\n",
    "out_dir=\"/content/figs\"\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def filter_tile_ids_by_substring(image_dir: str, base_names: list[str]) -> list[str]:\n",
    "    \"\"\"Filters filenames in a directory to extract tile IDs that match given substrings.\n",
    "\n",
    "    Looks for files ending in '-ortho.png' and checks if any given base name is present\n",
    "    in the filename. If matched, strips the '-ortho.png' suffix and returns the base ID.\n",
    "\n",
    "    Args:\n",
    "        image_dir (str): Path to the directory containing image files.\n",
    "        base_names (list[str]): List of substrings to match in the filenames.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: A list of tile IDs (filenames without '-ortho.png') that contain\n",
    "        any of the specified base substrings.\n",
    "    \"\"\"\n",
    "    return [\n",
    "        f.replace('-ortho.png', '')\n",
    "        for f in os.listdir(image_dir)\n",
    "        if any(base in f for base in base_names)\n",
    "    ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# List of all available file prefixes (tile_ids without coordinate suffix)\n",
    "# These are used to identify the source files for splitting the dataset.\n",
    "all_files = [\n",
    "    '107f24d6e9_F1BE1D4184INSPIRE', '11cdce7802_B6A62F8BE0INSPIRE', '12fa5e614f_53197F206FOPENPIPELINE', '130a76ebe1_68B40B480AOPENPIPELINE', \n",
    "    '1476907971_CHADGRISMOPENPIPELINE', '1553541487_APIGENERATED', '1553541585_APIGENERATED', '1553627230_APIGENERATED', '15efe45820_D95DF0B1F4INSPIRE', \n",
    "    '1726eb08ef_60693DB04DINSPIRE', '1d056881e8_29FEA32BC7INSPIRE', '1d4fbe33f3_F1BE1D4184INSPIRE', '1df70e7340_4413A67E91INSPIRE', '2552eb56dd_2AABB46C86OPENPIPELINE', \n",
    "    '25f1c24f30_EB81FE6E2BOPENPIPELINE', '2ef3a4994a_0CCD105428INSPIRE', '2ef883f08d_F317F9C1DFOPENPIPELINE', '34fbf7c2bd_E8AD935CEDINSPIRE', \n",
    "    '3502e187b2_23071E4605OPENPIPELINE', '39e77bedd0_729FB913CDOPENPIPELINE', '420d6b69b8_84B52814D2OPENPIPELINE', '520947aa07_8FCB044F58OPENPIPELINE', \n",
    "    '551063e3c5_8FCB044F58INSPIRE', '57426ebe1e_84B52814D2OPENPIPELINE', '5fa39d6378_DB9FF730D9OPENPIPELINE', '6f93b9026b_F1BFB8B17DOPENPIPELINE', \n",
    "    '7008b80b00_FF24A4975DINSPIRE', '74d7796531_EB81FE6E2BOPENPIPELINE', '7c719dfcc0_310490364FINSPIRE', '84410645db_8D20F02042OPENPIPELINE', \n",
    "    '8710b98ea0_06E6522D6DINSPIRE', '888432f840_80E7FD39EBINSPIRE', '9170479165_625EDFBAB6OPENPIPELINE', 'a1af86939f_F1BE1D4184OPENPIPELINE', \n",
    "    'b61673f780_4413A67E91INSPIRE', 'b705d0cc9c_E5F5E0E316OPENPIPELINE', 'b771104de5_7E02A41EBEOPENPIPELINE', 'c2e8370ca3_3340CAC7AEOPENPIPELINE', \n",
    "    'c37dbfae2f_84B52814D2OPENPIPELINE', 'c644f91210_27E21B7F30OPENPIPELINE', 'c6d131e346_536DE05ED2OPENPIPELINE', 'c8a7031e5f_32156F5DC2INSPIRE', \n",
    "    'cc4b443c7d_A9CBEF2C97INSPIRE', 'd06b2c67d2_2A62B67B52OPENPIPELINE', 'd9161f7e18_C05BA1BC72OPENPIPELINE', 'dabec5e872_E8AD935CEDINSPIRE', \n",
    "    'e87da4ebdb_29FEA32BC7INSPIRE', 'ebffe540d0_7BA042D858OPENPIPELINE', 'ec09336a6f_06BA0AF311OPENPIPELINE', \n",
    "    'f0747ed88d_E74C0DD8FDOPENPIPELINE', 'f4dd768188_NOLANOPENPIPELINE', 'f56b6b2232_2A62B67B52OPENPIPELINE', \n",
    "    'f971256246_MIKEINSPIRE', 'f9f43e5144_1DB9E6F68BINSPIRE', 'fc5837dcf8_7CD52BE09EINSPIRE'\n",
    "]\n",
    "\n",
    "# Prefixes for files designated for the validation set\n",
    "val_files = [\n",
    "    \"c644f91210_27E21B7F30OPENPIPELINE\",\n",
    "    \"f9f43e5144_1DB9E6F68BINSPIRE\",\n",
    "    \"1d056881e8_29FEA32BC7INSPIRE\",\n",
    "    \"3502e187b2_23071E4605OPENPIPELINE\",\n",
    "    \"d9161f7e18_C05BA1BC72OPENPIPELINE\",\n",
    "    \"c8a7031e5f_32156F5DC2INSPIRE\",\n",
    "    \"551063e3c5_8FCB044F58INSPIRE\",\n",
    "    \"fc5837dcf8_7CD52BE09EINSPIRE\",\n",
    "    \"39e77bedd0_729FB913CDOPENPIPELINE\",\n",
    "]\n",
    "\n",
    "# Prefixes for files designated for the test set\n",
    "test_files = [\n",
    "    \"25f1c24f30_EB81FE6E2BOPENPIPELINE\",\n",
    "    \"1d4fbe33f3_F1BE1D4184INSPIRE\",\n",
    "    \"15efe45820_D95DF0B1F4INSPIRE\",\n",
    "    \"c6d131e346_536DE05ED2OPENPIPELINE\",\n",
    "    \"12fa5e614f_53197F206FOPENPIPELINE\",\n",
    "    \"5fa39d6378_DB9FF730D9OPENPIPELINE\",\n",
    "    \"ebffe540d0_7BA042D858OPENPIPELINE\",\n",
    "    \"8710b98ea0_06E6522D6DINSPIRE\",\n",
    "    \"84410645db_8D20F02042OPENPIPELINE\",\n",
    "    \"a1af86939f_F1BE1D4184OPENPIPELINE\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# --- Visualisation ---\n",
    "\n",
    "'''\n",
    "def visualise_prediction(rgb, true_mask_onehot, pred_mask):\n",
    "    \"\"\"\n",
    "    Visualise prediction with magenta overlay for ignore regions.\n",
    "    \"\"\"\n",
    "    IGNORE_COLOR = (255, 0, 255)\n",
    "    CLASS_TO_COLOR = {\n",
    "        0: (230, 25, 75),    # Building\n",
    "        1: (145, 30, 180),   # Clutter\n",
    "        2: (60, 180, 75),    # Vegetation\n",
    "        3: (245, 130, 48),   # Water\n",
    "        4: (255, 255, 255),  # Background\n",
    "        5: (0, 130, 200),    # Car\n",
    "    }\n",
    "\n",
    "    ignore_mask = np.all(true_mask_onehot == 0, axis=-1)  # üü£ Detect ignored pixels\n",
    "\n",
    "    true_mask = np.argmax(true_mask_onehot, axis=-1)      # üî¢ Decode only after ignore_mask is made\n",
    "\n",
    "    h, w = true_mask.shape\n",
    "    true_rgb = np.zeros((h, w, 3), dtype=np.uint8)\n",
    "    pred_rgb = np.zeros((h, w, 3), dtype=np.uint8)\n",
    "\n",
    "    for cid, col in CLASS_TO_COLOR.items():\n",
    "        true_rgb[true_mask == cid] = col\n",
    "        pred_rgb[pred_mask == cid] = col\n",
    "\n",
    "    # üü£ Overlay ignored regions\n",
    "    true_rgb[ignore_mask] = IGNORE_COLOR\n",
    "    pred_rgb[ignore_mask] = IGNORE_COLOR\n",
    "\n",
    "    # --- Plot ---\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(16, 5))\n",
    "    axs[0].imshow(rgb)\n",
    "    axs[0].set_title(\"RGB Image\")\n",
    "    axs[0].axis(\"off\")\n",
    "    axs[1].imshow(true_rgb)\n",
    "    axs[1].set_title(\"True Mask\")\n",
    "    axs[1].axis(\"off\")\n",
    "    axs[2].imshow(pred_rgb)\n",
    "    axs[2].set_title(\"Predicted Mask\")\n",
    "    axs[2].axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- Class Weights ---\n",
    "def compute_class_weights(generator):\n",
    "    pixel_counts = np.zeros(NUM_CLASSES, dtype=np.int64)\n",
    "\n",
    "    for _, labels in generator:\n",
    "        flat = np.argmax(labels, axis=-1).flatten()\n",
    "        counts = np.bincount(flat, minlength=NUM_CLASSES)\n",
    "        pixel_counts[:len(counts)] += counts\n",
    "\n",
    "    total = np.sum(pixel_counts)\n",
    "    weights = total / (NUM_CLASSES * np.maximum(pixel_counts, 1))\n",
    "    weights = weights / np.sum(weights) * NUM_CLASSES\n",
    "    return tf.constant(weights, dtype=tf.float32)\n",
    "\n",
    "\n",
    "\n",
    "# --- Distribution ---\n",
    "def plot_class_distribution(class_counts, title=\"Class Distribution\"):\n",
    "    if isinstance(class_counts, dict):\n",
    "        pixel_counts = [class_counts.get(i, 0) for i in range(NUM_CLASSES)]\n",
    "    else:\n",
    "        pixel_counts = list(class_counts)\n",
    "\n",
    "    total_pixels = sum(pixel_counts)\n",
    "    if total_pixels == 0:\n",
    "        print(\"No pixels counted for class distribution plot.\")\n",
    "        return\n",
    "\n",
    "    class_names = [f\"{i}: {name}\" for i, name in enumerate(CLASS_NAMES)]\n",
    "    colours = [np.array(CLASS_TO_COLOR[i]) / 255.0 for i in range(NUM_CLASSES)]\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    bars = plt.bar(class_names, pixel_counts, color=colours, edgecolor='black')\n",
    "\n",
    "    max_height = max(pixel_counts)\n",
    "    plt.ylim(0, max_height * 1.15)  # Add 15% headroom above the tallest bar\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Class\")\n",
    "    plt.ylabel(\"Pixel Count\")\n",
    "    plt.grid(True, axis='y', linestyle='--', alpha=0.5)\n",
    "\n",
    "    for bar, count in zip(bars, pixel_counts):\n",
    "        percent = 100.0 * count / total_pixels\n",
    "        plt.text(\n",
    "            bar.get_x() + bar.get_width() / 2,\n",
    "            bar.get_height() + max_height * 0.01,\n",
    "            f\"{count:,}\\n({percent:.2f}%)\",\n",
    "            ha='center',\n",
    "            va='bottom',\n",
    "            fontsize=9\n",
    "        )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(out_dir, \"epoch_dist.png\"))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def print_class_distribution(generator, title=\"Class Distribution\", max_batches=32):\n",
    "    pixel_counts = np.zeros(NUM_CLASSES, dtype=np.int64)\n",
    "    total_batches = 0\n",
    "\n",
    "    print(f\"\\nüìä {title}\")\n",
    "    print(f\"üîç Starting distribution scan (max {max_batches} batches)...\")\n",
    "\n",
    "    for i, (_, labels) in enumerate(generator):\n",
    "        if labels.size == 0:\n",
    "            print(f\"‚ö†Ô∏è Skipping empty batch at index {i}\")\n",
    "            continue\n",
    "        \n",
    "        flat = np.argmax(labels, axis=-1).flatten()\n",
    "        counts = np.bincount(flat, minlength=NUM_CLASSES)\n",
    "        pixel_counts[:len(counts)] += counts\n",
    "        total_batches += 1\n",
    "\n",
    "        if total_batches >= max_batches:\n",
    "            print(f\"‚úÖ Processed {total_batches} batches. Stopping early.\")\n",
    "            break\n",
    "\n",
    "    total_pixels = np.sum(pixel_counts)\n",
    "    if total_pixels == 0:\n",
    "        print(\"‚ùå No valid pixels found.\")\n",
    "        return\n",
    "\n",
    "    class_names = [\n",
    "        \"0: Building\", \"1: Clutter\", \"2: Vegetation\",\n",
    "        \"3: Water\", \"4: Background\", \"5: Car\"\n",
    "    ]\n",
    "\n",
    "    print(f\"\\nüßÆ Total pixels processed: {total_pixels:,}\")\n",
    "    print(\"üìà Pixel Distribution (percentages):\\n\")\n",
    "    for i in range(NUM_CLASSES):\n",
    "        pct = (pixel_counts[i] / total_pixels) * 100\n",
    "        print(f\"Class {class_names[i]:<14} : {pct:6.2f}% ({pixel_counts[i]:,} px)\")\n",
    "\n",
    "    print(\"\\n‚úÖ Done.\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# =================================================================== \n",
    "# ------------------------------------------------------------------- \n",
    "# =================================================================== \n",
    "# ------------------------------------------------------------------- \n",
    "# =================================================================== \n",
    "# -------------------------------------------------------------------\n",
    "# =================================================================== \n",
    "\n",
    "\n",
    "\n",
    "# Class weights for 6 classes (0 to 5), based on imbalance\n",
    "weights = np.array([0.2374, 0.2374, 0.0356, 0.2374, 0.0148, 0.2374], dtype=np.float32)\n",
    "\n",
    "# Dice Loss\n",
    "def dice_loss(y_true, y_pred, smooth=1e-6):\n",
    "    # Ensure inputs are float32\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    \n",
    "    # Flatten predictions and ground truth\n",
    "    y_true_f = tf.reshape(y_true, [-1, 6])  # Shape: [batch_size * pixels, 6]\n",
    "    y_pred_f = tf.reshape(y_pred, [-1, 6])\n",
    "    \n",
    "    # Compute intersection and union per class\n",
    "    intersection = tf.reduce_sum(y_true_f * y_pred_f, axis=0)  # Sum over pixels, per class\n",
    "    denominator = tf.reduce_sum(y_true_f, axis=0) + tf.reduce_sum(y_pred_f, axis=0) + smooth\n",
    "    \n",
    "    # Dice score per class\n",
    "    dice = (2.0 * intersection + smooth) / denominator\n",
    "    \n",
    "    # Apply class weights and compute mean loss\n",
    "    weighted_dice = weights * dice\n",
    "    dice_loss = 1.0 - tf.reduce_mean(weighted_dice)\n",
    "    \n",
    "    return dice_loss\n",
    "\n",
    "# Categorical Focal Loss\n",
    "def categorical_focal_loss(gamma=2.0):\n",
    "    def focal_loss(y_true, y_pred):\n",
    "        # Ensure inputs are float32\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        y_pred = tf.cast(y_pred, tf.float32)\n",
    "        \n",
    "        # Clip predictions to avoid log(0)\n",
    "        y_pred = tf.clip_by_value(y_pred, 1e-7, 1.0 - 1e-7)\n",
    "        \n",
    "        # Compute cross-entropy\n",
    "        ce = -y_true * tf.math.log(y_pred)\n",
    "        \n",
    "        # Focal factor: (1 - p_t)^gamma\n",
    "        focal_factor = tf.pow(1.0 - y_pred, gamma)\n",
    "        \n",
    "        # Weighted focal loss\n",
    "        loss = focal_factor * ce\n",
    "        \n",
    "        # Mean over classes and pixels\n",
    "        return tf.reduce_mean(loss)\n",
    "    \n",
    "    return focal_loss\n",
    "\n",
    "# Combined Loss\n",
    "def combined_loss(y_true, y_pred):\n",
    "    return dice_loss(y_true, y_pred) + categorical_focal_loss(gamma=2.0)(y_true, y_pred)\n",
    "\n",
    "# Example: Compile model\n",
    "# model = your_model  # e.g., U-Net with 6 classes\n",
    "# model.compile(optimizer='adam', loss=combined_loss, metrics=['accuracy'])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOIvhFb8l65g0rsyWayDedf",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
