{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **init**"
      ],
      "metadata": {
        "id": "dHvUUJxQo2Ti"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "foyR1XBZ2inw"
      },
      "outputs": [],
      "source": [
        "# --- Installs ---\n",
        "import os\n",
        "os.environ[\"SM_FRAMEWORK\"] = \"tf.keras\"\n",
        "\n",
        "# Uninstall the current segmentation-models\n",
        "!pip uninstall -y segmentation-models\n",
        "# Install a specific, potentially more compatible version\n",
        "!pip install segmentation-models==1.0.1\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "\n",
        "# TensorFlow / Keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models, callbacks\n",
        "\n",
        "# Notebook execution\n",
        "import nbformat\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# --- Mount Google Drive ---\n",
        "from tqdm.notebook import tqdm\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "if not os.path.ismount(\"/content/drive\"):\n",
        "    drive.mount(\"/content/drive\")\n",
        "else:\n",
        "    print(\"âœ… Drive already mounted.\")\n",
        "\n",
        "\n",
        "\n",
        "# --- Extract Dataset from Drive ---\n",
        "import zipfile\n",
        "zip_path = \"/content/drive/MyDrive/Aerial Segmentation Machine Learning/chipped_data.zip\"\n",
        "extract_to = \"/content/chipped_data\"\n",
        "\n",
        "if not os.path.exists(extract_to):\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        file_list = zip_ref.namelist()\n",
        "        print(f\"ðŸ“¦ Extracting {len(file_list)} files...\")\n",
        "        for file in tqdm(file_list, desc=\"ðŸ”“ Unzipping\"):\n",
        "            zip_ref.extract(file, path=extract_to)\n",
        "    print(\"âœ… Dataset unzipped.\")\n",
        "else:\n",
        "    print(\"âœ… Dataset already extracted.\")\n",
        "\n",
        "'''\n",
        "zip_path = \"/content/drive/MyDrive/Aerial Segmentation Machine Learning/dataset-medium.zip\"\n",
        "extract_to = \"/content/dataset-medium\"\n",
        "\n",
        "if not os.path.exists(extract_to):\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        file_list = zip_ref.namelist()\n",
        "        print(f\"ðŸ“¦ Extracting {len(file_list)} files...\")\n",
        "        for file in tqdm(file_list, desc=\"ðŸ”“ Unzipping\"):\n",
        "            zip_ref.extract(file, path=extract_to)\n",
        "    print(\"âœ… Dataset unzipped.\")\n",
        "else:\n",
        "    print(\"âœ… Dataset already extracted.\")\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "# --- Directories ---\n",
        "base_dir = \"/content/chipped_data/chipped_data\"\n",
        "\n",
        "train_image_dir = os.path.join(base_dir, \"train/images\")\n",
        "train_elev_dir  = os.path.join(base_dir, \"train/elevations\")\n",
        "train_label_dir = os.path.join(base_dir, \"train/labels\")\n",
        "\n",
        "out_dir = \"/content/figs\"\n",
        "\n",
        "if not os.path.exists(out_dir):\n",
        "    os.makedirs(out_dir)\n",
        "    print(f\"ðŸ“‚ Created directory: {out_dir}\")\n",
        "else:\n",
        "    print(f\"âœ… Directory already exists: {out_dir}\")\n",
        "\n",
        "checkpoints = \"/content/checkpoints\"\n",
        "if not os.path.exists(checkpoints):\n",
        "    os.makedirs(checkpoints)\n",
        "    print(f\"ðŸ“‚ Created directory: {checkpoints}\")\n",
        "else:\n",
        "    print(f\"âœ… Directory already exists: {checkpoints}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Nx1wOuM2m3g"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import mixed_precision\n",
        "mixed_precision.set_global_policy('mixed_float16')\n",
        "\n",
        "# Install build tools and dependencies\n",
        "!apt-get install -y build-essential python3-dev\n",
        "!pip install cython\n",
        "!pip install git+https://github.com/lucasb-eyer/pydensecrf.git\n",
        "import pydensecrf.densecrf as dcrf\n",
        "from pydensecrf.utils import unary_from_softmax, create_pairwise_bilateral\n",
        "print(\"âœ… CRF imports working!\")\n",
        "\n",
        "\n",
        "# --- GitHub Access (for a public repository) ---\n",
        "import os\n",
        "\n",
        "# GitHub repo details\n",
        "repo_owner = \"AronBakes\"\n",
        "repo_name = \"Semantic-Segmentation-of-Aerial-Imagery\"\n",
        "branch = \"master\"\n",
        "\n",
        "# Construct the standard public URL without a token\n",
        "repo_url = f\"https://github.com/{repo_owner}/{repo_name}.git\"\n",
        "repo_dir = f\"/content/{repo_name}\"\n",
        "\n",
        "# The rest of your git clone logic remains exactly the same\n",
        "if not os.path.exists(repo_dir):\n",
        "    print(\"ðŸ“¥ Cloning public repo...\")\n",
        "    !git clone -b {branch} {repo_url} {repo_dir}\n",
        "else:\n",
        "    print(\"ðŸ”„ Pulling latest from GitHub...\")\n",
        "    %cd {repo_dir}\n",
        "    !git pull origin {branch}\n",
        "\n",
        "# Change directory into the repo\n",
        "%cd {repo_dir}\n",
        "\n",
        "\n",
        "\n",
        "# Dynamically load team notebooks\n",
        "import sys\n",
        "import nbformat\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "\n",
        "# notebook_dir = '/content/drive/MyDrive/Colab Notebooks'\n",
        "notebook_dir = '/content/Semantic-Segmentation-of-Aerial-Imagery'\n",
        "notebooks_to_import = [\n",
        "    \"util.ipynb\",\n",
        "    \"segformer.ipynb\",\n",
        "    \"models.ipynb\",\n",
        "    \"callbacks.ipynb\",\n",
        "    \"distribute.ipynb\",\n",
        "    \"data.ipynb\",\n",
        "    \"scoring.ipynb\",\n",
        "    \"training.ipynb\",\n",
        "\n",
        "    \"models_gen.ipynb\", \"data_gen.ipynb\", \"train_generator.ipynb\"\n",
        "]\n",
        "\n",
        "def run_notebook_cells(path):\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        nb = nbformat.read(f, as_version=4)\n",
        "        shell = InteractiveShell.instance()\n",
        "        for cell in nb.cells:\n",
        "            if cell.cell_type == 'code':\n",
        "                shell.run_cell(cell.source)\n",
        "\n",
        "\n",
        "# Load and run all notebooks (defines functions in global scope)\n",
        "for nb_file in notebooks_to_import:\n",
        "    nb_path = os.path.join(notebook_dir, nb_file)\n",
        "    print(f\"ðŸ“¥ Importing {nb_file}\")\n",
        "    run_notebook_cells(nb_path)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Segmentation Model Training**"
      ],
      "metadata": {
        "id": "tBYy4Vt0pQY-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fOv5JCv02qH6"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import mixed_precision\n",
        "mixed_precision.set_global_policy('mixed_float16')\n",
        "os.environ[\"SM_FRAMEWORK\"] = \"tf.keras\"\n",
        "\n",
        "\n",
        "# --- Training ---\n",
        "train_unet(\n",
        "    base_dir=base_dir, out_dir=out_dir,\n",
        "    input_type=\"rgb\",\n",
        "    model_type=\"enhanced_unet\",\n",
        "    batch_size=32,\n",
        "    epochs=150,\n",
        "    train_time=240,\n",
        "    tile_size=512,\n",
        "    verbose=1,\n",
        "    yummy=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Synthetic Data Generation**"
      ],
      "metadata": {
        "id": "Ox2ceOk2pFEG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- Configuration ---\n",
        "\n",
        "# The path to your saved generator model.\n",
        "# Use the latest/best one from your training run.\n",
        "# From your screenshot, 'gen_final_20250625-085333_epoch80.keras' is a good choice.\n",
        "GENERATOR_MODEL_PATH = '/content/drive/MyDrive/Aerial Segmentation Machine Learning/data_gen/checkpoints/gen_final_20250625-085333_epoch80.keras'\n",
        "\n",
        "# The path to your real chipped data, where the label maps are.\n",
        "CHIPPED_DATA_DIR = '/content/chipped_data/chipped_data'\n",
        "\n",
        "# The NEW directory on your Drive where the final synthetic dataset will be saved.\n",
        "SYNTHETIC_DATASET_DIR = '/content/drive/MyDrive/Aerial Segmentation Machine Learning/synthetic_dataset_v1'\n",
        "\n",
        "# --- Setup Directories ---\n",
        "# Create the main output folder\n",
        "os.makedirs(SYNTHETIC_DATASET_DIR, exist_ok=True)\n",
        "# Create the subdirectories for images and labels\n",
        "SYNTHETIC_IMAGES_DIR = os.path.join(SYNTHETIC_DATASET_DIR, 'train', 'images')\n",
        "SYNTHETIC_LABELS_DIR = os.path.join(SYNTHETIC_DATASET_DIR, 'train', 'labels')\n",
        "os.makedirs(SYNTHETIC_IMAGES_DIR, exist_ok=True)\n",
        "os.makedirs(SYNTHETIC_LABELS_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "# --- Main Generation Logic ---\n",
        "\n",
        "print(\"--- Starting Synthetic Data Generation ---\")\n",
        "\n",
        "# 1. Load the trained generator model\n",
        "print(f\"Loading generator model from: {GENERATOR_MODEL_PATH}\")\n",
        "generator = tf.keras.models.load_model(GENERATOR_MODEL_PATH)\n",
        "print(\"âœ… Generator loaded successfully.\")\n",
        "\n",
        "# 2. Find all the real label maps to use as blueprints\n",
        "real_label_dir = os.path.join(CHIPPED_DATA_DIR, 'train', 'labels')\n",
        "label_paths = sorted(glob.glob(os.path.join(real_label_dir, '*.png')))\n",
        "print(f\"Found {len(label_paths)} label maps to use as blueprints.\")\n",
        "\n",
        "# 3. Loop, Generate, and Save\n",
        "for label_path in tqdm(label_paths, desc=\"Generating synthetic pairs\"):\n",
        "    try:\n",
        "        # Load the original label map\n",
        "        label_img_raw = tf.io.read_file(label_path)\n",
        "        label_img = tf.image.decode_png(label_img_raw, channels=3)\n",
        "\n",
        "        # Preprocess it for the GAN (normalize to [-1, 1])\n",
        "        label_tensor = tf.cast(label_img, tf.float32)\n",
        "        label_tensor = (label_tensor / 127.5) - 1\n",
        "\n",
        "        # The generator expects a batch, so add a batch dimension\n",
        "        input_tensor = tf.expand_dims(label_tensor, 0)\n",
        "\n",
        "        # Generate the synthetic RGB image\n",
        "        generated_image_tensor = generator(input_tensor, training=False)[0] # Get the first image from the batch\n",
        "\n",
        "        # Denormalize the output from [-1, 1] to [0, 255] for saving\n",
        "        generated_image_np = (generated_image_tensor * 0.5 + 0.5).numpy() * 255\n",
        "        generated_image_np = generated_image_np.astype('uint8')\n",
        "\n",
        "        # --- Save the new pair ---\n",
        "        base_filename = os.path.basename(label_path)\n",
        "\n",
        "        # Define the output paths\n",
        "        new_label_path = os.path.join(SYNTHETIC_LABELS_DIR, base_filename)\n",
        "        # Create the corresponding image filename\n",
        "        new_image_filename = base_filename.replace('-label.png', '-ortho.png')\n",
        "        new_image_path = os.path.join(SYNTHETIC_IMAGES_DIR, new_image_filename)\n",
        "\n",
        "        # Save the original label map (the blueprint)\n",
        "        # We need to convert the tensor back to an image file format\n",
        "        tf.keras.utils.save_img(new_label_path, tf.cast(label_img, tf.uint8).numpy())\n",
        "\n",
        "        # Save the newly generated synthetic RGB image\n",
        "        # Convert from RGB (for tensorflow) to BGR (for OpenCV/cv2)\n",
        "        generated_image_bgr = cv2.cvtColor(generated_image_np, cv2.COLOR_RGB2BGR)\n",
        "        cv2.imwrite(new_image_path, generated_image_bgr)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Could not process {label_path}. Error: {e}\")\n",
        "\n",
        "print(\"\\n--- Synthetic Data Generation Complete! ---\")\n",
        "print(f\"âœ… New dataset saved in: {SYNTHETIC_DATASET_DIR}\")"
      ],
      "metadata": {
        "id": "XAEVd0BK0RCk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Q05Ed20M1tS_"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "if os.getcwd() not in sys.path:\n",
        "     sys.path.insert(0, os.getcwd())\n",
        "\n",
        "# params\n",
        "CHIPPED_DATA_DIR = '/content/chipped_data/chipped_data'\n",
        "\n",
        "# Prepare the dataset by calling the function from data_gan.py\n",
        "print(\"Preparing the dataset for the GAN...\")\n",
        "train_dataset = get_gan_dataset(CHIPPED_DATA_DIR, augment=True, shuffle=True)\n",
        "print(\"Dataset ready.\")\n",
        "\n",
        "# Call the main training function from train_generator.py\n",
        "print(f\"Starting GAN training for {EPOCHS} epochs...\")\n",
        "train(train_dataset, EPOCHS)\n",
        "print(\"--- Training Finished ---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Other**"
      ],
      "metadata": {
        "id": "OfMVb_kVpd7N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Dataset Distribution**"
      ],
      "metadata": {
        "id": "NDBkpAPnpZV-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rYVVU1420nU8"
      },
      "outputs": [],
      "source": [
        "df_full = csv_to_full_df()\n",
        "plot_class_distribution_from_df(df_full, \"Class Distribution\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "collapsed_sections": [
        "Ox2ceOk2pFEG",
        "OfMVb_kVpd7N"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}