{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "foyR1XBZ2inw"
      },
      "outputs": [],
      "source": [
        "# --- Installs ---\n",
        "import os\n",
        "os.environ[\"SM_FRAMEWORK\"] = \"tf.keras\"\n",
        "\n",
        "# Uninstall the current segmentation-models\n",
        "!pip uninstall -y segmentation-models\n",
        "# Install a specific, potentially more compatible version\n",
        "!pip install segmentation-models==1.0.1\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "\n",
        "# TensorFlow / Keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models, callbacks\n",
        "\n",
        "# Notebook execution\n",
        "import nbformat\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# --- Mount Google Drive ---\n",
        "from tqdm.notebook import tqdm\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "if not os.path.ismount(\"/content/drive\"):\n",
        "    drive.mount(\"/content/drive\")\n",
        "else:\n",
        "    print(\"âœ… Drive already mounted.\")\n",
        "\n",
        "\n",
        "\n",
        "# --- Extract Dataset from Drive ---\n",
        "import zipfile\n",
        "zip_path = \"/content/drive/MyDrive/chipped_data.zip\"\n",
        "extract_to = \"/content/chipped_data\"\n",
        "\n",
        "\n",
        "\n",
        "if not os.path.exists(extract_to):\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        file_list = zip_ref.namelist()\n",
        "        print(f\"ðŸ“¦ Extracting {len(file_list)} files...\")\n",
        "        for file in tqdm(file_list, desc=\"ðŸ”“ Unzipping\"):\n",
        "            zip_ref.extract(file, path=extract_to)\n",
        "    print(\"âœ… Dataset unzipped.\")\n",
        "else:\n",
        "    print(\"âœ… Dataset already extracted.\")\n",
        "\n",
        "\n",
        "\n",
        "# --- Directories ---\n",
        "base_dir = \"/content/chipped_data/content/chipped_data\"\n",
        "\n",
        "train_image_dir = os.path.join(base_dir, \"train/images\")\n",
        "train_elev_dir  = os.path.join(base_dir, \"train/elevations\")\n",
        "train_label_dir = os.path.join(base_dir, \"train/labels\")\n",
        "\n",
        "out_dir = \"/content/figs\"\n",
        "\n",
        "if not os.path.exists(out_dir):\n",
        "    os.makedirs(out_dir)\n",
        "    print(f\"ðŸ“‚ Created directory: {out_dir}\")\n",
        "else:\n",
        "    print(f\"âœ… Directory already exists: {out_dir}\")\n",
        "\n",
        "checkpoints = \"/content/checkpoints\"\n",
        "if not os.path.exists(checkpoints):\n",
        "    os.makedirs(checkpoints)\n",
        "    print(f\"ðŸ“‚ Created directory: {checkpoints}\")\n",
        "else:\n",
        "    print(f\"âœ… Directory already exists: {checkpoints}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Nx1wOuM2m3g"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import mixed_precision\n",
        "mixed_precision.set_global_policy('mixed_float16')\n",
        "\n",
        "# Install build tools and dependencies\n",
        "!apt-get install -y build-essential python3-dev\n",
        "!pip install cython\n",
        "!pip install git+https://github.com/lucasb-eyer/pydensecrf.git\n",
        "import pydensecrf.densecrf as dcrf\n",
        "from pydensecrf.utils import unary_from_softmax, create_pairwise_bilateral\n",
        "print(\"âœ… CRF imports working!\")\n",
        "\n",
        "\n",
        "# --- GitHub Access ---\n",
        "from getpass import getpass\n",
        "import os\n",
        "\n",
        "# GitHub repo details\n",
        "# token = getpass(\"ðŸ”‘ Enter your GitHub token: \")\n",
        "token = \"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\"\n",
        "\n",
        "repo_owner = \"AronBakes\"\n",
        "repo_name = \"CAB420\"\n",
        "branch = \"master\"\n",
        "repo_url = f\"https://{token}@github.com/{repo_owner}/{repo_name}.git\"\n",
        "repo_dir = f\"/content/{repo_name}\"\n",
        "\n",
        "# Clone or pull repo\n",
        "if not os.path.exists(repo_dir):\n",
        "    print(\"ðŸ“¥ Cloning fresh repo...\")\n",
        "    !git clone -b {branch} {repo_url} {repo_dir}\n",
        "else:\n",
        "    print(\"ðŸ”„ Pulling latest from GitHub...\")\n",
        "    %cd {repo_dir}\n",
        "    !git pull origin {branch}\n",
        "\n",
        "%cd {repo_dir}\n",
        "\n",
        "\n",
        "\n",
        "# Dynamically load team notebooks\n",
        "import sys\n",
        "import nbformat\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "\n",
        "# notebook_dir = '/content/drive/MyDrive/Colab Notebooks'\n",
        "notebook_dir = '/content/CAB420'\n",
        "notebooks_to_import = [\n",
        "    \"segformer.ipynb\",\n",
        "    \"models.ipynb\",\n",
        "    \"util.ipynb\",\n",
        "    \"callbacks.ipynb\",\n",
        "    \"distribute.ipynb\",\n",
        "    \"data.ipynb\",\n",
        "    \"scoring.ipynb\",\n",
        "    \"training.ipynb\",\n",
        "    \"inference.ipynb\",\n",
        "]\n",
        "\n",
        "def run_notebook_cells(path):\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        nb = nbformat.read(f, as_version=4)\n",
        "        shell = InteractiveShell.instance()\n",
        "        for cell in nb.cells:\n",
        "            if cell.cell_type == 'code':\n",
        "                shell.run_cell(cell.source)\n",
        "\n",
        "\n",
        "# Load and run all notebooks (defines functions in global scope)\n",
        "for nb_file in notebooks_to_import:\n",
        "    nb_path = os.path.join(notebook_dir, nb_file)\n",
        "    print(f\"ðŸ“¥ Importing {nb_file}\")\n",
        "    run_notebook_cells(nb_path)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fOv5JCv02qH6"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'train_unet' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      6\u001b[39m os.environ[\u001b[33m\"\u001b[39m\u001b[33mSM_FRAMEWORK\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mtf.keras\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# --- Training ---\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[43mtrain_unet\u001b[49m(\n\u001b[32m     11\u001b[39m     base_dir=base_dir, out_dir=out_dir,\n\u001b[32m     12\u001b[39m     input_type=\u001b[33m\"\u001b[39m\u001b[33mrgb\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     13\u001b[39m     model_type=\u001b[33m\"\u001b[39m\u001b[33menhanced_unet\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     14\u001b[39m     batch_size=\u001b[32m8\u001b[39m,\n\u001b[32m     15\u001b[39m     epochs=\u001b[32m50\u001b[39m,\n\u001b[32m     16\u001b[39m     train_time=\u001b[32m60\u001b[39m,\n\u001b[32m     17\u001b[39m     tile_size=\u001b[32m512\u001b[39m,\n\u001b[32m     18\u001b[39m     verbose=\u001b[32m1\u001b[39m,\n\u001b[32m     19\u001b[39m     yummy=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     20\u001b[39m )\n",
            "\u001b[31mNameError\u001b[39m: name 'train_unet' is not defined"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras import mixed_precision\n",
        "mixed_precision.set_global_policy('mixed_float16')\n",
        "os.environ[\"SM_FRAMEWORK\"] = \"tf.keras\"\n",
        "\n",
        "\n",
        "# --- Training ---\n",
        "train_unet(\n",
        "    base_dir=base_dir, out_dir=out_dir,\n",
        "    input_type=\"rgb\",\n",
        "    model_type=\"enhanced_unet\",\n",
        "    batch_size=8,\n",
        "    epochs=50,\n",
        "    train_time=60,\n",
        "    tile_size=512,\n",
        "    verbose=1,\n",
        "    yummy=False,\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
