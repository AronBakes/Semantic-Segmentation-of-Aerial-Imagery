{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7171e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_generator.py\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Configuration ---\n",
    "EPOCHS = 150\n",
    "OUTPUT_DIR = 'gan_training_output' # To save sample generated images\n",
    "CHECKPOINT_DIR = './gan_training_checkpoints'\n",
    "LAMBDA = 100 # Weight for the L1 loss (reconstruction loss)\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# --- Loss Functions and Optimizers ---\n",
    "loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "def discriminator_loss(disc_real_output, disc_generated_output):\n",
    "    real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)\n",
    "    generated_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)\n",
    "    total_disc_loss = real_loss + generated_loss\n",
    "    return total_disc_loss\n",
    "\n",
    "def generator_loss(disc_generated_output, gen_output, target):\n",
    "    gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output)\n",
    "    # L1 loss (mean absolute error) to make the generated image structurally similar to the target\n",
    "    l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n",
    "    total_gen_loss = gan_loss + (LAMBDA * l1_loss)\n",
    "    return total_gen_loss, gan_loss, l1_loss\n",
    "\n",
    "generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "\n",
    "# --- Checkpoint Manager ---\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "checkpoint_prefix = os.path.join(CHECKPOINT_DIR, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)\n",
    "\n",
    "# --- Image Generation and Plotting ---\n",
    "def generate_images(model, test_input, tar, epoch):\n",
    "    \"\"\"Generates and saves a plot of the input, real, and predicted images.\"\"\"\n",
    "    prediction = model(test_input, training=True)\n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "    display_list = [test_input[0], tar[0], prediction[0]]\n",
    "    title = ['Input Label Map', 'Ground Truth', 'Predicted Image']\n",
    "\n",
    "    for i in range(3):\n",
    "        plt.subplot(1, 3, i+1)\n",
    "        plt.title(title[i])\n",
    "        # Denormalize image from [-1, 1] to [0, 1] for display\n",
    "        plt.imshow(display_list[i] * 0.5 + 0.5)\n",
    "        plt.axis('off')\n",
    "    \n",
    "    # Save the plot\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, f'image_at_epoch_{epoch+1:04d}.png'))\n",
    "    plt.close()\n",
    "\n",
    "# --- The Core Training Step ---\n",
    "@tf.function\n",
    "def train_step(input_image, target, epoch):\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        gen_output = generator(input_image, training=True)\n",
    "\n",
    "        disc_real_output = discriminator([input_image, target], training=True)\n",
    "        disc_generated_output = discriminator([input_image, gen_output], training=True)\n",
    "\n",
    "        gen_total_loss, gen_gan_loss, gen_l1_loss = generator_loss(disc_generated_output, gen_output, target)\n",
    "        disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n",
    "\n",
    "    generator_gradients = gen_tape.gradient(gen_total_loss, generator.trainable_variables)\n",
    "    discriminator_gradients = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(generator_gradients, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(discriminator_gradients, discriminator.trainable_variables))\n",
    "\n",
    "    return disc_loss, gen_total_loss\n",
    "\n",
    "# --- Main Training Function ---\n",
    "def train(dataset, epochs):\n",
    "    # Take one batch from the dataset to use for visualization throughout training\n",
    "    example_input, example_target = next(iter(dataset.take(1)))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "        \n",
    "        disc_loss_epoch = []\n",
    "        gen_loss_epoch = []\n",
    "\n",
    "        for n, (input_image, target) in tqdm(dataset.enumerate(), desc=f\"  Training...\"):\n",
    "            disc_loss, gen_loss = train_step(input_image, target, epoch)\n",
    "            disc_loss_epoch.append(disc_loss)\n",
    "            gen_loss_epoch.append(gen_loss)\n",
    "\n",
    "        # Generate and save a sample image at the end of the epoch\n",
    "        generate_images(generator, example_input, example_target, epoch)\n",
    "\n",
    "        # Save a checkpoint every 20 epochs\n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "\n",
    "        print (f'Time taken for epoch {epoch + 1} is {time.time()-start:.2f} sec')\n",
    "        print (f'  -> Discriminator Loss: {tf.reduce_mean(disc_loss_epoch):.4f}, Generator Loss: {tf.reduce_mean(gen_loss_epoch):.4f}')\n",
    "\n",
    "# --- Run the Training ---\n",
    "if __name__ == '__main__':\n",
    "    chipped_data_dir = 'chipped_data_512'\n",
    "    \n",
    "    # Build the dataset\n",
    "    train_dataset = get_gan_dataset(chipped_data_dir, augment=True, shuffle=True)\n",
    "    \n",
    "    # Start training\n",
    "    train(train_dataset, EPOCHS)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
