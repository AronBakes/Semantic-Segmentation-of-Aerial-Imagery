{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7171e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_generator.py (New Robust Version)\n",
    "'''\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from google.colab import drive # NEW: Import for Google Drive\n",
    "\n",
    "# --- Configuration ---\n",
    "EPOCHS = 80\n",
    "LAMBDA = 100\n",
    "\n",
    "# --- NEW: Define Google Drive paths ---\n",
    "DRIVE_MOUNT_PATH = '/content/drive'\n",
    "# This is the folder you specified for all outputs\n",
    "DRIVE_OUTPUT_PATH = '/content/drive/MyDrive/Aerial Segmentation Machine Learning/data_gen'\n",
    "# Create specific subdirectories for organization\n",
    "DRIVE_CHECKPOINT_DIR = os.path.join(DRIVE_OUTPUT_PATH, 'checkpoints')\n",
    "DRIVE_IMAGE_DIR = os.path.join(DRIVE_OUTPUT_PATH, 'image_samples')\n",
    "\n",
    "# --- Main Training Function ---\n",
    "def train(dataset, epochs):\n",
    "    # --- NEW: Mount Drive and Create Directories ---\n",
    "    print(\"ðŸ’½ Mounting Google Drive...\")\n",
    "    if not os.path.ismount(DRIVE_MOUNT_PATH):\n",
    "        drive.mount(DRIVE_MOUNT_PATH)\n",
    "    else:\n",
    "        print(\"âœ… Drive already mounted.\")\n",
    "\n",
    "    os.makedirs(DRIVE_CHECKPOINT_DIR, exist_ok=True)\n",
    "    os.makedirs(DRIVE_IMAGE_DIR, exist_ok=True)\n",
    "    print(f\"âœ… Outputs will be saved to: {DRIVE_OUTPUT_PATH}\")\n",
    "    # ---\n",
    "\n",
    "    # Initialize models and optimizers (assumes these functions are in the global scope)\n",
    "    generator = Generator()\n",
    "    discriminator = Discriminator()\n",
    "    generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "    discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "\n",
    "    # Loss function\n",
    "    loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "    def discriminator_loss(disc_real_output, disc_generated_output):\n",
    "        # ... (loss logic is the same)\n",
    "        real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)\n",
    "        generated_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)\n",
    "        return real_loss + generated_loss\n",
    "\n",
    "    def generator_loss(disc_generated_output, gen_output, target):\n",
    "        # ... (loss logic is the same)\n",
    "        gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output)\n",
    "        l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n",
    "        return gan_loss + (LAMBDA * l1_loss)\n",
    "\n",
    "    # The core training step function\n",
    "    @tf.function\n",
    "    def train_step(input_image, target):\n",
    "        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "            gen_output = generator(input_image, training=True)\n",
    "            disc_real_output = discriminator([input_image, target], training=True)\n",
    "            disc_generated_output = discriminator([input_image, gen_output], training=True)\n",
    "            gen_loss = generator_loss(disc_generated_output, gen_output, target)\n",
    "            disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n",
    "\n",
    "        generator_gradients = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "        discriminator_gradients = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "        generator_optimizer.apply_gradients(zip(generator_gradients, generator.trainable_variables))\n",
    "        discriminator_optimizer.apply_gradients(zip(discriminator_gradients, discriminator.trainable_variables))\n",
    "        return disc_loss, gen_loss\n",
    "\n",
    "    # Image generation/plotting function\n",
    "    def generate_and_save_images(model, test_input, tar, epoch):\n",
    "        prediction = model(test_input, training=True)\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        display_list = [test_input[0], tar[0], prediction[0]]\n",
    "        title = ['Input Label Map', 'Ground Truth', 'Generated Image']\n",
    "        for i in range(3):\n",
    "            plt.subplot(1, 3, i+1)\n",
    "            plt.title(title[i])\n",
    "            plt.imshow(display_list[i] * 0.5 + 0.5) # Denormalize for viewing\n",
    "            plt.axis('off')\n",
    "        \n",
    "        # NEW: Save plot directly to Google Drive\n",
    "        save_path = os.path.join(DRIVE_IMAGE_DIR, f'image_at_epoch_{epoch+1:04d}.png')\n",
    "        plt.savefig(save_path)\n",
    "        plt.close()\n",
    "\n",
    "    # --- The Main Training Loop ---\n",
    "    example_input, example_target = next(iter(dataset.take(1)))\n",
    "    start_time_total = time.time()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        start_time_epoch = time.time()\n",
    "        print(f\"--- Starting Epoch {epoch + 1}/{epochs} ---\")\n",
    "        \n",
    "        disc_loss_epoch, gen_loss_epoch = [], []\n",
    "\n",
    "        for input_image, target in tqdm(dataset, desc=f\"  Training...\"):\n",
    "            disc_loss, gen_loss = train_step(input_image, target)\n",
    "            disc_loss_epoch.append(disc_loss)\n",
    "            gen_loss_epoch.append(gen_loss)\n",
    "\n",
    "        # Generate and save a sample image at the end of the epoch\n",
    "        generate_and_save_images(generator, example_input, example_target, epoch)\n",
    "        print(f\"âœ… Sample image for epoch {epoch+1} saved to Drive.\")\n",
    "\n",
    "        # --- NEW: Save models periodically to Google Drive ---\n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "            gen_save_path = os.path.join(DRIVE_CHECKPOINT_DIR, f'gen_{timestamp}_epoch{epoch+1}.keras')\n",
    "            disc_save_path = os.path.join(DRIVE_CHECKPOINT_DIR, f'disc_{timestamp}_epoch{epoch+1}.keras')\n",
    "            \n",
    "            generator.save(gen_save_path)\n",
    "            discriminator.save(disc_save_path)\n",
    "            print(f\"âœ… Saved models to Drive for epoch {epoch+1}.\")\n",
    "        # ---\n",
    "        \n",
    "        print(f'Time for epoch {epoch + 1} is {time.time()-start_time_epoch:.2f} sec')\n",
    "        print(f'  -> Avg Discriminator Loss: {tf.reduce_mean(disc_loss_epoch):.4f}, Avg Generator Loss: {tf.reduce_mean(gen_loss_epoch):.4f}')\n",
    "\n",
    "    # --- Final Save ---\n",
    "    print(\"--- Training finished. Saving final models. ---\")\n",
    "    timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    generator.save(os.path.join(DRIVE_CHECKPOINT_DIR, f'gen_final_{timestamp}_epoch{epochs}.keras'))\n",
    "    discriminator.save(os.path.join(DRIVE_CHECKPOINT_DIR, f'disc_final_{timestamp}_epoch{epochs}.keras'))\n",
    "    print(f\"âœ… Final models saved to: {DRIVE_CHECKPOINT_DIR}\")\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113f2ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_generator.py (New Unified Version)\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from google.colab import drive\n",
    "from datetime import datetime\n",
    "\n",
    "# --- Loss Functions (can be defined globally as they are stateless) ---\n",
    "loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "LAMBDA = 100\n",
    "\n",
    "def discriminator_loss(disc_real_output, disc_generated_output):\n",
    "    real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)\n",
    "    generated_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)\n",
    "    return real_loss + generated_loss\n",
    "\n",
    "def generator_loss(disc_generated_output, gen_output, target):\n",
    "    gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output)\n",
    "    l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n",
    "    return gan_loss + (LAMBDA * l1_loss)\n",
    "\n",
    "# --- Main Unified Training Function ---\n",
    "def train(dataset, epochs, resume_from_checkpoint_dir=None):\n",
    "    \"\"\"\n",
    "    Main training function for the GAN. Can start from scratch or resume from a checkpoint.\n",
    "\n",
    "    Args:\n",
    "        dataset (tf.data.Dataset): The prepared training dataset.\n",
    "        epochs (int): The total number of epochs to train for.\n",
    "        resume_from_checkpoint_dir (str, optional): Path to the checkpoint directory to resume from.\n",
    "                                                     If None, training starts from scratch. Defaults to None.\n",
    "    \"\"\"\n",
    "    # --- 1. Setup ---\n",
    "    # Mount Drive and Create Directories\n",
    "    print(\"ðŸ’½ Mounting Google Drive...\")\n",
    "    drive.mount('/content/drive', force_remount=True)\n",
    "    \n",
    "    DRIVE_OUTPUT_PATH = '/content/drive/MyDrive/Aerial Segmentation Machine Learning/data_gen'\n",
    "    DRIVE_CHECKPOINT_DIR = os.path.join(DRIVE_OUTPUT_PATH, 'checkpoints')\n",
    "    DRIVE_IMAGE_DIR = os.path.join(DRIVE_OUTPUT_PATH, 'image_samples')\n",
    "    os.makedirs(DRIVE_CHECKPOINT_DIR, exist_ok=True)\n",
    "    os.makedirs(DRIVE_IMAGE_DIR, exist_ok=True)\n",
    "    print(f\"âœ… Outputs will be saved to: {DRIVE_OUTPUT_PATH}\")\n",
    "\n",
    "    # --- 2. Initialize Models, Optimizers, and Checkpoint ---\n",
    "    generator = Generator()\n",
    "    discriminator = Discriminator()\n",
    "    generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "    discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "\n",
    "    checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                     discriminator_optimizer=discriminator_optimizer,\n",
    "                                     generator=generator,\n",
    "                                     discriminator=discriminator)\n",
    "    \n",
    "    start_epoch = 0 # Default start epoch\n",
    "\n",
    "    # --- 3. Resume from Checkpoint if Provided ---\n",
    "    if resume_from_checkpoint_dir:\n",
    "        latest_checkpoint = tf.train.latest_checkpoint(resume_from_checkpoint_dir)\n",
    "        if latest_checkpoint:\n",
    "            print(f\"âœ… Restoring from checkpoint: {latest_checkpoint}\")\n",
    "            checkpoint.restore(latest_checkpoint).expect_partial()\n",
    "            # Try to parse the epoch number from the filename\n",
    "            try:\n",
    "                start_epoch = int(latest_checkpoint.split('_epoch')[-1])\n",
    "                print(f\"âœ… Models restored. Resuming training from epoch {start_epoch}.\")\n",
    "            except (IndexError, ValueError):\n",
    "                print(\"âš ï¸ Could not parse epoch from checkpoint filename. You may need to set initial_epoch manually.\")\n",
    "        else:\n",
    "            print(f\"âš ï¸ No checkpoint found in '{resume_from_checkpoint_dir}'. Starting training from scratch.\")\n",
    "    else:\n",
    "        print(\"ðŸš€ Starting training from scratch.\")\n",
    "\n",
    "\n",
    "    # --- 4. The Core Training Step Function ---\n",
    "    @tf.function\n",
    "    def train_step(input_image, target):\n",
    "        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "            gen_output = generator(input_image, training=True)\n",
    "            disc_real_output = discriminator([input_image, target], training=True)\n",
    "            disc_generated_output = discriminator([input_image, gen_output], training=True)\n",
    "            gen_loss = generator_loss(disc_generated_output, gen_output, target)\n",
    "            disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n",
    "        \n",
    "        generator_gradients = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "        discriminator_gradients = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "        generator_optimizer.apply_gradients(zip(generator_gradients, generator.trainable_variables))\n",
    "        discriminator_optimizer.apply_gradients(zip(discriminator_gradients, discriminator.trainable_variables))\n",
    "        return disc_loss, gen_loss\n",
    "\n",
    "    # --- 5. Image Generation Function ---\n",
    "    def generate_and_save_images(model, test_input, tar, epoch, save_dir):\n",
    "        # ... (This function remains the same as before) ...\n",
    "        prediction = model(test_input, training=True)\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        display_list = [test_input[0], tar[0], prediction[0]]\n",
    "        title = ['Input Label Map', 'Ground Truth', 'Generated Image']\n",
    "        for i in range(3):\n",
    "            plt.subplot(1, 3, i+1)\n",
    "            plt.title(title[i])\n",
    "            plt.imshow(display_list[i] * 0.5 + 0.5)\n",
    "            plt.axis('off')\n",
    "        save_path = os.path.join(save_dir, f'image_at_epoch_{epoch+1:04d}.png')\n",
    "        plt.savefig(save_path)\n",
    "        plt.close()\n",
    "\n",
    "    # --- 6. The Main Training Loop ---\n",
    "    example_input, example_target = next(iter(dataset.take(1)))\n",
    "\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        start_time_epoch = time.time()\n",
    "        print(f\"--- Starting Epoch {epoch + 1}/{epochs} ---\")\n",
    "        \n",
    "        disc_loss_epoch, gen_loss_epoch = [], []\n",
    "\n",
    "        for input_image, target in tqdm(dataset, desc=f\"  Training...\"):\n",
    "            disc_loss, gen_loss = train_step(input_image, target)\n",
    "            disc_loss_epoch.append(disc_loss)\n",
    "            gen_loss_epoch.append(gen_loss)\n",
    "\n",
    "        generate_and_save_images(generator, example_input, example_target, epoch, DRIVE_IMAGE_DIR)\n",
    "        print(f\"âœ… Sample image for epoch {epoch+1} saved to Drive.\")\n",
    "\n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            checkpoint.save(file_prefix=os.path.join(DRIVE_CHECKPOINT_DIR, \"ckpt\"))\n",
    "            print(f\"âœ… Saved checkpoint to Drive for epoch {epoch+1}.\")\n",
    "        \n",
    "        print(f'Time for epoch {epoch + 1} is {time.time()-start_time_epoch:.2f} sec')\n",
    "        print(f'  -> Avg Discriminator Loss: {tf.reduce_mean(disc_loss_epoch):.4f}, Avg Generator Loss: {tf.reduce_mean(gen_loss_epoch):.4f}')\n",
    "\n",
    "    # --- 7. Final Save ---\n",
    "    print(\"--- Training finished. Saving final models. ---\")\n",
    "    final_gen_path = os.path.join(DRIVE_CHECKPOINT_DIR, f'gen_final_epoch{epochs}.keras')\n",
    "    final_disc_path = os.path.join(DRIVE_CHECKPOINT_DIR, f'disc_final_epoch{epochs}.keras')\n",
    "    generator.save(final_gen_path)\n",
    "    discriminator.save(final_disc_path)\n",
    "    print(f\"âœ… Final models saved to: {DRIVE_CHECKPOINT_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
