{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d3b309",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "# --- Global Configuration and Constants ---\n",
    "\n",
    "# List of all available file prefixes (tile_ids without coordinate suffix)\n",
    "# These are used to identify the source files for splitting the dataset.\n",
    "all_files = [\n",
    "    '107f24d6e9_F1BE1D4184INSPIRE', '11cdce7802_B6A62F8BE0INSPIRE', '12fa5e614f_53197F206FOPENPIPELINE', '130a76ebe1_68B40B480AOPENPIPELINE', \n",
    "    '1476907971_CHADGRISMOPENPIPELINE', '1553541487_APIGENERATED', '1553541585_APIGENERATED', '1553627230_APIGENERATED', '15efe45820_D95DF0B1F4INSPIRE', \n",
    "    '1726eb08ef_60693DB04DINSPIRE', '1d056881e8_29FEA32BC7INSPIRE', '1d4fbe33f3_F1BE1D4184INSPIRE', '1df70e7340_4413A67E91INSPIRE', '2552eb56dd_2AABB46C86OPENPIPELINE', \n",
    "    '25f1c24f30_EB81FE6E2BOPENPIPELINE', '2ef3a4994a_0CCD105428INSPIRE', '2ef883f08d_F317F9C1DFOPENPIPELINE', '34fbf7c2bd_E8AD935CEDINSPIRE', \n",
    "    '3502e187b2_23071E4605OPENPIPELINE', '39e77bedd0_729FB913CDOPENPIPELINE', '420d6b69b8_84B52814D2OPENPIPELINE', '520947aa07_8FCB044F58OPENPIPELINE', \n",
    "    '551063e3c5_8FCB044F58INSPIRE', '57426ebe1e_84B52814D2OPENPIPELINE', '5fa39d6378_DB9FF730D9OPENPIPELINE', '6f93b9026b_F1BFB8B17DOPENPIPELINE', \n",
    "    '7008b80b00_FF24A4975DINSPIRE', '74d7796531_EB81FE6E2BOPENPIPELINE', '7c719dfcc0_310490364FINSPIRE', '84410645db_8D20F02042OPENPIPELINE', \n",
    "    '8710b98ea0_06E6522D6DINSPIRE', '888432f840_80E7FD39EBINSPIRE', '9170479165_625EDFBAB6OPENPIPELINE', 'a1af86939f_F1BE1D4184OPENPIPELINE', \n",
    "    'b61673f780_4413A67E91INSPIRE', 'b705d0cc9c_E5F5E0E316OPENPIPELINE', 'b771104de5_7E02A41EBEOPENPIPELINE', 'c2e8370ca3_3340CAC7AEOPENPIPELINE', \n",
    "    'c37dbfae2f_84B52814D2OPENPIPELINE', 'c644f91210_27E21B7F30OPENPIPELINE', 'c6d131e346_536DE05ED2OPENPIPELINE', 'c8a7031e5f_32156F5DC2INSPIRE', \n",
    "    'cc4b443c7d_A9CBEF2C97INSPIRE', 'd06b2c67d2_2A62B67B52OPENPIPELINE', 'd9161f7e18_C05BA1BC72OPENPIPELINE', 'dabec5e872_E8AD935CEDINSPIRE', \n",
    "    'e87da4ebdb_29FEA32BC7INSPIRE', 'ebffe540d0_7BA042D858OPENPIPELINE', 'ec09336a6f_06BA0AF311OPENPIPELINE', \n",
    "    'f0747ed88d_E74C0DD8FDOPENPIPELINE', 'f4dd768188_NOLANOPENPIPELINE', 'f56b6b2232_2A62B67B52OPENPIPELINE', \n",
    "    'f971256246_MIKEINSPIRE', 'f9f43e5144_1DB9E6F68BINSPIRE', 'fc5837dcf8_7CD52BE09EINSPIRE'\n",
    "]\n",
    "\n",
    "# Prefixes for files designated for the validation set\n",
    "val_files = [\n",
    "    \"c644f91210_27E21B7F30OPENPIPELINE\",\n",
    "    \"f9f43e5144_1DB9E6F68BINSPIRE\",\n",
    "    \"1d056881e8_29FEA32BC7INSPIRE\",\n",
    "    \"3502e187b2_23071E4605OPENPIPELINE\",\n",
    "    \"d9161f7e18_C05BA1BC72OPENPIPELINE\",\n",
    "    \"c8a7031e5f_32156F5DC2INSPIRE\",\n",
    "    \"551063e3c5_8FCB044F58INSPIRE\",\n",
    "    \"fc5837dcf8_7CD52BE09EINSPIRE\",\n",
    "    \"39e77bedd0_729FB913CDOPENPIPELINE\",\n",
    "]\n",
    "\n",
    "# Prefixes for files designated for the test set\n",
    "test_files = [\n",
    "    \"25f1c24f30_EB81FE6E2BOPENPIPELINE\",\n",
    "    \"1d4fbe33f3_F1BE1D4184INSPIRE\",\n",
    "    \"15efe45820_D95DF0B1F4INSPIRE\",\n",
    "    \"c6d131e346_536DE05ED2OPENPIPELINE\",\n",
    "    \"12fa5e614f_53197F206FOPENPIPELINE\",\n",
    "    \"5fa39d6378_DB9FF730D9OPENPIPELINE\",\n",
    "    \"ebffe540d0_7BA042D858OPENPIPELINE\",\n",
    "    \"8710b98ea0_06E6522D6DINSPIRE\",\n",
    "    \"84410645db_8D20F02042OPENPIPELINE\",\n",
    "    \"a1af86939f_F1BE1D4184OPENPIPELINE\"\n",
    "]\n",
    "\n",
    "# --- Class Definitions ---\n",
    "\n",
    "NUM_CLASSES = 6 # Total number of semantic classes\n",
    "# Column names for class pixel counts in the metadata CSV\n",
    "class_cols = ['0: Building', '1: Clutter', '2: Vegetation', '3: Water', '4: Background', '5: Car']\n",
    "CLASS_NAMES = ['Building', 'Clutter', 'Vegetation', 'Water', 'Background', 'Car']\n",
    "\n",
    "# Mapping from RGB color (as tuple) to integer class ID\n",
    "COLOR_TO_CLASS = {\n",
    "    (230, 25, 75): 0,      # Red: Building\n",
    "    (145, 30, 180): 1,     # Purple: Clutter\n",
    "    (60, 180, 75): 2,      # Green: Vegetation\n",
    "    (245, 130, 48): 3,     # Orange: Water\n",
    "    (255, 255, 255): 4,    # White: Background\n",
    "    (0, 130, 200): 5,      # Blue: Car\n",
    "    # Note: (255, 0, 255): 6 exists in original COLOR_TO_CLASS but not in CLASS_NAMES.\n",
    "    # Assuming it's an un-used or ignored class based on NUM_CLASSES = 6.\n",
    "}\n",
    "\n",
    "# Inverse mapping from integer class ID to RGB color (as tuple)\n",
    "CLASS_TO_COLOR = {v: k for k, v in COLOR_TO_CLASS.items() if v < NUM_CLASSES} # Ensure it matches NUM_CLASSES\n",
    "\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "def compute_chip_score(row: pd.Series) -> float:\n",
    "    \"\"\"\n",
    "    Computes a 'score' for a given chip (row in the DataFrame) to guide selection.\n",
    "    Lower scores indicate more desirable chips for training.\n",
    "    \n",
    "    Chips with cars are always kept (-1 score).\n",
    "    Chips with water (but not almost entirely water) are always kept (-1 score).\n",
    "    Chips dominated by background are skipped (inf score).\n",
    "    Chips with only one class present are skipped (inf score).\n",
    "    Other chips are scored based on the inverse proportion of certain classes,\n",
    "    with bonuses for diversity.\n",
    "\n",
    "    Args:\n",
    "        row (pd.Series): A row from the metadata DataFrame, containing normalized\n",
    "                         pixel counts for each class.\n",
    "\n",
    "    Returns:\n",
    "        float: A score indicating the chip's desirability.\n",
    "               -1: Always keep (e.g., contains cars or significant water).\n",
    "               inf: Always skip (e.g., mostly background or too homogeneous).\n",
    "               Other float: Score for ranking; lower is better.\n",
    "    \"\"\"\n",
    "    car_ratio = row[\"5: Car_norm\"]\n",
    "    background_ratio = row[\"4: Background_norm\"]\n",
    "    water_ratio = row[\"3: Water_norm\"]\n",
    "\n",
    "    # Create a dictionary of normalized class ratios for easier access\n",
    "    class_ratios = {f\"{i}: {cls}\": row[f\"{i}: {cls}_norm\"] for i, cls in enumerate(CLASS_NAMES)}\n",
    "\n",
    "    # Rule 1: Always keep chips with cars\n",
    "    if car_ratio > 0:\n",
    "        return -1.0 # Use float to be consistent with inf\n",
    "\n",
    "    # Rule 2: Always keep chips with water, unless it's almost pure water\n",
    "    if water_ratio > 0 and water_ratio < 0.95:\n",
    "        return -1.0\n",
    "\n",
    "    # Rule 3: Skip chips dominated by background\n",
    "    if background_ratio >= 0.95: # Original: 0.825, adjusted to 0.95 as per original logic.\n",
    "        return float('inf')\n",
    "\n",
    "    # Rule 4: Count how many unique classes exist in the chip\n",
    "    # A class is considered \"present\" if its normalized pixel count is above a small threshold\n",
    "    unique_class_count = sum(1 for val in class_ratios.values() if val > 0.001)\n",
    "    if unique_class_count == 1:\n",
    "        return float('inf') # Skip if only one class is significantly present (too homogeneous)\n",
    "\n",
    "    # Rule 5: Calculate score based on class proportions (lower is better)\n",
    "    # Penalize lack of building, water, or too much vegetation (inverted score)\n",
    "    score = 0.0\n",
    "    score += 2.5 * max(1 - class_ratios[\"0: Building\"], 0)   # Penalize chips with few buildings\n",
    "    score += 11.0 * max(1 - class_ratios[\"3: Water\"], 0)      # Heavily penalize chips with few water\n",
    "    score += -19.5 * min(0.1 - class_ratios[\"2: Vegetation\"], 0) # Reward chips with moderate vegetation (up to 0.1)\n",
    "\n",
    "    # Rule 6: Reward chips for class diversity (lower score for more diverse chips)\n",
    "    if unique_class_count >= 4:\n",
    "        score -= 6.0\n",
    "    elif unique_class_count == 3:\n",
    "        score -= 3.0\n",
    "    elif unique_class_count == 2:\n",
    "        score -= 0.5\n",
    "    # If unique_class_count is 1, it's already skipped by Rule 4, so this last `elif` is effectively unreachable\n",
    "    # elif unique_class_count == 1:\n",
    "    #     score += 5.0 # This would penalize homogeneous chips, but they are already skipped\n",
    "\n",
    "    return score\n",
    "\n",
    "def plot_class_distribution_from_df(dataframe: pd.DataFrame, title: str = \"Class Distribution\"):\n",
    "    \"\"\"\n",
    "    Plots the normalized pixel distribution for each class across a given DataFrame of chips.\n",
    "\n",
    "    Args:\n",
    "        dataframe (pd.DataFrame): The DataFrame containing '0: Building', '1: Clutter', etc., columns.\n",
    "        title (str): The title for the plot.\n",
    "    \"\"\"\n",
    "    if dataframe.empty:\n",
    "        print(f\"Warning: DataFrame for '{title}' is empty. Cannot plot distribution.\")\n",
    "        return\n",
    "\n",
    "    # Calculate total pixel sums for each class across all selected chips\n",
    "    pixel_sums = dataframe[class_cols].sum()\n",
    "    total_pixels_across_df = pixel_sums.sum()\n",
    "\n",
    "    if total_pixels_across_df == 0:\n",
    "        print(f\"Warning: No pixels found in DataFrame for '{title}'. Cannot plot distribution.\")\n",
    "        return\n",
    "\n",
    "    # Calculate proportion of each class relative to the total pixels in the DataFrame\n",
    "    pixel_props = pixel_sums / total_pixels_across_df\n",
    "\n",
    "    # Prepare labels and colors for the plot\n",
    "    class_labels = [f\"{i}: {CLASS_NAMES[i]}\" for i in range(NUM_CLASSES)]\n",
    "    # Convert normalized RGB to 0-1 range for matplotlib\n",
    "    colours = [np.array(CLASS_TO_COLOR[i]) / 255.0 for i in range(NUM_CLASSES)]\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    bars = plt.bar(class_labels, pixel_props, color=colours, edgecolor='black')\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Class\")\n",
    "    plt.ylabel(\"Proportion\")\n",
    "    plt.grid(True, axis='y', linestyle='--', alpha=0.5)\n",
    "\n",
    "    # Add proportion labels on top of bars\n",
    "    for bar, prop in zip(bars, pixel_props):\n",
    "        plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height(), f\"{prop:.2%}\",\n",
    "                 ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# --- Main Dataframe Loading Functions ---\n",
    "\n",
    "def csv_to_df(split: str, subset: float = 0.27) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads metadata from CSV and filters/scores chips for a specific data split (train, val, or test).\n",
    "\n",
    "    Args:\n",
    "        split (str): The desired data split ('train', 'val', or 'test').\n",
    "        subset (float): For 'train' split, the proportion of chips to select based on scoring.\n",
    "                        Ignored for 'val' and 'test'.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame of selected chips for the specified split.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If an invalid split type is provided.\n",
    "    \"\"\"\n",
    "    metadata_path = \"/content/chipped_data/content/chipped_data/train_metadata.csv\"\n",
    "    df = pd.read_csv(metadata_path)\n",
    "\n",
    "    if split == 'train':\n",
    "        # Exclude chips that belong to the validation or test sets based on their tile_id prefix\n",
    "        excluded_prefixes = val_files + test_files\n",
    "        df = df[~df['tile_id'].apply(lambda tid: any(tid.startswith(p) for p in excluded_prefixes))].copy()\n",
    "\n",
    "        # Calculate normalized pixel counts for scoring\n",
    "        df['total'] = df[class_cols].sum(axis=1)\n",
    "        for col in class_cols:\n",
    "            # Handle potential division by zero if 'total' pixels for a chip is 0 (shouldn't happen with valid data)\n",
    "            df[col + '_norm'] = df[col] / df['total'].replace(0, np.nan) # Replace 0 with NaN to avoid /0, will result in NaN ratios\n",
    "            df[col + '_norm'] = df[col + '_norm'].fillna(0) # Fill NaN ratios with 0\n",
    "\n",
    "        # Compute a score for each chip to determine inclusion in the training set\n",
    "        df[\"score\"] = df.apply(compute_chip_score, axis=1)\n",
    "\n",
    "        # Filter chips based on computed scores\n",
    "        keep_chips_car_water = df[df[\"score\"] == -1.0].copy()\n",
    "        skipped_chips_background_homo = df[df[\"score\"] == float('inf')].copy()\n",
    "\n",
    "        print(f\"🚗 Chips with cars/significant water (kept): {len(keep_chips_car_water)} ({len(keep_chips_car_water)/len(df):.2%})\")\n",
    "        print(f\"🧱 Chips skipped due to background/homogeneity: {len(skipped_chips_background_homo)} ({len(skipped_chips_background_homo)/len(df):.2%})\")\n",
    "\n",
    "        # Select the remaining chips based on their score\n",
    "        # 'rest' contains chips that were not 'always_keep' and not 'skipped'\n",
    "        rest = df[(df['score'] != -1.0) & (df['score'] != float('inf'))].sort_values('score').copy()\n",
    "\n",
    "        # Determine how many more chips are needed to reach the 'subset' proportion\n",
    "        num_chips_to_select_from_rest = int(len(df) * subset) - len(keep_chips_car_water)\n",
    "        \n",
    "        # Ensure we don't try to select more chips than available\n",
    "        if num_chips_to_select_from_rest < 0:\n",
    "            num_chips_to_select_from_rest = 0 # Should not happen if subset is reasonable\n",
    "        \n",
    "        # Concatenate the always-kept chips with the best-scoring chips from 'rest'\n",
    "        best_chips = pd.concat([keep_chips_car_water, rest.head(num_chips_to_select_from_rest)])\n",
    "        final_n = len(best_chips)\n",
    "\n",
    "        plot_class_distribution_from_df(best_chips, title=\"Training Class Distribution\")\n",
    "        print(f\"\\n📦 Selected {final_n:,} chips from {len(df):,} total ({final_n / len(df):.2%})\")\n",
    "\n",
    "        return best_chips\n",
    "\n",
    "    elif split in ['val', 'test']:\n",
    "        # For validation and test sets, select chips based on predefined file lists\n",
    "        file_list = val_files if split == 'val' else test_files\n",
    "        df = df[df['tile_id'].apply(lambda tid: any(tid.startswith(p) for p in file_list))].copy()\n",
    "        # Filter for chips that are at 256x256 pixel boundaries (assuming full tiles)\n",
    "        df = df[(df['x'] % 256 == 0) & (df['y'] % 256 == 0)].copy()\n",
    "        plot_class_distribution_from_df(df, title=f\"{split.capitalize()} Class Distribution\")\n",
    "        return df\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid split: {split}. Choose from 'train', 'val', or 'test'.\")\n",
    "\n",
    "\n",
    "def csv_to_hard_df() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads metadata and filters for 'hard' chips, typically used for a second stage of training.\n",
    "    'Hard' chips are defined by specific criteria focusing on mixed classes,\n",
    "    presence of water, building, clutter, and specific thresholds.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing chips selected based on 'hard' criteria.\n",
    "    \"\"\"\n",
    "    metadata_path = \"/content/chipped_data/content/chipped_data/train_metadata.csv\"\n",
    "    df = pd.read_csv(metadata_path)\n",
    "\n",
    "    # Exclude chips that belong to the validation or test sets (for training data)\n",
    "    exclude_files = set(val_files + test_files)\n",
    "    df = df[~df[\"source_file\"].isin(exclude_files)].copy() # Use .copy() to avoid SettingWithCopyWarning\n",
    "\n",
    "    # Define column references for convenience\n",
    "    cols = {\n",
    "        \"building\": \"0: Building\",\n",
    "        \"clutter\": \"1: Clutter\",\n",
    "        \"vegetation\": \"2: Vegetation\",\n",
    "        \"water\": \"3: Water\",\n",
    "        \"background\": \"4: Background\",\n",
    "        \"car\": \"5: Car\"\n",
    "    }\n",
    "\n",
    "    # Helper column to count classes with non-zero pixel counts\n",
    "    df['non_zero_classes'] = df[[*cols.values()]].gt(0).sum(axis=1)\n",
    "\n",
    "    # --- Criteria for 'Hard' Chips ---\n",
    "    # 1. Chips with water present alongside background\n",
    "    water_and_background = df[(df[cols[\"water\"]] > 0) & (df[cols[\"background\"]] > 0)].copy()\n",
    "    # 2. Chips with water present alongside vegetation\n",
    "    water_and_veg = df[(df[cols[\"water\"]] > 0) & (df[cols[\"vegetation\"]] > 0)].copy()\n",
    "    # 3. Chips with water, background, and vegetation all present\n",
    "    water_background_veg = df[\n",
    "        (df[cols[\"water\"]] > 0) &\n",
    "        (df[cols[\"background\"]] > 0) &\n",
    "        (df[cols[\"vegetation\"]] > 0)\n",
    "    ].copy()\n",
    "    # 4. Chips with significant building and background, but building not dominating\n",
    "    building_and_background = df[\n",
    "        (df[cols[\"building\"]] > 0.05) & (df[cols[\"background\"]] > 0.1) &\n",
    "        (df[cols[\"building\"]] < 0.4) # reduce building-dominant chips\n",
    "    ].copy()\n",
    "    # 5. Chips that are almost pure water\n",
    "    pure_water = df[(df[[*cols.values()]].sum(axis=1) > 0) & # Ensure not entirely empty\n",
    "                    (df[cols[\"water\"]] / df[[*cols.values()]].sum(axis=1) > 0.7) \n",
    "                   ].copy()\n",
    "\n",
    "    # 6. A sample of chips that are almost pure building (to ensure building class isn't forgotten)\n",
    "    # Ensure there are enough pure building chips to sample from before trying to sample\n",
    "    potential_pure_building = df[(df[[*cols.values()]].sum(axis=1) > 0) &\n",
    "                                 (df[cols[\"building\"]] / df[[*cols.values()]].sum(axis=1) > 0.7)\n",
    "                                ].copy()\n",
    "    num_pure_building_to_sample = min(100, len(potential_pure_building)) \n",
    "    pure_building = potential_pure_building.sample(n=num_pure_building_to_sample, random_state=42).copy()\n",
    "\n",
    "\n",
    "    # Combine all selected 'hard' chips and remove duplicates\n",
    "    stage2_df = pd.concat([\n",
    "        water_and_background,\n",
    "        water_and_veg,\n",
    "        water_background_veg,\n",
    "        building_and_background,\n",
    "        pure_water,\n",
    "        pure_building,\n",
    "    ]).drop_duplicates().reset_index(drop=True) # Reset index after concat and drop_duplicates\n",
    "\n",
    "    plot_class_distribution_from_df(stage2_df, title=\"Stage 2 Class Distribution\")\n",
    "    print(f\"\\n📦 Selected {len(stage2_df):,} hard chips for Stage 2 training.\")\n",
    "    return stage2_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
